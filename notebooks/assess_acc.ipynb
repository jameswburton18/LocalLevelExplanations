{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from src.utils import linearise_input\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs = \"peachy-armadillo-21 worthy-fog-20 absurd-dew-49 glad-resonance-21 dry-thunder-55 sleek-armadillo-70 solar-serenity-72 hearty-shadow-89 fresh-oath-71 glowing-firebrand-87 earthy-hill-73 twilight-terrain-69 gallant-wave-65 lively-glitter-40 scarlet-capybara-56 breezy-night-41 logical-rain-50 crimson-leaf-88 rich-valley-82 graceful-lion-83 fallen-shadow-80 serene-serenity-84 decent-donkey-81\".split()\n",
    "# \"ethereal-dream-23\" not there\n",
    "runs = \"worthy-fog-20 hearty-shadow-89 lively-glitter-40 graceful-lion-83\".split()\n",
    "runs = \"balmy-forest-91 absurd-dew-49 dry-thunder-55 solar-serenity-72 fresh-oath-71 earthy-hill-73 gallant-wave-65 scarlet-capybara-56 logical-rain-50 rich-valley-82 fallen-shadow-80 decent-donkey-81\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration james-burton--textual-explanations-19ff8605823ae74a\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-19ff8605823ae74a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-19ff8605823ae74a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6c6dda6c2f05f317.arrow\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "test = load_dataset(\"james-burton/textual-explanations\", split=\"test\")\n",
    "test = test.map(\n",
    "        lambda x: linearise_input(x, 'ord_first', 15, True),\n",
    "        ) \n",
    "\n",
    "for run in runs:\n",
    "    with open(f\"model_preds/{run}/test_predictions.txt\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if i not in preds:\n",
    "                    preds[i] = {'narrations': [], 'names': [], 'input': test['input'][i]}\n",
    "                split_line = line#.split('. ')\n",
    "                preds[i]['narrations'].append(split_line)\n",
    "                preds[i]['names'].append([run])#*len(split_line))\n",
    "              \n",
    "# # Split by line\n",
    "# for j in range(len(preds)):\n",
    "#     max_len = max([len(narr) for narr in preds[j]['narrations']])\n",
    "#     preds[j]['max_len'] = max_len\n",
    "#     for i in range(max_len):\n",
    "#         preds[j][f'line_{i}'] = []\n",
    "#         preds[j][f'name_{i}'] = []\n",
    "#         for narr, name in zip(preds[j]['narrations'], preds[j]['names']):\n",
    "#             if len(narr) > i:\n",
    "#                 preds[j][f'line_{i}'].append(narr[i])\n",
    "#                 preds[j][f'name_{i}'].append(name[i])\n",
    "#             else:\n",
    "#                 preds[j][f'line_{i}'].append('Blank row')\n",
    "#                 preds[j][f'name_{i}'].append('Blank row')\n",
    "            \n",
    "\n",
    "# # shuffle lines but keep names and lines together\n",
    "# import random\n",
    "# random.seed(42)\n",
    "# for j in range(len(preds)):\n",
    "#     for i in range(preds[j]['max_len']):\n",
    "#         combined = list(zip(preds[j][f'line_{i}'], preds[j][f'name_{i}']))\n",
    "#         random.shuffle(combined)\n",
    "#         preds[j][f'line_{i}'], preds[j][f'name_{i}'] = zip(*combined)\n",
    "\n",
    "# for j in range(len(preds)):\n",
    "#     with open(f\"model_preds/combined/test_case_{j}.csv\", 'w') as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([preds[j]['input']])\n",
    "#         for i in range(preds[j]['max_len']):\n",
    "#             for row in zip(preds[j][f'line_{i}'], preds[j][f'name_{i}']):\n",
    "#                 writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given that the likelihoods of class C1 and class C2 are 36.38% and 63.62%, respectively, the case under consideration is labelled as C2 by the model. The top relevant features for this prediction decision are F9, F12, F16, and F6, while the least relevant features include F5 and F1. From the analysis performed, the values of F9, F12, and F16 are shown to have strong positive support for C2, while F6 and F13 are the top negative attributes, pulling the prediction towards C1. However, the combined magnitude of their impacts on the model is less than that of the positive input features mentioned above, hence the moderately strong push towards labelling the case as C2. The values of some features show a low influence on C2 prediction and these are F2, F10, F5, and F1.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]['narration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 7\n",
    "with open(f'test-case_{idx}.txt', 'w') as out:\n",
    "    out.write(f\"Answer: \\n {test[idx]['narration']} \\n\\n\")\n",
    "    out.write(f\"Input: \\n {preds[idx]['input']} \\n\\n\")\n",
    "    out.write(f\"Outputs: \\n\")\n",
    "    for i, narr in enumerate(preds[idx]['narrations']):\n",
    "        out.write(narr)\n",
    "        out.write('\\n\\n')\n",
    "    out.write(f\"Run names: \\n\")\n",
    "    out.write(' '.join([p[0] for p in preds[idx]['names']]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balmy-forest-91',\n",
       " 'absurd-dew-49',\n",
       " 'dry-thunder-55',\n",
       " 'solar-serenity-72',\n",
       " 'fresh-oath-71',\n",
       " 'earthy-hill-73',\n",
       " 'gallant-wave-65',\n",
       " 'scarlet-capybara-56',\n",
       " 'logical-rain-50',\n",
       " 'rich-valley-82',\n",
       " 'fallen-shadow-80',\n",
       " 'decent-donkey-81']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p[0] for p in preds[2]['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(10,15):\n",
    "    with open(f\"model_preds/testing/test_case_{j}.csv\", 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([test['narration'][j]])\n",
    "        writer.writerow([preds[j]['input']])\n",
    "        for row in zip(preds[j]['narrations']):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(len(preds)):\n",
    "#     with open(f\"model_preds/combined/test_case_{j}.txt\", 'w') as file:\n",
    "#         file.write(f\"Input: {preds[j]['input']}\")\n",
    "#         for i in range(preds[j]['max_len']):\n",
    "#             for k in range(len(preds[j][f'line_{i}'])):\n",
    "#                 file.write(preds[j][f'line_{i}'][k])\n",
    "                \n",
    "# repeat again but write to csv\n",
    "import csv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we split the narratives into a question answer thing\n",
    "How many lines per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration james-burton--textual-explanations-19ff8605823ae74a\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-19ff8605823ae74a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 375/375 [00:00<00:00, 3081.93ex/s]\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"james-burton/textual-explanations\", split=\"train\")\n",
    "\n",
    "train = train.map(\n",
    "        lambda x: linearise_input(x, 'ord_first', 15, True),\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The classifier is very uncertain about the correct label for the case given.  Regarding the classifier's decision, there is close to an even split on the probability of either of the possible labels is the correct label but the classifier chooses the label as C2. The prediction verdict above is attributed to the contributions of mainly the following features: F4, F11, F3, and F15, however, the lowest ranked features are F7, F19, and F1. Analysing the direction of influence of the features shows that there are ten positive and ten negative features.  Positive features such as F3, F15, F8, and F20 increase the response of the classifier in favour of the assigned label. Conversely, negative features such as F4, F11, F12, and F2 decrease the likelihood of C2 being the correct label given that their values support the alternative label, C1. The uncertainty concerning the label assignment can be due to the fact that the top negative features F4 and F11 have very high attributions shifting the classifier's verdict away from the C2 class.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['narration'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarize the prediction made for the test under consideration along with the likelihood of the different possible class labels.',\n",
       " 'Provide a statement summarizing the ranking of the features as shown in the feature impact plot.',\n",
       " 'Compare the direction of impact of the features: F4 and F11.',\n",
       " 'Summarize the direction of influence of the features (F3, F15, F8 and F12) with moderate impact on the prediction made for this test case.',\n",
       " 'Provide a statement on the features with the least impact on the prediction made for this test case.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]['narrative_questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| predicted class | C2 50.00% | other classes | C1 50.00% | 1st | F4 negative -0.32 | 2nd | F11 negative -0.24 | 3rd | F3 positive 0.04 | 4th | F15 positive 0.03 | 5th | F8 positive 0.02 | 6th | F12 negative -0.02 | 7th | F2 negative -0.02 | 8th | F20 positive 0.02 | 9th | F10 negative -0.02 | 10th | F9 negative -0.02 | 11th | F14 positive 0.02 | 12th | F16 positive 0.01 | 13th | F18 negative -0.01 | 14th | F6 negative -0.01 | 15th | F5 positive 0.01 |'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "# random.seed(42)\n",
    "\n",
    "all_train = json.load(open('raw_data/all_train.json',encoding='utf-8'))\n",
    "test = json.load(open('raw_data/test_set_new.json',encoding='utf-8'))\n",
    "all = all_train + test\n",
    "no_task = [x for x in all if x.get('task_name', None) == None]\n",
    "all = [x for x in all if x.get('task_name', None) != None]\n",
    "\n",
    "sign_dict = {'red': 'negative', 'green': 'positive', 'yellow': 'negligible'}\n",
    "\n",
    "for i in range(len(all)):\n",
    "    # Some of the data is in string form, eval() is to convert it to dict\n",
    "    try:\n",
    "        all[i]['feature_division'] = eval(all[i]['feature_division'])\n",
    "    except:\n",
    "        all[i]['feature_division'] = all[i]['feature_division']\n",
    "    all[i]['feature_division']['explainable_df'] = eval(all[i]['feature_division']['explainable_df'])\n",
    "    \n",
    "    # Some of the fields we want are inside the feature_division dict, moving them to the top level\n",
    "    all[i]['values'] = [format(val, '.2f') for val in all[i]['feature_division']['explainable_df']['Values'].values()]\n",
    "    ft_nums =[re.search('F\\d*', val).group() for val in list(all[i]['feature_division']['explainable_df']['annotate_placeholder'].values())]\n",
    "    ft_names = list(all[i]['feature_division']['explainable_df']['Variable'].values())\n",
    "    all[i]['sign'] = [sign_dict[x] for x in all[i]['feature_division']['explainable_df']['Sign'].values()]\n",
    "    all[i]['narrative_id'] = all[i].pop('id')\n",
    "    all[i]['unique_id'] = i\n",
    "    all[i]['classes_dict'] = {v[0].strip(): v[1].strip() for v in [y.split(':') for y in [x for x in all[i]['prediction_confidence_level'].split(',')]]}\n",
    "    all[i]['narrative_questions'] = all[i]['narrative_question'].strip('<ul><li>/ ').split(' </li> <li> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The classifier is very uncertain about the correct label for the case given.  Regarding the classifier's decision, there is close to an even split on the probability of either of the possible labels is the correct label but the classifier chooses the label as C1. The prediction verdict above is attributed to the contributions of mainly the following features: F11, F9, F1, and F3, however, the lowest ranked features are F18, F13, and F17. Analysing the direction of influence of the features shows that there are ten positive and ten negative features.  Positive features such as F1, F3, F20, and F4 increase the response of the classifier in favour of the assigned label. Conversely, negative features such as F11, F9, F7, and F15 decrease the likelihood of C1 being the correct label given that their values support the alternative label, C2. The uncertainty concerning the label assignment can be due to the fact that the top negative features F11 and F9 have very high attributions shifting the classifier's verdict away from the C1 class.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(all)) if all[i]['narration'][:75] == \"The classifier is very uncertain about the correct label for the case given\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The classifier is very uncertain about the correct label for the case given.  Regarding the classifier's decision, there is close to an even split on the probability of either of the possible labels is the correct label but the classifier chooses the label as C1. The prediction verdict above is attributed to the contributions of mainly the following features: F11, F9, F1, and F3, however, the lowest ranked features are F18, F13, and F17. Analysing the direction of influence of the features shows that there are ten positive and ten negative features.  Positive features such as F1, F3, F20, and F4 increase the response of the classifier in favour of the assigned label. Conversely, negative features such as F11, F9, F7, and F15 decrease the likelihood of C1 being the correct label given that their values support the alternative label, C2. The uncertainty concerning the label assignment can be due to the fact that the top negative features F11 and F9 have very high attributions shifting the classifier's verdict away from the C1 class.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all[107]['narration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarize the prediction made for the test under consideration along with the likelihood of the different possible class labels.',\n",
       " 'Provide a statement summarizing the ranking of the features as shown in the feature impact plot.',\n",
       " 'Compare the direction of impact of the features: F11 and F9.',\n",
       " 'Summarize the direction of influence of the features (F1, F3, F20 and F7) with moderate impact on the prediction made for this test case.',\n",
       " 'Provide a statement on the features with the least impact on the prediction made for this test case.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all[107]['narrative_questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The classifier is very uncertain about the correct label for the case given.  Regarding the classifier's decision, there is close to an even split on the probability of either of the possible labels is the correct label but the classifier chooses the label as C2. The prediction verdict above is attributed to the contributions of mainly the following features: F4, F11, F3, and F15, however, the lowest ranked features are F7, F19, and F1. Analysing the direction of influence of the features shows that there are ten positive and ten negative features.  Positive features such as F3, F15, F8, and F20 increase the response of the classifier in favour of the assigned label. Conversely, negative features such as F4, F11, F12, and F2 decrease the likelihood of C2 being the correct label given that their values support the alternative label, C1. The uncertainty concerning the label assignment can be due to the fact that the top negative features F4 and F11 have very high attributions shifting the classifier's verdict away from the C2 class.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['narration'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarize the prediction made for the test under consideration along with the likelihood of the different possible class labels.',\n",
       " 'Provide a statement summarizing the ranking of the features as shown in the feature impact plot.',\n",
       " 'Compare the direction of impact of the features: F4 and F11.',\n",
       " 'Summarize the direction of influence of the features (F3, F15, F8 and F12) with moderate impact on the prediction made for this test case.',\n",
       " 'Provide a statement on the features with the least impact on the prediction made for this test case.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[1]['narrative_questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"The classifier is very uncertain about the correct label for the case given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f53d471a03fb5b9741311ec5f82522ec5f217d64ed47634b801d3f5199a0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

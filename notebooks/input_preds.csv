preds,input
"The classifier is 100.0% certain that the correct label for the given data instance is C1 since there is little to no chance that any of the other labels, C2 and C3, could be the true label.
The classification decision above is mainly due to the influence of features such as F40, F8, and F5.
Among these top features, four are shown to have negative attributions, shifting the verdict in a different direction.
Other positive features include F9,  F33,and F1.
On the flip side, among the top influential features mentioned above, only F4 has a negative impact, decreasing the likelihood of being the right label with respect to this case.
To sum up, it is important to note that not all the input features are considered relevant when arriving at the prediction probabilities across the classes under consideration here.
In simple terms, we can see why the model is very confident about the assigned label (C1) while the others positively support labelling the case as ""C2"".
Finally, considering the degree of uncertainty associated with the aforementioned classification conclusion, one can conclude that perhaps the least relevant feature is F7, which contributes negatively towards the assignment of an alternative label on the basis of its values.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F40, F8, F5, F22, F6, F29, F4, F45, F27, F36, F7, F35, F42, F2, F23, F21, F13, F9, F33, and F1], with values [-0.07, -0.05, 0.05, -0.05, 0.05, -0.04, -0.04, 0.04, 0.03, 0.03, 0.03, 0.03, -0.03, -0.03, -0.03, 0.03, -0.02, 0.02, 0.02, and 0.02].
Postive features are [F5, F6, F45, F27, F36, F7, F35, F21, F9, F33, and F1].
Negative features are [F40, F8, F22, F29, F4, F42, F2, F23, and F13].
Lowest impact features are [F31, F24, F34, F25, and F41] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a 91.95% confidence level, implying that there is only an 8.05% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F6, F5, and F11.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these influential features, only F26 has a negative impact, decreasing the likelihood of being the correct label for the case under consideration.
Other positive features increasing the odds of arriving at the assigned label include F21, or F20.
To cut a long story short, we can see from the attribution analysis that all the input features have varying degrees of influence on the algorithm's final labelling decision here.
In simple terms, the most relevant features considered by the model when deciding the appropriate label are F7,  F24,and F3.
As a result, it is not surprising that the classifier is very confident about the correctness of this classification output.
","Predicted class is C1, value of 91.95%.
Other classes and values are C2 8.05%.
Top features are [F6, F5, F11, F26, F25, F21, F20, F1, F15, F7, F24, F3, F14, F4, F18, F13, F16, F2, F19, and F23], with values [0.41, -0.19, 0.10, -0.06, -0.04, 0.03, 0.03, -0.03, 0.03, 0.03, 0.03, 0.03, -0.02, -0.02, 0.02, 0.01, 0.01, -0.01, -0.01, and 0.01].
Postive features are [F6, F11, F21, F20, F15, F7, F24, F3, F18, F13, F16, and F23].
Negative features are [F5, F26, F25, F1, F14, F4, F2, and F19].
Lowest impact features are [F8, F9, F22, F17, and F12] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 89.16%, implying that there is only a 10.84% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F1, F5, and F2 which are shown to have varying degrees of influence on the algorithm's output decision for this case.
On the other hand, the least important features are F7 and  F9 since their attributions are very close to zero when it comes to determining the correct label for the case under consideration.
In simple terms, we can see from the attribution analysis that all the input features have positive contributions, increasing the odds in favour of the assigned label.
","Predicted class is C1, value of 89.16%.
Other classes and values are C2 10.84%.
Top features are [F1, F5, F2, F3, F8, F6, F10, F4, F7, and F9], with values [-0.16, 0.12, 0.07, -0.05, -0.05, 0.02, -0.01, 0.01, 0.01, and 0.00].
Postive features are [F5, F2, F6, F4, F7, and F9].
Negative features are [F1, F3, F8, and F10].
Lowest impact features are [F6, F10, F4, F7, and F9] with values [0.02, -0.01, 0.01, 0.01, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level of 97.02%, implying that there is only a 2.98% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F10, F11, and F13.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
Among these top features, only F5 has a positive contribution, increasing the odds of being the correct label for the case under consideration.
In contrast, on the contrary, the least relevant features are F3,  F17, or F4.
Considering the degree of uncertainty associated with the assigned label, it is not surprising that the algorithm is very confident about the correctness of this classification output.
","Predicted class is C1, value of 97.02%.
Other classes and values are C2 2.98%.
Top features are [F10, F11, F13, F6, F5, F15, F2, F7, F1, F16, F8, F9, F3, F17, F4, F12, and F14], with values [0.38, 0.36, 0.13, 0.03, -0.02, 0.01, -0.01, -0.01, -0.01, -0.01, 0.00, -0.00, -0.00, -0.00, -0.00, -0.00, and 0.00].
Postive features are [F10, F11, F13, F6, F15, F8, and F14].
Negative features are [F5, F2, F7, F1, F16, F9, F3, F17, F4, and F12].
Lowest impact features are [F3, F17, F4, F12, and F14] with values [-0.00, -0.00, -0.00, -0.00, and 0.00]."
"The model predicts class C2 with a 51.62% chance of being correct, implying that there is a 48.38% likelihood that the true label could be C1.
The classification decision above is mainly due to the influence of features such as F4, F5, and F6.
Among these top features, only F8 has a negative impact on the final prediction made by the model for the case under consideration.
On the other hand, all the remaining features are shown to have positive attributions, increasing the odds in favour of the assigned label.
In simple terms, it is not surprising that we can see this confidence level across the two classes since their respective degrees of influence are very close to zero.
","Predicted class is C2, value of 51.62%.
Other classes and values are C1 48.38%.
Top features are [F4, F5, F6, F9, F10, F3, F1, F7, F2, and F8], with values [-0.10, 0.06, 0.01, -0.01, 0.01, 0.01, 0.01, 0.01, 0.00, and -0.00].
Postive features are [F5, F6, F10, F3, F1, F7, and F2].
Negative features are [F4, F9, and F8].
Lowest impact features are [F3, F1, F7, F2, and F8] with values [0.01, 0.01, 0.01, 0.00, and -0.00]."
"The classification algorithm labels the given case as C3 with a confidence level equal to 82.81%.
This means that there is about a 15.52% chance that it could be C2, 0.08% and 1.60%, respectively.
The uncertainty associated with this classification decision can be attributed to the negative contributions of F2 and F1.
However, considering the direction of influence of each input feature, it is not surprising that the algorithm is very confident in the correctness of the assigned label.
From the attribution analysis, all the features are shown to have positive attributions, increasing the odds of being the correct label for the case under consideration.
On the other hand, the least important variables are F4, F3,  F5, and finally F6.
To sum up, we can see from the prediction probabilities across the classes that their respective degrees of impact are either moderate or low.
","Predicted class is C3, value of 82.81%.
Other classes and values are C2 0.08%& C1 1.60%& C4 15.52%.
Top features are [F1, F4, F3, F5, F2, and F6], with values [0.23, 0.12, 0.04, 0.04, -0.01, and 0.00].
Postive features are [F1, F4, F3, F5, and F6].
Negative features are [F2].
Lowest impact features are [F4, F3, F5, F2, and F6] with values [0.12, 0.04, 0.04, -0.01, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 97.82%.
This implies that there is only a 2.18% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F3, F4, and F7.
On the other hand, the least relevant features are F11,  F5,and F14.
Looking at the attributions of the input features, we can see that their respective degrees of influence are very close to zero when it comes to predicting the correct label for the case under consideration.
Among the top positive features considered by the algorithm in this case, four are shown to have negative contributions, shifting the verdict in the opposite direction favouring the assigned label.
In simple terms, all the remaining ones positively contribute towards the model's labelling choice here.
","Predicted class is C2, value of 97.82%.
Other classes and values are C1 2.18%.
Top features are [F3, F4, F7, F12, F10, F2, F11, F6, F13, F9, F5, F8, F14, and F1], with values [0.34, 0.13, 0.07, 0.07, 0.06, 0.05, -0.04, 0.04, -0.02, 0.02, -0.01, 0.01, -0.00, and 0.00].
Postive features are [F3, F4, F7, F12, F10, F2, F6, F9, F8, and F1].
Negative features are [F11, F13, F5, and F14].
Lowest impact features are [F9, F5, F8, F14, and F1] with values [0.02, -0.01, 0.01, -0.00, and 0.00]."
"The classification algorithm labels the given case as C3 with a confidence level of 68.12%, implying that there is only a 2.01% chance that it could be C2 and 29.87%.
The above prediction decision is mainly due to the influence of features such as F10, F8, and F9.
On the other hand, the remaining features are shown to have negative attributions, increasing the odds of the label C1 being the correct label for the case under consideration.
In simple terms, these positive features can be ranked in order of their respective direction of impact on the algorithm's labelling output: F4,  F6, or F7.
However, considering the predicted likelihoods across the classes, it is valid to conclude that the model is very uncertain about which label is appropriate for this case.
","Predicted class is C3, value of 68.12%.
Other classes and values are C2 2.01%& C1 29.87%.
Top features are [F10, F8, F9, F2, F3, F7, F11, F5, F12, F1, F4, and F6], with values [0.08, 0.07, -0.05, -0.04, 0.04, -0.03, 0.03, 0.01, 0.01, -0.01, 0.00, and 0.00].
Postive features are [F10, F8, F3, F11, F5, F12, F4, and F6].
Negative features are [F9, F2, F7, and F1].
Lowest impact features are [F5, F12, F1, F4, and F6] with values [0.01, 0.01, -0.01, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with an 80.0% confidence level, implying that there is about a 20.00% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F27, F15, and F2 which are shown to have varying degrees of influence on the algorithm's output decision for this case.
Among these influential features, only F5 has a negative impact, shifting the verdict in the opposite direction towards either of the other two classes.
Other positive features increasing the likelihood of labelling the provided case under consideration are F21, or F7.
In contrast, the top negative features decreasing the odds of arriving at the assigned label include F9,  F6,and F36.
Considering the attributions of all the input features mentioned above, it can be concluded that the very strong positive contribution of F14 outweighs the negative ones; therefore, we can conclude that they are not relevant when determining the correct label here.
Finally, considering the degree of uncertainty associated with the predicted probabilities across the three classes, perhaps it is appropriate to take into account the values of each feature.
","Predicted class is C1, value of 80.00%.
Other classes and values are C2 20.00%.
Top features are [F27, F15, F2, F21, F7, F9, F5, F38, F24, F6, F37, F36, F39, F43, F1, F11, F34, F25, F20, and F8], with values [-0.08, 0.06, -0.05, 0.05, 0.04, -0.04, 0.03, 0.03, 0.03, -0.03, 0.03, -0.02, 0.02, 0.02, 0.02, -0.02, 0.02, 0.02, 0.02, and 0.02].
Postive features are [F15, F21, F7, F5, F38, F24, F37, F39, F43, F1, F34, F25, F20, and F8].
Negative features are [F27, F2, F9, F6, F36, and F11].
Lowest impact features are [F35, F22, F16, F26, and F42] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with an 83.0% confidence level, implying that there is only a small chance that it could be C3 or C2.
The above prediction decision is mainly due to the influence of features such as F2, F11, and F10.
On the other hand, the least important features are F6, which has a very low impact on the algorithm's labelling decision for this case.
Among the top influential features, seven have negative attributions, shifting the verdict in the direction of either of the remaining two classes.
However, compared to all the positive features mentioned above, we can conclude that the model is not 100 percent certain about the correct label for the case under consideration since their contributions decrease the likelihood of C4 being the proper label.
In summary, looking at the attribution probabilities across the classes, it can be concluded that each feature contributes positively towards the assigned label while the others contribute negatively.
","Predicted class is C1, value of 83.00%.
Other classes and values are C3 3.00%& C2 14.00%.
Top features are [F2, F11, F10, F1, F7, F9, F3, F5, F12, F8, F4, and F6], with values [0.21, -0.02, -0.02, -0.02, 0.02, 0.01, -0.01, 0.01, 0.01, 0.01, 0.00, and 0.00].
Postive features are [F2, F7, F9, F5, F12, F8, F4, and F6].
Negative features are [F11, F10, F1, and F3].
Lowest impact features are [F5, F12, F8, F4, and F6] with values [0.01, 0.01, 0.01, 0.00, and 0.00]."
"The classifier is 100.0% certain that the correct label for the given data instance is C2 since there is little to no chance that either of the other labels, C1 and C3, could be the true label.
The classification decision above is mainly due to the influence of features such as F5, F6, and F9.
Among these top features, only F11 has a negative contribution, shifting the prediction verdict in the opposite direction.
Other positive features increasing the odds of labelling the case as ""C2"" include F8, or F7.
Overall, it is important to note that all the input features are shown to have varying degrees of influence on the model's output decision with respect to this case.
In simple terms, we can see why the confidence level is very high when you look at the predicted probabilities across the classes.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%& C3 0.00%.
Top features are [F5, F6, F9, F2, F11, F10, F8, F7, F4, F3, F12, and F1], with values [0.23, 0.19, 0.17, 0.06, -0.06, 0.05, 0.04, 0.01, 0.01, -0.01, -0.01, and 0.00].
Postive features are [F5, F6, F9, F2, F10, F8, F7, F4, and F1].
Negative features are [F11, F3, and F12].
Lowest impact features are [F7, F4, F3, F12, and F1] with values [0.01, 0.01, -0.01, -0.01, and 0.00]."
"The classification algorithm labels the given case as C1 since there is a 40.0% chance that it could be C2.
F11, F3, and F6 are the most influential features resulting in the above-mentioned classification decision.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in favour of an alternative label.
Among these negative features, only F5 has a positive impact, increasing the odds of being the correct label for this case.
The next set of features with moderate to low influence on the algorithm's prediction output are F4,  F10, or F9.
Considering the direction of influence of each input feature, it is not surprising that the confidence level across the two classes is about 60.00%.
In summary, we can see from the attribution analysis that all the relevant features positively contribute to the labelling choice here.
","Predicted class is C1, value of 60.00%.
Other classes and values are C2 40.00%.
Top features are [F11, F3, F6, F1, F7, F8, F4, F2, F10, F9, and F5], with values [0.26, -0.09, 0.07, 0.07, 0.05, -0.04, -0.03, 0.03, -0.03, -0.03, and 0.00].
Postive features are [F11, F6, F1, F7, F2, and F5].
Negative features are [F3, F8, F4, F10, and F9].
Lowest impact features are [F4, F2, F10, F9, and F5] with values [-0.03, 0.03, -0.03, -0.03, and 0.00]."
"The classifier is 100.0% certain that the correct label for the given data instance is C2 since there is little to no chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F27, F1, and F29.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in favour of a different label.
These include F13,  F18,and F17.
Among the top influential features, only F26 has a negative impact, decreasing the likelihood of being the true label with respect to this case.
Other notable positive features increasing the odds of labelling the case as C3 are F28, or F12.
In contrast, the marginal uncertainty in the classification here can be blamed on the fact that all the input features have values that contradict the prediction made here.
To cut a long story short, we can take into consideration the very high confidence level associated with the assigned label (C2), however, considering the direction of effect of each input feature, it is valid to conclude that not all relevant features contribute positively towards the model's output decision.
This might explain why the most important features considered when deciding the appropriate label should be referred to as ""C1"".
Finally, looking at the predicted probabilities across the classes mentioned above, you can see that they are quite close to zero.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F27, F1, F29, F12, F22, F20, F8, F15, F10, F3, F2, F5, F7, F28, F23, F11, F14, F21, F30, and F19], with values [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, and 0.01].
Postive features are [F27, F1, F29, F12, F22, F20, F8, F15, F10, F3, F2, F5, F7, F28, F23, F11, F14, F21, F30, and F19].
Negative features are [ ].
Lowest impact features are [F26, F13, F18, F17, and F24] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classifier is 99.45% certain that the correct label for the given case is C3, implying that there is little to no chance that any of the other labels could be the true label.
F2, F16, and F11 are the most influential variables resulting in the classification verdict above.
The remaining variables have a moderate to low influence on the final labelling decision with respect to the case under consideration.
Among these top features, only F1 has a negative contribution, decreasing the likelihood of being the right label here.
Other positive variables increasing the odds of this prediction include F12, or F10.
To cut a long story short, it is important to take into account the direction of influence of each input feature as shown by the predicted probabilities across the two classes: C1 and C2.
As a result, we can see from the attributions analysis that not all the relevant features contribute positively towards the assigned label since their respective degrees of impact are very close to zero.
","Predicted class is C3, value of 99.45%.
Other classes and values are C1 0.47%& C2 0.04%& C4 0.05%.
Top features are [F2, F16, F11, F12, F10, F9, F8, F14, F15, F3, F7, F17, F19, F20, F18, F5, F4, F13, F6, and F1], with values [0.78, 0.11, -0.10, -0.07, 0.04, -0.04, 0.03, -0.03, 0.03, -0.02, -0.02, -0.02, 0.01, -0.01, -0.01, -0.01, 0.01, -0.00, -0.00, and -0.00].
Postive features are [F2, F16, F10, F8, F15, F19, and F4].
Negative features are [F11, F12, F9, F14, F3, F7, F17, F20, F18, F5, F13, F6, and F1].
Lowest impact features are [F5, F4, F13, F6, and F1] with values [-0.01, 0.01, -0.00, -0.00, and -0.00]."
"The classification algorithm labels the given case as C1 with a confidence level of 97.94%, implying that there is only a 2.06% chance that it could be C2.
The most important features contributing to the above prediction are F38, F19, and F46.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
These include F43,  F5,and F37.
Among the top influential features considered by the algorithm for this labelling assignment, only F40 has a negative impact, increasing the odds of being the correct label.
Other positive features driving the decision higher towards an alternative label included F31, M42, or F39.
In contrast, on the flip side, the least relevant features are shown to be F17, which can be blamed for the uncertainty associated with the assigned label here.
Finally, considering the predicted likelihoods across the classes, it is valid to conclude that the very strong positive influence of F33 outweighs the negative ones; hence, we can take into account the fact that not all the input features contribute positively when classifying the case under consideration.
","Predicted class is C1, value of 97.94%.
Other classes and values are C2 2.06%.
Top features are [F38, F19, F46, F40, F17, F31, F42, F43, F5, F37, F39, F28, F7, F1, F10, F16, F3, F25, F6, and F27], with values [0.17, 0.14, -0.14, 0.13, -0.12, 0.11, 0.10, -0.10, -0.08, 0.07, -0.06, -0.06, 0.06, 0.06, 0.06, -0.05, 0.05, 0.05, -0.05, and 0.04].
Postive features are [F38, F19, F40, F31, F42, F37, F7, F1, F10, F3, F25, and F27].
Negative features are [F46, F17, F43, F5, F39, F28, F16, and F6].
Lowest impact features are [F15, F12, F2, F24, and F11] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model predicts class C2 with a confidence level equal to 88.0%.
This means that there is only a marginal chance that either of the other labels, C3 and C1, could be the correct label for the case under consideration.
The features with the most influence on the above classification are F11, F9, and F3.
However, it is important to note that all the remaining features have negative attributions, shifting the prediction verdict in the opposite direction.
Positive features such as F7,  F6, or F12 increase the odds of C 2 being the right label.
On the contrary, the top positive features increasing the likelihood of labelling the given case as ‘C2’ include F8, which has a very strong positive contribution, pushing the decision higher towards C4.
In simple terms, we can see why the model is quite confident about the assigned label since their respective degrees of influence are close to zero.
","Predicted class is C2, value of 88.00%.
Other classes and values are C3 2.00%& C1 10.00%.
Top features are [F11, F9, F3, F5, F7, F6, F1, F12, F8, F4, F10, and F2], with values [0.21, -0.02, -0.02, -0.02, 0.02, 0.01, -0.01, 0.01, 0.01, 0.01, 0.00, and 0.00].
Postive features are [F11, F7, F6, F12, F8, F4, F10, and F2].
Negative features are [F9, F3, F5, and F1].
Lowest impact features are [F12, F8, F4, F10, and F2] with values [0.01, 0.01, 0.01, 0.00, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level of 93.32%, implying that there is only a 6.68% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F17, F26, and F25.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
These include F7,  F13,and F19.
Among the top influential features considered by the algorithm for this labelling assignment, only F10 has a negative contribution, decreasing the likelihood of class C3 being the correct label here.
Other positive features increasing the odds of predicting an alternative label are F21, however, these can be regarded as less important when determining the proper label for the case under consideration.
Finally, considering the predicted likelihoods across the classes, it is not unexpected that the model is confident about the assigned label.
In summary, we can conclude from the attribution analysis that all the input features had varying degrees of impact, ranging from moderately low to very high.
","Predicted class is C2, value of 93.32%.
Other classes and values are C1 6.68%.
Top features are [F17, F26, F25, F2, F10, F32, F21, F3, F9, F23, F7, F13, F19, F14, F18, F24, F20, F27, F4, and F15], with values [0.20, 0.03, -0.03, 0.03, -0.03, -0.03, -0.03, 0.02, 0.02, 0.02, 0.02, -0.02, 0.02, -0.02, -0.01, -0.01, -0.01, -0.01, 0.01, and 0.01].
Postive features are [F17, F26, F2, F3, F9, F23, F7, F19, F4, and F15].
Negative features are [F25, F10, F32, F21, F13, F14, F18, F24, F20, and F27].
Lowest impact features are [F22, F16, F29, F8, and F33] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model is 75.63% certain that the correct label for the given data instance is C3, however, there is a 5.86% chance that it could be C2 and 18.51%.
The classification decision above is mainly due to the influence of features such as F1, F7, and F5.
On the other hand, the least important features are F8 and  F4.
Examining the attributions of the input features revealed that only four of them have negative contributions, shifting the verdict in favour of labelling the case as C1.
Among the top influential features considered by the model with respect to this case, only F9 has a negative contribution, increasing the odds of being the true label here.
However, looking at the direction of effect of each feature, we can see that all the remaining ones positively contribute towards the assigned label which explains why the confidence level across the classes is very high.
In simple terms, these positive features outweigh the negative ones, hence they are deemed less relevant when determining the appropriate label based on the attribution analysis.
","Predicted class is C3, value of 75.63%.
Other classes and values are C2 5.86%& C1 18.51%.
Top features are [F1, F7, F5, F10, F9, F6, F12, F2, F3, F11, F8, and F4], with values [0.41, -0.10, -0.06, 0.05, 0.05, 0.04, 0.03, -0.02, 0.02, 0.02, 0.02, and -0.00].
Postive features are [F1, F10, F9, F6, F12, F3, F11, and F8].
Negative features are [F7, F5, F2, and F4].
Lowest impact features are [F2, F3, F11, F8, and F4] with values [-0.02, 0.02, 0.02, 0.02, and -0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 94.37%.
This means that there is only a 5.63% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F4, F6, and F7.
On the other hand, the least important features are F10, which has a negative impact on the algorithm's labelling decision for this case.
However, unlike all the remaining features mentioned above, these ones have positive attributions, increasing the odds in favour of the assigned label.
Finally, considering the direction of influence or contribution of each input feature, it is not unexpected that the model is very confident about the correct label for the case under consideration.
","Predicted class is C2, value of 94.37%.
Other classes and values are C1 5.63%.
Top features are [F4, F6, F7, F11, F10, F2, F8, F9, F1, F3, and F5], with values [0.10, 0.09, 0.07, 0.05, -0.04, 0.02, 0.02, -0.02, -0.01, -0.01, and 0.00].
Postive features are [F4, F6, F7, F11, F2, F8, and F5].
Negative features are [F10, F9, F1, and F3].
Lowest impact features are [F8, F9, F1, F3, and F5] with values [0.02, -0.02, -0.01, -0.01, and 0.00]."
"The model is 100.0% certain that C2 is the correct label for the case under consideration since there is little to no chance that it is any of the other labels.
F7, F10, and F9 are the most influential variables resulting in the classification decision above.
The least relevant variables are F6,  F2, or F5, with a very low impact on the final labelling decision made by the model.
Among the top positive variables, only F11 has a negative contribution, increasing the odds of C1 being the right label.
On the flip side, features such as F12, but F4 have a positive influence, shifting the verdict in favour of an alternative label (C2).
In summary, we can see from the attributions analysis that all the input variables positively contribute to the assigned label which explains why the confidence level across the classes is quite high.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F7, F10, F9, F1, F4, F11, F12, F8, F6, F2, F5, F3, and F13], with values [0.38, 0.30, -0.27, 0.26, 0.16, -0.14, 0.11, 0.07, 0.07, -0.07, 0.06, 0.03, and 0.01].
Postive features are [F7, F10, F1, F4, F12, F8, F6, F5, F3, and F13].
Negative features are [F9, F11, and F2].
Lowest impact features are [F6, F2, F5, F3, and F13] with values [0.07, -0.07, 0.06, 0.03, and 0.01]."
"The classification algorithm labels the given case as C2 with a confidence level of 58.75%, implying that there is only a 41.25% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F14, F8, and F1 which are ranked according to their respective degrees of impact on the algorithm's output label choice for this case.
Among these top features, only F12 has negative contributions, shifting the verdict in favour of an alternative label.
On the other hand, all the remaining features have positive attributions, increasing the odds of being the correct label for the case under consideration.
To sum up, we can see from the attribution analysis that not all of the input features contribute positively towards the assigned label, therefore, it is not surprising that the model is very confident about the correctness of its labelling decision here.
","Predicted class is C2, value of 58.75%.
Other classes and values are C1 41.25%.
Top features are [F14, F8, F1, F12, F4, F2, F6, F11, F3, F9, F13, F10, F15, F16, F5, and F7], with values [0.08, 0.03, 0.02, -0.02, -0.02, 0.02, 0.02, 0.01, -0.01, -0.01, -0.01, -0.01, -0.00, -0.00, 0.00, and -0.00].
Postive features are [F14, F8, F1, F2, F6, F11, and F5].
Negative features are [F12, F4, F3, F9, F13, F10, F15, F16, and F7].
Lowest impact features are [F10, F15, F16, F5, and F7] with values [-0.01, -0.00, -0.00, 0.00, and -0.00]."
"The classifier is 90.65% certain that the correct label for the given data instance is C1 since there is only a 9.35% chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F1, F2, and F3.
On the other hand, the least important features are F7, which has a negative impact on the prediction made here.
However, unlike all the input features mentioned above, these ones have positive attributions, increasing the likelihood of the assigned label.
In summary, we can see from the attribution analysis that each feature contributes positively towards the model's labelling decision with respect to this case under consideration.
Among the top influential features, four are shown to have negative contributions, shifting the verdict in the opposite direction, favouring the assignment of an alternative label (C2).
Finally, considering the predicted likelihoods across the classes, it is valid to conclude that they are very close to zero when it comes to predicting the appropriate label among the three possible classes.
","Predicted class is C1, value of 90.65%.
Other classes and values are C2 9.35%.
Top features are [F1, F2, F3, F4, F6, F7, F5, and F8], with values [0.15, 0.13, 0.13, 0.08, 0.08, -0.02, -0.02, and -0.01].
Postive features are [F1, F2, F3, F4, and F6].
Negative features are [F7, F5, and F8].
Lowest impact features are [F4, F6, F7, F5, and F8] with values [0.08, 0.08, -0.02, -0.02, and -0.01]."
"The classification algorithm labels the given case as C1 with an 80.35% confidence level, implying that there is only a 19.65% chance that it could be C2.
The most important features resulting in the above prediction decision are F15, F2, and F6.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in favour of another label.
These include F11,  F10,and F3.
However, unlike all the top features, these ones have very little to no impact on the algorithm when determining the correct label for the case under consideration.
In addition, the values of F16, or F9 can be ranked according to the direction of influence of each input feature since their contributions increase the odds of being the true label here.
As a result, we can see from the attribution analysis that the positive features outweigh the negative ones by about 0.05%, hence they are deemed less relevant when classifying this case.
Finally, considering the degree of uncertainty associated with the assigned label, it is valid to conclude that neither F5 nor F19 has a negative contribution towards the final labelling decision among the different classes.
","Predicted class is C1, value of 80.35%.
Other classes and values are C2 19.65%.
Top features are [F15, F2, F6, F11, F10, F3, F19, F16, F1, F9, F8, F5, F13, F4, F7, F12, F14, F17, and F18], with values [0.18, -0.18, 0.10, -0.08, 0.08, 0.07, 0.06, -0.06, 0.04, -0.04, -0.04, 0.03, 0.03, 0.02, -0.02, 0.01, 0.01, -0.01, and 0.01].
Postive features are [F15, F6, F10, F3, F19, F1, F5, F13, F4, F12, F14, and F18].
Negative features are [F2, F11, F16, F9, F8, F7, and F17].
Lowest impact features are [F7, F12, F14, F17, and F18] with values [-0.02, 0.01, 0.01, -0.01, and 0.01]."
"The classifier is 99.30% certain that the correct label for the given case is C2, implying that there is only a 0.70% chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F9, F5, and F10.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these top features, only F4 has a negative impact, increasing the odds of being the true label since their values favour labelling the case differently.
Other positive features with respect to this prediction include F6,  F3,and F13.
However, considering the fact that all the input features have different degrees of influence, it is valid to conclude that they are less relevant when deciding the appropriate label here.
","Predicted class is C2, value of 99.30%.
Other classes and values are C1 0.70%.
Top features are [F9, F5, F10, F4, F6, F3, F15, F13, F7, F16, F8, F1, F2, F11, F14, F17, and F12], with values [0.37, -0.35, 0.13, 0.03, 0.02, 0.01, -0.01, 0.01, 0.01, 0.00, 0.00, -0.00, 0.00, 0.00, 0.00, 0.00, and -0.00].
Postive features are [F9, F10, F4, F6, F3, F13, F7, F16, F8, F2, F11, F14, and F17].
Negative features are [F5, F15, F1, and F12].
Lowest impact features are [F2, F11, F14, F17, and F12] with values [0.00, 0.00, 0.00, 0.00, and -0.00]."
"The classification algorithm labels the given case as C2 with a 64.11% likelihood, implying that there is a 35.89% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F11, F5, and F6.
On the other hand, all the remaining features are shown to have varying degrees of impact on the algorithm's output decision for this case.
Among these top features, only F12 has negative attributions, shifting the verdict in the opposite direction towards one of the least probable classes.
Other positive features increasing the odds of C3 being the correct label include F9, or F7.
To cut a long story short, the marginal uncertainty associated with the assigned label can be explained away by looking at the attribution analysis performed to understand the degree of effect of each input feature.
In summary, we can conclude that the confidence level across the two classes is not 100.0%.
","Predicted class is C2, value of 64.11%.
Other classes and values are C1 35.89%.
Top features are [F11, F5, F6, F12, F10, F1, F8, F13, F4, F3, F9, F7, and F2], with values [0.14, 0.08, -0.07, 0.07, -0.04, 0.03, 0.02, 0.02, 0.02, -0.01, 0.01, -0.01, and -0.00].
Postive features are [F11, F5, F12, F1, F8, F13, F4, and F9].
Negative features are [F6, F10, F3, F7, and F2].
Lowest impact features are [F4, F3, F9, F7, and F2] with values [0.02, -0.01, 0.01, -0.01, and -0.00]."
"The classifier is 100.0% certain that the correct label for the given case is C2, implying that there is little to no chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F1, F10, and F5.
On the other hand, the remaining features are shown to have negative attributions, shifting the verdict in a different direction.
This can be blamed on the fact that some of the input features have values that contradict the prediction made here.
Among the top positive features with respect to this classification instance, only F11 has a negative impact, increasing the odds of labelling the case as C3.
However, considering the predicted likelihoods across the classes, it is valid to conclude that they are very close to zero when it comes to predicting the true label under consideration.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F1, F10, F5, F2, F6, F4, F3, F7, F8, F9, and F11], with values [0.09, -0.05, -0.03, -0.02, -0.02, -0.01, -0.01, 0.01, -0.00, 0.00, and 0.00].
Postive features are [F1, F7, F9, and F11].
Negative features are [F10, F5, F2, F6, F4, F3, and F8].
Lowest impact features are [F3, F7, F8, F9, and F11] with values [-0.01, 0.01, -0.00, 0.00, and 0.00]."
"The classifier is 99.0% certain that the correct label for the given data instance is C2, implying that there is only a marginal chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F29, F9, and F88.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
These include F53,  F17,and F77.
However, unlike all the top features mentioned above, these ones have very little impact on the final labelling decision with respect to this case.
To sum up, we can see from the predicted likelihoods across the classes that each input feature has a different degree of effect or contribution towards the prediction made here.
Among the influential features ranked according to their respective degrees of influence (i.e., those with the most positive contributions), F23, M44,F69,, F31, finally F27, is identified as the least relevant since its values contradict the assigned label.
In summary, when you take into consideration the attribution analysis, it is obvious why the model is very confident about the chosen label: It is important to note that neither F30 nor F3 supports the assignment of an alternative label, hence they are deemed less likely than the others.
","Predicted class is C2, value of 99.00%.
Other classes and values are C1 1.00%.
Top features are [F29, F9, F88, F53, F17, F77, F30, F3, F7, F85, F20, F81, F26, F44, F69, F31, F1, F27, F89, and F8], with values [0.01, 0.01, 0.01, -0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, -0.00, -0.00, -0.00, 0.00, -0.00, 0.00, and 0.00].
Postive features are [F29, F9, F88, F17, F77, F30, F3, F7, F85, F20, F81, F26, F1, F89, and F8].
Negative features are [F53, F44, F69, F31, and F27].
Lowest impact features are [F41, F40, F72, F79, and F75] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model predicts class C2 with 100.0% certainty, implying that there is little to no chance that the true label for the given case is C1.
The above prediction decision is mainly due to the influence of features such as F7, F8, and F3.
On the other hand, all the remaining features are shown to have negative attributions, shifting the verdict in a different direction.
Among these positive features, only F9 has a negative contribution, increasing the odds of being the correct label here.
Overall, it is important to take into consideration why the model is very certain about the correctness of the assigned label.
In simple terms, we can see from the predicted likelihoods across the classes that each input feature contributes positively or negatively towards the assignment of an alternative label since their contributions favour labelling the case differently.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F7, F8, F3, F2, F9, F4, F5, F6, and F1], with values [0.62, -0.04, -0.02, 0.02, 0.01, -0.01, 0.00, -0.00, and -0.00].
Postive features are [F7, F2, F9, and F5].
Negative features are [F8, F3, F4, F6, and F1].
Lowest impact features are [F9, F4, F5, F6, and F1] with values [0.01, -0.01, 0.00, -0.00, and -0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 96.77%.
This means that there is only a 3.23% chance that it could be C1.
F19, F11, and F6 are the most important variables resulting in the prediction probabilities across the two classes.
The remaining variables have varying degrees of influence on the decision made by the algorithm for this case.
Among these influential variables, four are shown to have negative attributions, shifting the verdict in favour of an alternative label (C1).
On the other hand, some of the top positive variables positively contribute to the classifier's final labelling decision here.
These include F16,  F2,and F22.
In addition, features such as F8 has values that increase the odds of being the correct label for the case under consideration.
Finally, considering the direction of effect of each input variable, it is not surprising that the model is very confident about the assigned label.
","Predicted class is C2, value of 96.77%.
Other classes and values are C1 3.23%.
Top features are [F19, F11, F6, F16, F26, F2, F22, F8, F1, F9, F13, F18, F28, F25, F5, F15, F14, F21, F12, and F3], with values [0.16, -0.09, -0.09, 0.07, -0.05, 0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, -0.02, -0.01, 0.01, 0.01, and -0.01].
Postive features are [F19, F16, F2, F22, F8, F1, F9, F13, F18, F28, F25, F5, F21, and F12].
Negative features are [F11, F6, F26, F15, F14, and F3].
Lowest impact features are [F30, F7, F23, F29, and F17] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 57.83%, implying that there is about a 42.17% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F22, F24, and F2.
Among these top features, only F30 has negative attributions, shifting the verdict in the opposite direction.
On the other hand, the values of F6 are considered positive by the algorithm when determining the appropriate label for the case under consideration since their contributions increase the likelihood of being correct.
Other positive features increasing the odds of C3 being the correct label include F19,  F8,and F21.
To cut a long story short, we can see that not all of the input features contribute positively towards the assigned label, hence they are shown to have very little impact on the final labelling decision here.
Finally, considering the predicted likelihoods across the classes, it is valid to conclude that the uncertainty associated with the classifier's output decision may be attributed to some combination of negative features resulting in less than 50.0% certainty in its correctness.
","Predicted class is C2, value of 57.83%.
Other classes and values are C1 42.17%.
Top features are [F22, F24, F2, F30, F1, F6, F17, F19, F7, F3, F8, F20, F4, F9, F26, F13, F28, F14, F21, and F10], with values [0.12, 0.07, 0.07, -0.05, 0.04, -0.04, -0.03, 0.03, -0.02, -0.02, 0.02, -0.02, -0.02, -0.02, -0.02, -0.01, -0.01, -0.01, 0.01, and -0.01].
Postive features are [F22, F24, F2, F1, F19, F8, and F21].
Negative features are [F30, F6, F17, F7, F3, F20, F4, F9, F26, F13, F28, F14, and F10].
Lowest impact features are [F15, F23, F25, F12, and F16] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 96.35%, implying that there is only a 3.65% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F2, F18, and F19.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these top features, only F11 has a positive impact, increasing the odds of being the correct label for the case under consideration.
Other influential features include F14,  F4,and F13.
However, unlike all the input features mentioned above, we can conclude that the uncertainty associated with the assigned label may explain why the algorithm is not 100.0% certain about the correctness of this classification output.
Finally, considering the predicted likelihoods across the classes, it is reasonable to deduce that neither F3 nor F15 is the true label here.
","Predicted class is C1, value of 96.35%.
Other classes and values are C2 3.65%.
Top features are [F2, F18, F19, F11, F14, F8, F4, F7, F16, F13, F20, F9, F17, F12, F6, F5, F10, F1, F3, and F15], with values [0.44, 0.29, 0.08, -0.04, -0.03, 0.03, -0.03, 0.03, 0.03, -0.02, -0.02, -0.02, 0.02, -0.02, 0.02, -0.01, 0.00, -0.00, 0.00, and -0.00].
Postive features are [F2, F18, F19, F8, F7, F16, F17, F6, F10, and F3].
Negative features are [F11, F14, F4, F13, F20, F9, F12, F5, F1, and F15].
Lowest impact features are [F5, F10, F1, F3, and F15] with values [-0.01, 0.00, -0.00, 0.00, and -0.00]."
"The model is confident that the correct label for the given case is C2 since there is only a 7.0% chance that it could be C1.
The features with the most influence on the above classification are F11, F2, and F9.
On the other hand, some of the remaining features have negative attributions, shifting the prediction verdict in the opposite direction.
Among these negative features, only F4 has a positive impact, increasing the odds of being the true label among the three possible classes.
However, considering the direction of influence of each input feature, it can be concluded that not all the features positively contribute to the decision made by the model regarding the case under consideration.
In summary, we can see from the predicted probabilities across the two classes that they are very close to zero when it comes to predicting the appropriate label here.
","Predicted class is C2, value of 93.00%.
Other classes and values are C1 7.00%.
Top features are [F11, F2, F9, F6, F3, F1, F4, F5, F8, F7, and F10], with values [0.10, -0.02, 0.01, -0.01, 0.01, 0.01, -0.00, -0.00, -0.00, -0.00, and -0.00].
Postive features are [F11, F9, F3, and F1].
Negative features are [F2, F6, F4, F5, F8, F7, and F10].
Lowest impact features are [F4, F5, F8, F7, and F10] with values [-0.00, -0.00, -0.00, -0.00, and -0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 72.93%.
This implies that there is a 27.07% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F4, F3, and F9.
Among these top features, only F8 has negative attributions, decreasing the likelihood of being the correct label for this case.
On the other hand, all the remaining features are shown to have positive contributions, increasing the response of the algorithm in favour of labelling the case differently.
In summary, we can see from the predicted probabilities across the classes that the model is very confident about the assigned label here.
","Predicted class is C2, value of 72.93%.
Other classes and values are C1 27.07%.
Top features are [F4, F3, F9, F8, F5, F1, F2, F7, and F6], with values [0.13, -0.05, -0.05, -0.05, 0.03, 0.02, 0.01, 0.00, and 0.00].
Postive features are [F4, F5, F1, F2, F7, and F6].
Negative features are [F3, F9, and F8].
Lowest impact features are [F5, F1, F2, F7, and F6] with values [0.03, 0.02, 0.01, 0.00, and 0.00]."
"The classifier is 51.60% certain that the correct label for the given data instance is C1, implying that there is a 48.40% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F21, F8, and F26.
On the other hand, some of the input features have negative attributions, shifting the classification verdict in the opposite direction.
These include F11,  F32, or F14.
Among the top influential features, only F7 has a negative impact, decreasing the likelihood of labelling the case as ""C2"".
Other positive features increasing the odds of this being the true label are F13, Analysing the contributions of all the remaining features showed that they had little effect on the predicted probabilities across the two classes.
In summary, we can see from the attribution analysis that not all relevant features contribute positively towards the assigned label, which explains why the model's confidence level is moderately high.
Finally, considering the degree of uncertainty associated with the aforementioned classification conclusion, it is valid to conclude that neither F3 nor F25 contradict the output decision made here.
","Predicted class is C1, value of 51.60%.
Other classes and values are C2 48.40%.
Top features are [F21, F8, F26, F11, F32, F14, F18, F13, F30, F3, F2, F25, F20, F15, F31, F24, F6, F22, F28, and F33], with values [-0.45, 0.10, -0.09, 0.07, -0.06, 0.06, -0.05, 0.05, 0.05, 0.05, -0.04, 0.04, 0.03, -0.03, 0.03, 0.03, -0.03, -0.02, -0.02, and 0.02].
Postive features are [F8, F11, F14, F13, F30, F3, F25, F20, F31, F24, and F33].
Negative features are [F21, F26, F32, F18, F2, F15, F6, F22, and F28].
Lowest impact features are [F23, F5, F1, F16, and F7] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 98.21%.
This means that there is only a 1.79% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F2, F5, and F8.
On the other hand, the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these positive features, only F4 has a negative contribution, increasing the odds of C3 being the correct label for the case under consideration.
To sum up, we can see why the algorithm is very confident about the correctness of the assigned label here.
","Predicted class is C2, value of 98.21%.
Other classes and values are C1 1.79%.
Top features are [F2, F5, F8, F7, F10, F9, F4, F11, F1, F12, F3, and F6], with values [0.33, 0.32, 0.20, 0.14, 0.13, 0.09, -0.06, -0.04, -0.03, -0.03, -0.01, and 0.01].
Postive features are [F2, F5, F8, F7, F10, F9, and F6].
Negative features are [F4, F11, F1, F12, and F3].
Lowest impact features are [F11, F1, F12, F3, and F6] with values [-0.04, -0.03, -0.03, -0.01, and 0.01]."
"The model is 91.30% certain that the correct label for the given case is C2, implying that there is only a 9.70% chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F1, F3, and F4.
On the other hand, all the remaining features are shown to have negative attributions, increasing the odds of the prediction being made by the model in favour of labelling the case differently.
Among the input features, only F5 has a negative attribution, shifting the verdict in the opposite direction towards an alternative label.
In simple terms, we can see from the predicted likelihoods across the classes that neither F7 nor F6 contradict the assigned label with respect to this case.
","Predicted class is C2, value of 91.30%.
Other classes and values are C1 9.70%.
Top features are [F1, F3, F4, F7, F2, F6, and F5], with values [0.22, -0.22, 0.19, 0.18, -0.05, 0.04, and 0.03].
Postive features are [F1, F4, F7, F6, and F5].
Negative features are [F3 and F2].
Lowest impact features are [F4, F7, F2, F6, and F5] with values [0.19, 0.18, -0.05, 0.04, and 0.03]."
"The classifier is 100.0% certain that C2 is the correct label for the case under consideration since the probability of C1 being the right label is only 0.00%.
The classification decision above is mainly due to the influence of features such as F2, F1, and F7.
Among these top features, only F11 has negative attributions, shifting the verdict in the opposite direction towards a different label.
On the other hand, all the remaining features are shown to have positive contributions, increasing the likelihood of the assigned label with respect to this case.
In addition, the values of F8,  F9,and F13 are regarded as less important when it comes to arriving at the prediction probabilities across the two classes.
To cut a long story short, we can conclude that there is little to no doubt about the true label here.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F2, F1, F7, F11, F3, F10, F12, F4, F5, F6, F8, F9, F14, and F13], with values [0.43, 0.18, 0.10, -0.07, 0.07, -0.06, 0.06, 0.05, 0.04, 0.04, 0.04, -0.04, 0.02, and 0.01].
Postive features are [F2, F1, F7, F3, F12, F4, F5, F6, F8, F14, and F13].
Negative features are [F11, F10, and F9].
Lowest impact features are [F6, F8, F9, F14, and F13] with values [0.04, 0.04, -0.04, 0.02, and 0.01]."
"The classification algorithm labels the given case as C1 with an 82.0% confidence level, implying that there is only an 18.00% chance that it could be C2.
F44, F8, and F3 are the most influential features resulting in the prediction probabilities across the classes.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in favour of a different label.
These include F27, or F10, since their respective degrees of influence are less important to the algorithm's labelling decision for this case.
Other positive features increasing the odds of being the correct label included F37,  F33,and F22.
In contrast, the top negative features decreasing the likelihood of arriving at the assigned label are F4, which can be blamed for the uncertainty associated with the classifier's output decision here.
Finally, considering the direction of effect of each input feature, it is valid to conclude that the majority of them positively contribute towards the above-mentioned label assignment.
From the attribution analysis, we can see that not all the relevant features contribute positively, hence they are regarded as irrelevant when determining the appropriate label based on the information provided about them.
","Predicted class is C1, value of 82.00%.
Other classes and values are C2 18.00%.
Top features are [F44, F8, F3, F27, F10, F37, F40, F46, F28, F33, F26, F22, F42, F17, F19, F6, F23, F35, F36, and F1], with values [-0.08, 0.06, -0.05, 0.05, 0.04, -0.04, 0.03, 0.03, 0.03, -0.03, 0.03, -0.02, 0.02, 0.02, 0.02, -0.02, 0.02, 0.02, 0.02, and 0.02].
Postive features are [F8, F27, F10, F40, F46, F28, F26, F42, F17, F19, F23, F35, F36, and F1].
Negative features are [F44, F3, F37, F33, F22, and F6].
Lowest impact features are [F31, F15, F24, F29, and F25] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classifier is 100.0% certain that the correct label for the given data instance is C1 since there is little to no chance that C2 is the right label.
The classification decision above is mainly due to the influence of features such as F22, F19, and F13.
Among these top features, only F10 has a negative impact, shifting the verdict in the opposite direction towards one of the other classes.
Other features with positive attributions include F3,  F17,and F16.
On the flip side, all the remaining features are shown to be irrelevant when it comes to labelling the case under consideration.
In addition, among the top influential features ranked according to their respective degree of influence from the most important to least relevant, we can conclude that they have very little effect on the prediction made here.
Finally, considering the attribution analysis, it is not unexpected that this level of certainty could be explained away by looking at the confidence level associated with the assigned label rather than the predicted probabilities across the labels.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F22, F19, F13, F10, F20, F1, F3, F17, F16, F18, F11, F9, F7, F4, F2, F6, F21, F12, F15, and F14], with values [0.54, 0.36, 0.28, -0.10, 0.09, 0.09, 0.08, -0.07, 0.05, -0.05, 0.05, -0.05, -0.05, 0.04, -0.04, 0.03, -0.03, 0.03, 0.03, and -0.02].
Postive features are [F22, F19, F13, F20, F1, F3, F16, F11, F4, F6, F12, and F15].
Negative features are [F10, F17, F18, F9, F7, F2, F21, and F14].
Lowest impact features are [F12, F15, F14, F5, and F8] with values [0.03, 0.03, -0.02, 0.00, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level of 65.07%, implying that there is only a 34.93% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F5, F3, and F2.
On the other hand, the least important features are F4, which has a negative impact on the classifier's labelling decision for this case.
Other positive features increasing the odds in favour of the assigned label include F9, or F11.
However, considering the direction of effect of each input feature, it is not unexpected that the algorithm is very uncertain about the correct label for the case under consideration.
In summary, we can see from the predicted likelihoods across the classes that neither F7 nor F10 have negative attributions, hence they are less likely to be the true label here.
","Predicted class is C2, value of 65.07%.
Other classes and values are C1 34.93%.
Top features are [F5, F3, F2, F4, F9, F11, F6, F8, F7, F1, and F10], with values [0.06, 0.04, -0.03, -0.03, -0.03, -0.03, -0.02, -0.02, 0.01, -0.01, and 0.00].
Postive features are [F5, F3, F7, and F10].
Negative features are [F2, F4, F9, F11, F6, F8, and F1].
Lowest impact features are [F6, F8, F7, F1, and F10] with values [-0.02, -0.02, 0.01, -0.01, and 0.00]."
"The classifier is 99.72% certain that the correct label for the given data instance is C2 since there is only a 0.28% chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F9, F8, and F4.
Among these top features, only F7 has a negative impact, shifting the verdict in the opposite direction.
On the other hand, all the remaining features are shown to have positive attributions, increasing the likelihood of the assigned label with respect to this case.
In summary, we can see from the predicted probabilities across the classes that each input feature contributes positively towards the prediction made here.
To cut a long story short, the very strong positive contribution of F6 or F2 outweighs the negative ones, hence explaining the confidence level associated with labelling the case as C3.
","Predicted class is C2, value of 99.72%.
Other classes and values are C1 0.28%.
Top features are [F9, F8, F4, F5, F10, F3, F7, F1, F6, and F2], with values [0.43, 0.30, 0.27, 0.17, 0.17, 0.11, -0.10, 0.09, -0.04, and -0.00].
Postive features are [F9, F8, F4, F5, F10, F3, and F1].
Negative features are [F7, F6, and F2].
Lowest impact features are [F3, F7, F1, F6, and F2] with values [0.11, -0.10, 0.09, -0.04, and -0.00]."
"The classification algorithm labels the given case as C1 with a confidence level of 65.51%, implying that there is only a 34.49% chance that it could be C2.
The most relevant features driving the above prediction are F9, F1, and F5.
On the other hand, each input feature has a different degree of influence on the algorithm's output decision in terms of the direction of effect or contribution of their respective features.
In this case, we can see from the attribution analysis that all the negative features have varying degrees of impact, ranging from moderate to low.
To put it in simple terms, the positive features increasing the odds of being the correct label for the case under consideration include F8,  F6,and F3.
As a matter of fact, considering the predicted likelihoods across the classes, it is valid to conclude that the model is not 100.0% certain about the assigned label.
","Predicted class is C1, value of 65.51%.
Other classes and values are C2 34.49%.
Top features are [F9, F1, F5, F8, F7, F4, F6, F3, and F2], with values [0.34, 0.08, -0.03, -0.02, 0.02, 0.01, -0.01, -0.01, and -0.01].
Postive features are [F9, F1, F7, and F4].
Negative features are [F5, F8, F6, F3, and F2].
Lowest impact features are [F7, F4, F6, F3, and F2] with values [0.02, 0.01, -0.01, -0.01, and -0.01]."
"The model predicts class C2 with a 71.87% chance of being correct, implying that there is a 28.13% likelihood that the true label could be C1.
The above prediction decision is mainly due to the influence of features such as F5, F1, and F11.
Among these top features, only F10 has a negative impact, shifting the prediction verdict in the opposite direction.
On the other hand, all the remaining features have positive attributions, increasing the odds of the predicted label for the case under consideration.
In contrast, the least relevant features are F8, which negatively influences the model's decision regarding the given case.
To sum up, it is important to keep in mind that not all input features contribute positively towards the assigned label since their contributions decrease the likelihood of C3 being the correct label.
As a matter of fact, we can see from the very strong positive attribution across the two classes that support labelling this case as ""C2"".
Finally, considering the degree of uncertainty associated with the classification made here, one can conclude that neither F9 nor F2 nor  F6 are considered essential features when arriving at the aforementioned classification conclusion.
","Predicted class is C2, value of 71.87%.
Other classes and values are C1 28.13%.
Top features are [F5, F1, F11, F8, F10, F7, F3, F4, F12, F9, F2, and F6], with values [0.26, 0.07, 0.02, 0.01, -0.01, -0.00, -0.00, 0.00, -0.00, -0.00, -0.00, and -0.00].
Postive features are [F5, F1, F11, F8, and F4].
Negative features are [F10, F7, F3, F12, F9, F2, and F6].
Lowest impact features are [F4, F12, F9, F2, and F6] with values [0.00, -0.00, -0.00, -0.00, and -0.00]."
"The model is 61.74% certain that the correct label for the given data instance is C2, implying that there is a 38.26% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F1, F31, and F14.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these influential features, only F6 has a positive impact, increasing the odds of being the true label with respect to this case.
Other important features considered by the model when arriving at the predicted probabilities across the two classes are F16,  F26, or F22.
However, unlike all the top-ranked features mentioned above, most of them have very little effect on the final labelling decision here.
In summary, the confidence level associated with the assigned label can be explained away by considering the fact that not all relevant features positively contribute to classifying the case under consideration since their contributions decrease the probability that C3 is the right label.
To cut a long story short, we can see from the attribution analysis that each feature contributes negatively towards the assignment of an alternative label (C1).
","Predicted class is C2, value of 61.74%.
Other classes and values are C1 38.26%.
Top features are [F1, F31, F14, F10, F6, F16, F5, F4, F20, F7, F26, F22, F21, F11, F23, F2, F3, F18, F9, and F15], with values [0.33, -0.04, -0.03, -0.03, -0.03, 0.03, -0.02, -0.02, -0.02, -0.02, 0.02, 0.02, 0.02, 0.01, -0.01, -0.01, 0.01, 0.01, 0.01, and 0.01].
Postive features are [F1, F16, F26, F22, F21, F11, F3, F18, F9, and F15].
Negative features are [F31, F14, F10, F6, F5, F4, F20, F7, F23, and F2].
Lowest impact features are [F24, F33, F27, F13, and F25] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classifier is 100.0% certain that the correct label for the given data instance is C3 since there is little to no chance that any of the other labels, C2 and C1, could be the true label.
The above prediction decision is mainly due to the contributions of features such as F15, F4, and F8.
Among these top features, only F9 has a negative impact, shifting the classification verdict in the opposite direction.
Other positive features increasing the odds of C 3 being the right label include F7,  F18, or F20.
On the flip side, all the remaining features have varying degrees of influence, ranging from moderately low to very high.
To sum up, it is important to note that not all input features are considered relevant when making this labelling decision with respect to this case.
In addition, considering the predicted likelihoods across the classes, we can conclude that neither F14 nor F19 had negative attributions, hence they are regarded as irrelevant when determining the appropriate label here.
Finally, looking at the attribution analysis performed to understand the degree of effect of each input feature, most of them are shown to contribute positively towards the assigned label which explains why the confidence level is higher than expected.
","Predicted class is C3, value of 100.00%.
Other classes and values are C2 0.00%& C1 0.00%& C4 0.00%.
Top features are [F15, F4, F8, F1, F7, F18, F20, F12, F17, F13, F16, F2, F3, F5, F10, F9, F11, F6, F14, and F19], with values [0.78, -0.05, 0.04, 0.04, 0.02, 0.02, 0.01, -0.01, 0.01, -0.01, 0.01, 0.01, 0.01, -0.01, -0.01, -0.00, 0.00, 0.00, 0.00, and -0.00].
Postive features are [F15, F8, F1, F7, F18, F20, F17, F16, F2, F3, F11, F6, and F14].
Negative features are [F4, F12, F13, F5, F10, F9, and F19].
Lowest impact features are [F9, F11, F6, F14, and F19] with values [-0.00, 0.00, 0.00, 0.00, and -0.00]."
"The model is 100.0% certain that the correct label for the given case is C1 since there is little to no chance that it is any of the other labels.
F28, F13, and F1 are the most important features resulting in the classification verdict above.
The next set of features with a positive influence on the final labelling decision are F20, or F30.
Among these top features, only F6 has a negative impact, decreasing the likelihood of C2 being the true label.
Other influential features considered by the model when arriving at the prediction probabilities across the two classes include F8,  F11,and F9.
Considering the direction of influence of each input feature, it can be concluded that all the remaining features have varying degrees of attributions, ranging from moderate to low.
In summary, we can conclude that not all relevant features contribute positively to the classifier's output decision here.
Positive features such as F21, F22, (F3), F18, Analysing the directions of effect of different input features indicated that they strongly support the assigned label which could explain the very high confidence level associated with this classification choice.
On the contrary, the negative features increasing the odds of an alternative label are referred to as ""negative features"" because their contributions decrease the probability that C4 is the right label among the selected classes.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F28, F13, F1, F20, F30, F12, F10, F6, F14, F16, F15, F21, F3, F22, F18, F8, F11, F9, F24, and F29], with values [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, and 0.01].
Postive features are [F28, F13, F1, F20, F30, F12, F10, F6, F14, F16, F15, F21, F3, F22, F18, F8, F11, F9, F24, and F29].
Negative features are [ ].
Lowest impact features are [F19, F2, F17, F4, and F26] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model is 100.0% certain that the correct label for the given data instance is C1 since there is little to no chance that it could be C2.
The features with the most influence on the above classification are F5, F1, and F6.
Among these top features, only F4 is shown to have negative contributions, shifting the verdict in the opposite direction towards a different label.
On the other hand, all the remaining features positively contribute to the decision made by the model regarding the case under consideration.
These positive features increase the likelihood of the assigned label (C1).
In contrast, the negative features decrease the odds of C3 being the right label hence they favour labelling this case as ""C2"".
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F5, F1, F6, F3, F2, and F4], with values [0.21, 0.13, 0.13, 0.10, 0.09, and -0.05].
Postive features are [F5, F1, F6, F3, and F2].
Negative features are [F4].
Lowest impact features are [F1, F6, F3, F2, and F4] with values [0.13, 0.13, 0.10, 0.09, and -0.05]."
"The classifier is 99.28% certain that the correct label for the given data instance is C1, implying that there is only a 0.72% chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F17, F13, and F4.
On the other hand, the least relevant features are F6, which has a negative impact on the model's prediction in this case.
Among the top influential features, four have positive attributions, increasing the odds of the assigned label (C1).
Other positive features include F12,  F15,and F11.
However, unlike all the remaining features mentioned above, not all of them positively contribute to labelling the case as C3 since their contributions decrease the likelihood of being the true label here.
In addition, considering the direction of influence or contribution of each input feature, it is valid to conclude that neither F10 nor F5 can explain why the confidence level across the classes is very high.
To cut a long story short, we can look at the attribution analysis performed to understand the degree of uncertainty associated with the aforementioned classification verdict.
Finally, comparing the values of negatively contributing features to those of positive ones shows that they increase the probability that C0 is the right label.
","Predicted class is C1, value of 99.28%.
Other classes and values are C2 0.72%.
Top features are [F17, F13, F4, F12, F15, F20, F6, F3, F11, F8, F9, F19, F7, F1, F14, F18, F2, F16, F10, and F5], with values [0.45, 0.25, -0.13, 0.11, 0.04, -0.04, -0.03, -0.03, 0.02, 0.02, -0.02, -0.01, 0.01, 0.01, 0.01, -0.01, -0.01, 0.01, 0.00, and 0.00].
Postive features are [F17, F13, F12, F15, F11, F8, F7, F1, F14, F16, F10, and F5].
Negative features are [F4, F20, F6, F3, F9, F19, F18, and F2].
Lowest impact features are [F18, F2, F16, F10, and F5] with values [-0.01, -0.01, 0.01, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 95.09%.
This implies that there is about a 1.06% chance that it could be C2, 3.85% and 0.0%, respectively.
The above prediction decision is mainly due to the influence of features such as F15, F3, and F7.
On the other hand, some of the remaining features are shown to have negative attributions, shifting the verdict in the direction of C4.
Among these top features, only F5 has a negative impact, decreasing the likelihood of being the correct label for the case under consideration.
Other positive features include F8,  F10,and F11.
Overall, we can conclude that the uncertainty associated with the assigned label might explain why the algorithm is less certain about the correctness of this classification.
Finally, considering the fact that all the input features have varying degrees of influence on the final labelling decision, it is reasonable to deduce that they are not relevant when determining the appropriate label here.
","Predicted class is C1, value of 95.09%.
Other classes and values are C2 1.06%& C3 3.85%& C4 0.00%.
Top features are [F15, F3, F7, F1, F14, F17, F2, F12, F20, F18, F19, F8, F10, F11, F4, F5, F6, F16, F13, and F9], with values [0.52, -0.06, -0.05, 0.04, -0.03, -0.03, -0.02, 0.02, 0.02, -0.01, 0.01, 0.01, -0.01, 0.01, 0.01, 0.01, 0.01, 0.00, 0.00, and -0.00].
Postive features are [F15, F1, F12, F20, F19, F8, F11, F4, F5, F6, F16, and F13].
Negative features are [F3, F7, F14, F17, F2, F18, F10, and F9].
Lowest impact features are [F5, F6, F16, F13, and F9] with values [0.01, 0.01, 0.00, 0.00, and -0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 63.62%.
This means that there is only a 36.38% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F9, F6, and F12.
On the other hand, the least important features are F13, which has a negative impact on the classifier's labelling decision for this case.
However, comparing the values of these features to those of the remaining ones, it is not surprising that the algorithm is very confident about the correctness of its label choice here.
Among the top influential features, four are shown to have negative attributions, shifting the verdict in the opposite direction favouring the alternative label (C1).
To sum up, we can see from the attribution analysis that all the positive features contribute positively towards the assigned label, while the negative features decrease the odds of C3 being the correct label.
Finally, considering the predicted likelihoods across the two classes, one can conclude that they are mostly responsible for the uncertainty associated with the model's conclusion regarding the case under consideration.
","Predicted class is C2, value of 63.62%.
Other classes and values are C1 36.38%.
Top features are [F9, F6, F12, F16, F13, F3, F11, F15, F7, F8, F4, F14, F2, F10, F5, and F1], with values [0.83, -0.44, 0.27, 0.23, -0.19, -0.12, -0.08, 0.08, -0.08, 0.06, -0.06, 0.05, 0.03, 0.02, -0.01, and 0.00].
Postive features are [F9, F12, F16, F15, F8, F14, F2, F10, and F1].
Negative features are [F6, F13, F3, F11, F7, F4, and F5].
Lowest impact features are [F14, F2, F10, F5, and F1] with values [0.05, 0.03, 0.02, -0.01, and 0.00]."
"The model is 100.0% certain that the correct label for the given data instance is C1 with a very high confidence level, indicating that there is little to no chance that it could be C2.
The classification decision above is mainly based on the influence of features such as F2, F1, and F3.
Among these top features, only F6 is shown to have negative attributions, shifting the verdict in the opposite direction towards either of the other classes.
From the attribution analysis, we can see that all the input features positively contribute to the prediction made by the model for this case under consideration.
All the remaining positive features increase the odds of being the true label here.
Conversely, the negative features decreasing the likelihood of labelling the case as C4 contradicting the assigned label.
Finally, considering the predicted likelihoods across the two classes, it is valid to conclude that neither class nor label is the right one.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F2, F1, F3, F4, F5, and F6], with values [0.21, 0.13, 0.13, 0.10, 0.09, and -0.05].
Postive features are [F2, F1, F3, F4, and F5].
Negative features are [F6].
Lowest impact features are [F1, F3, F4, F5, and F6] with values [0.13, 0.13, 0.10, 0.09, and -0.05]."
"The classifier is 64.62% certain that the correct label for the given data instance is C2, implying that there is a 35.38% chance that it could be C1.
The classification decision above is mainly due to the influence of features such as F6, F3, and F8.
On the other hand, the least important features are F9, which has a negative impact on the prediction made here.
However, looking at the attributions of the input features, they can be ranked in order of their respective degrees of influence from most relevant to least relevant with respect to this case.
From the attribution analysis, only four features have negative contributions, shifting the verdict in the opposite direction favouring the assigned label (C1).
In simple terms, we can see why the confidence level associated with labelling the case as ""C2"" is quite high.
","Predicted class is C2, value of 64.62%.
Other classes and values are C1 35.38%.
Top features are [F6, F3, F8, F2, F9, F5, F4, F1, and F7], with values [-0.11, 0.08, -0.08, -0.06, 0.06, 0.03, -0.03, 0.01, and 0.00].
Postive features are [F3, F9, F5, F1, and F7].
Negative features are [F6, F8, F2, and F4].
Lowest impact features are [F9, F5, F4, F1, and F7] with values [0.06, 0.03, -0.03, 0.01, and 0.00]."
"The model is 73.58% certain that the correct label for the given data instance is C2, implying that there is only a 4.16% chance that it could be C3 and 22.27%.
The classification decision above is mainly due to the influence of features such as F12, F7, and F5.
On the other hand, the least relevant features are F9,  F4,and F2.
Considering the direction of influence or contribution of each input feature, it can be concluded that all the remaining features have positive attributions, increasing the odds of the assigned label.
Among the top influential features, four are shown to have negative contributions, shifting the verdict in the opposite direction favouring the alternative label, C1.
However, considering the attribution probabilities across the classes, we can conclude that they are very close to zero when it comes to classifying the case under consideration.
","Predicted class is C2, value of 73.58%.
Other classes and values are C3 4.16%& C1 22.27%.
Top features are [F12, F7, F5, F8, F11, F1, F9, F4, F2, F10, F3, and F6], with values [0.32, -0.28, -0.07, 0.06, -0.06, 0.03, -0.03, 0.02, 0.02, 0.02, -0.01, and -0.01].
Postive features are [F12, F8, F1, F4, F2, and F10].
Negative features are [F7, F5, F11, F9, F3, and F6].
Lowest impact features are [F4, F2, F10, F3, and F6] with values [0.02, 0.02, 0.02, -0.01, and -0.01]."
"The classification algorithm labels the given case as C4 with a 35.74% chance of being correct, whereas that of C1 is 30.83%, and C2 is 33.42%.
According to the attribution analysis, only F3 and F2 are shown to have negative attributions, shifting the decision in the direction of either of the other classes.
Positive features such as F1, F4, or F6 increase the odds of this label being the correct label.
On the flip side, all the remaining features positively support the prediction made by the algorithm for the case under consideration.
These positive features can be ranked according to their respective degrees of influence from the most relevant feature to least relevant ones: F8,  F9,.
Finally, it is important to note that each input feature has a different degree of impact on the classifier's final labelling decision here.
","Predicted class is C4, value of 35.74%.
Other classes and values are C1 30.83%& C2 0.00%& C3 33.42%.
Top features are [F1, F4, F6, F5, F3, and F2], with values [0.08, 0.05, 0.02, 0.00, -0.00, and -0.00].
Postive features are [F1, F4, F6, and F5].
Negative features are [F3 and F2].
Lowest impact features are [F4, F6, F5, F3, and F2] with values [0.05, 0.02, 0.00, -0.00, and -0.00]."
"The model predicts class C2 with 100.0% certainty, implying that there is little to no chance that the true label for the given case is C1.
F8, F3, and F1 are the most important variables resulting in the above classification decision.
The remaining variables have a moderately low impact on the final prediction made by the model.
Among these negative variables, only F6 has a positive contribution, increasing the odds of the assigned label.
On the other hand, it is not unexpected that this confidence level could be attributed to the very strong positive influence of F7 or F5.
To sum up, looking at the direction of effect of each input variable, we can see that all the features have varying degrees of attributions, ranging from moderate to low.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F8, F3, F1, F9, F7, F4, F5, F2, and F6], with values [0.62, -0.04, -0.02, 0.02, 0.01, -0.01, 0.00, -0.00, and -0.00].
Postive features are [F8, F9, F7, and F5].
Negative features are [F3, F1, F4, F2, and F6].
Lowest impact features are [F7, F4, F5, F2, and F6] with values [0.01, -0.01, 0.00, -0.00, and -0.00]."
"The classification algorithm classifies the given case as C2 with a confidence level equal to 97.71%, implying that there is only a 2.29% chance that it could be C1.
The most relevant features driving the above prediction are F11, F4, and F9.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
Among these negative features, only F5 has a positive contribution, increasing the odds of being the correct label for the case under consideration.
However, compared to all the top-ranked features mentioned above, we can see why the algorithm is not 100.0% certain about the correctness of this classification decision.
In addition, the values of F12,  F6, or F1 positively support the assigned label.
Finally, considering the predicted likelihoods across the classes, it is valid to conclude that the model is very uncertain about which label is appropriate for this case.
","Predicted class is C2, value of 97.71%.
Other classes and values are C1 2.29%.
Top features are [F11, F4, F9, F2, F13, F12, F6, F1, F10, F7, F3, F8, F14, and F5], with values [0.62, 0.24, -0.14, 0.09, -0.08, 0.08, 0.06, 0.06, 0.05, -0.02, -0.02, -0.02, 0.01, and -0.00].
Postive features are [F11, F4, F2, F12, F6, F1, F10, and F14].
Negative features are [F9, F13, F7, F3, F8, and F5].
Lowest impact features are [F7, F3, F8, F14, and F5] with values [-0.02, -0.02, -0.02, 0.01, and -0.00]."
"The classifier is 100.0% certain that C2 is the correct label for the case under consideration since there is little to no chance that it is any of the other labels.
F81, F35, and F89 are the most influential features resulting in the classification verdict above.
The next set of features with a very strong positive influence on the final decision are F42,  F69, or F25.
Among these top features, only F85 has a negative impact, decreasing the likelihood of labelling the given case as C1.
On the flip side, all the remaining features have a positive contribution, increasing the odds of C3 being the right label.
In addition, the values of F32, P26,and F45 can be regarded as less important when determining the appropriate label here.
Finally, considering the direction of influence of each input feature, we can conclude that their respective attributions are not enough to swing the model's judgement in favour of an alternative label which could explain why the confidence level across the classes is quite high.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F81, F35, F89, F42, F69, F25, F91, F86, F51, F32, F8, F26, F88, F76, F45, F10, F73, F31, F22, and F12], with values [-0.30, -0.11, 0.10, -0.10, -0.09, 0.07, -0.06, -0.05, 0.05, 0.05, -0.05, 0.04, -0.04, -0.04, 0.04, 0.03, 0.03, -0.03, 0.03, and 0.03].
Postive features are [F89, F25, F51, F32, F26, F45, F10, F73, F22, and F12].
Negative features are [F81, F35, F42, F69, F91, F86, F8, F88, F76, and F31].
Lowest impact features are [F7, F18, F3, F52, and F27] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 97.03%, implying that there is only a 2.97% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F9, F14, and F10.
On the other hand, the least important features are F7, which has a negative impact on the classifier's final labelling decision for this case.
Among these top features, only F8 have a positive contribution, increasing the odds of being the correct label for the case under consideration.
In addition, all the remaining features positively support the assigned label in terms of their respective attributions across the two classes.
Finally, some of the input features negatively contribute towards the assignment of another label, namely F4.
To sum up, we can see that the marginal uncertainty in the classification verdict here can be explained away by looking at the attribution analysis performed to understand the direction of effect of each feature.
","Predicted class is C1, value of 97.03%.
Other classes and values are C2 2.97%.
Top features are [F9, F14, F10, F11, F2, F1, F12, F7, F6, F8, F5, F13, F3, and F4], with values [0.51, 0.14, 0.12, 0.07, 0.07, 0.07, 0.06, -0.03, -0.03, 0.02, -0.02, 0.01, 0.01, and -0.01].
Postive features are [F9, F14, F10, F11, F2, F1, F12, F8, F13, and F3].
Negative features are [F7, F6, F5, and F4].
Lowest impact features are [F8, F5, F13, F3, and F4] with values [0.02, -0.02, 0.01, 0.01, and -0.01]."
"The model predicts C1 for the case under consideration with an 80.35% chance of being correct, whereas that of C2 is only 19.65%.
The classification decision above is mainly due to the influence of features such as F18, F5, and F7.
On the other hand, the least relevant features are F4, which has a negative impact on the final prediction made by the model in this case.
This can be explained away by looking at the attributions of the input features shown to have varying degrees of influence when it comes to labelling the given case or instance.
Among all the influential features mentioned above, only F6 positively supports the assignment of either label.
Other positive features increasing the likelihood of class C 1 include F17,  F13,and F9.
Finally, among the top negative features shifting the verdict in the opposite direction, we can conclude that there is little to no chance that any of them could be the true label since their respective values contradict the assigned label here.
","Predicted class is C1, value of 80.35%.
Other classes and values are C2 19.65%.
Top features are [F18, F5, F7, F2, F15, F4, F17, F10, F13, F8, F11, F9, F14, F16, F6, F1, F3, F12, and F19], with values [0.18, -0.18, 0.10, -0.08, 0.08, 0.07, 0.06, -0.06, 0.04, -0.04, -0.04, 0.03, 0.03, 0.02, -0.02, 0.01, 0.01, -0.01, and 0.01].
Postive features are [F18, F7, F15, F4, F17, F13, F9, F14, F16, F1, F3, and F19].
Negative features are [F5, F2, F10, F8, F11, F6, and F12].
Lowest impact features are [F6, F1, F3, F12, and F19] with values [-0.02, 0.01, 0.01, -0.01, and 0.01]."
"The classifier is 100.0% certain that the correct label for the given data instance is C1 since there is little to no chance that it could be C2.
The classification decision above is mainly due to the contributions of features such as F10, F2, and F5.
Among these top features, only F6 is shown to have negative attributions, shifting the verdict in the opposite direction towards a different label.
On the other hand, all the remaining features positively support the model's labelling decision with respect to this case.
In terms of the direction of influence of each input feature, they can be ranked according to their respective degrees of impact from most important to least relevant: F9,  F3, or F4.
To sum up, we can see why the confidence level is very high when you look at the predicted probabilities across the classes.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F10, F2, F5, F9, F3, F4, F7, F8, F1, and F6], with values [0.25, 0.24, 0.16, 0.16, 0.13, 0.04, 0.03, 0.02, 0.02, and -0.02].
Postive features are [F10, F2, F5, F9, F3, F4, F7, F8, and F1].
Negative features are [F6].
Lowest impact features are [F4, F7, F8, F1, and F6] with values [0.04, 0.03, 0.02, 0.02, and -0.02]."
"The classifier is 70.0% certain that the correct label for the given data instance is C1, implying that there is about a 30.00% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F5, F9, and F12.
Among these top features, only F8 has negative attributions, shifting the verdict in the direction of either of the other classes.
Other positive features increasing the odds of labelling this case as C3 are F7, or F1.
On the flip side, all the remaining features have varying degrees of impact, ranging from moderately low to very high.
These include F4,  F10,and F6.
To sum up, looking at the attribution analysis, we can see that each input feature has a different degree of effect on the classification made here.
It is important to note that not all relevant features are shown to contribute positively towards the assigned label, hence they are referred to as negative features since their contributions decrease the likelihood of being the true label.
","Predicted class is C1, value of 70.00%.
Other classes and values are C2 30.00%.
Top features are [F5, F9, F12, F8, F7, F1, F18, F17, F20, F11, F16, F15, F14, F19, F3, F2, F4, F13, F10, and F6], with values [0.11, -0.10, 0.05, -0.03, 0.02, -0.02, 0.02, 0.02, 0.01, -0.01, 0.01, -0.01, 0.01, -0.01, 0.00, -0.00, 0.00, -0.00, -0.00, and 0.00].
Postive features are [F5, F12, F7, F18, F17, F20, F16, F14, F3, F4, and F6].
Negative features are [F9, F8, F1, F11, F15, F19, F2, F13, and F10].
Lowest impact features are [F2, F4, F13, F10, and F6] with values [-0.00, 0.00, -0.00, -0.00, and 0.00]."
"The classification algorithm labels the given case as C2 with an 87.50% confidence level, implying that there is only a small chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F4, F5, and F1 which are ranked in order of their respective degrees of influence from most important to least important.
Among these top features, only F6 has negative attributions, decreasing the odds of being the correct label for this case.
On the other hand, all the remaining features have positive contributions, increasing the likelihood of the assigned label.
In summary, we can see why the algorithm is very confident about the correctness of its class assignment here.
","Predicted class is C2, value of 87.50%.
Other classes and values are C1 12.50%.
Top features are [F4, F5, F1, F7, F6, F9, F2, F8, and F3], with values [0.23, -0.08, -0.08, -0.06, -0.06, 0.05, 0.04, 0.01, and 0.01].
Postive features are [F4, F9, F2, F8, and F3].
Negative features are [F5, F1, F7, and F6].
Lowest impact features are [F6, F9, F2, F8, and F3] with values [-0.06, 0.05, 0.04, 0.01, and 0.01]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 98.80%, implying that there is only a 1.20% chance that it could be C1.
The most important features resulting in the above prediction are F22, F41, and F16.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in favour of an alternative label (C1).
These include F17,  F7,and F14.
Among the top influential features considered by the algorithm for this labelling assignment, four are shown to have positive contributions, increasing the odds of being the correct label here.
However, on the lower end are the irrelevant features such as F32, Analysing the direction of influence of each input feature indicates that they can be blamed for the uncertainty associated with the assigned label since their respective degrees of impact are very close to zero.
Finally, considering the attribution probabilities across the classes, it is valid to conclude that not all the relevant features positively contribute to the classifier's decision regarding the provided data or case under consideration.
","Predicted class is C2, value of 98.80%.
Other classes and values are C1 1.20%.
Top features are [F22, F41, F16, F17, F7, F14, F38, F2, F40, F27, F29, F10, F23, F32, F35, F12, F19, F21, F11, and F3], with values [0.30, 0.22, -0.11, -0.07, 0.06, -0.05, -0.05, 0.04, 0.04, 0.04, -0.04, 0.04, -0.04, 0.04, -0.03, -0.03, -0.03, 0.03, -0.02, and 0.02].
Postive features are [F22, F41, F7, F2, F40, F27, F10, F32, F21, and F3].
Negative features are [F16, F17, F14, F38, F29, F23, F35, F12, F19, and F11].
Lowest impact features are [F20, F15, F30, F34, and F24] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model is 75.0% certain that the correct label for the given case is C1, implying that there is about a 25.00% chance that it could be C2.
The features with the most influence on the above classification are F12, F9, and F1.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
Among these negative features, only F6 has a positive impact, increasing the odds of being the true label here.
However, looking at the attribution analysis performed to understand the direction of influence of each input feature, it can be concluded that not all the features positively contribute to the prediction made by the model for this case.
In summary, we can see that among the top influential features (i.e., F11, or F2), only three are identified as negative since their contributions favour labelling the case as ""C2"".
To cut a long story short, the confidence level across the two classes is higher than that of any other class.
","Predicted class is C1, value of 75.00%.
Other classes and values are C2 25.00%.
Top features are [F12, F9, F1, F7, F11, F5, F10, F2, F4, F8, F3, and F6], with values [-0.31, -0.30, 0.22, -0.21, 0.19, -0.18, -0.15, 0.12, -0.07, -0.05, -0.03, and -0.02].
Postive features are [F1, F11, and F2].
Negative features are [F12, F9, F7, F5, F10, F4, F8, F3, and F6].
Lowest impact features are [F2, F4, F8, F3, and F6] with values [0.12, -0.07, -0.05, -0.03, and -0.02]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 84.87%.
This means that there is only a 15.13% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F5, F7, and F8.
On the other hand, the least relevant features are F9, which has a very low impact on the algorithm's labelling decision for this case.
However, comparing the direction of effect of each input feature to that of the remaining ones, it can be concluded that all the top features have positive attributions, increasing the odds of being the correct label for the case under consideration.
Finally, considering the degree of uncertainty associated with the assigned label, we can conclude that the model is not 100.0% certain about the correctness of its assignment here.
","Predicted class is C2, value of 84.87%.
Other classes and values are C1 15.13%.
Top features are [F5, F7, F8, F2, F3, F10, F9, F12, F6, F4, F11, and F1], with values [0.36, 0.24, -0.17, 0.15, -0.09, 0.09, 0.04, 0.03, -0.02, -0.01, -0.00, and -0.00].
Postive features are [F5, F7, F2, F10, F9, and F12].
Negative features are [F8, F3, F6, F4, F11, and F1].
Lowest impact features are [F12, F6, F4, F11, and F1] with values [0.03, -0.02, -0.01, -0.00, and -0.00]."
"The classifier is 83.08% certain that the correct label for the given case is C1, implying that there is about a 16.87% chance that it could be C3 or C4.
The uncertainty in the classification decision can be attributed to the influence of features such as F3, F6, and F4 since their respective degrees of influence favour labelling the case as C2.
On the other hand, all the remaining features have negative attributions, increasing the odds of the alternative label being the true label.
In this case, we can see from the prediction probabilities across the classes that they are very close to zero when it comes to predicting the appropriate label here.
","Predicted class is C1, value of 83.08%.
Other classes and values are C3 16.87%& C4 0.00%& C2 0.05%.
Top features are [F3, F6, F4, F5, F2, and F1], with values [-0.27, 0.16, 0.12, -0.04, 0.03, and 0.01].
Postive features are [F6, F4, F2, and F1].
Negative features are [F3 and F5].
Lowest impact features are [F6, F4, F5, F2, and F1] with values [0.16, 0.12, -0.04, 0.03, and 0.01]."
"The classification algorithm labels the given case as C2 with an 83.98% confidence level, implying that there is only a 16.02% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F14, F15, and F12.
On the other hand, the top negative features are F2, which can be blamed for the uncertainty associated with the assigned label since their respective attributions contradict the algorithm's labelling decision in this case.
Other positive features increasing the odds of being correct include F9,  F19,and F6.
Finally, considering the direction of influence or contribution of each input feature, it is valid to conclude that the majority of the relevant features have positive contributions, shifting the verdict in the opposite direction.
In summary, we can see from the attribution analysis that all the influential features positively contribute towards the model's output choice here.
","Predicted class is C2, value of 83.98%.
Other classes and values are C1 16.02%.
Top features are [F14, F15, F12, F18, F9, F2, F19, F6, F13, F4, F11, F16, F3, F8, F10, F1, F7, F17, and F5], with values [0.12, 0.07, 0.05, 0.05, 0.04, -0.04, 0.03, 0.02, 0.02, 0.01, -0.01, -0.01, 0.01, 0.00, -0.00, 0.00, 0.00, 0.00, and 0.00].
Postive features are [F14, F15, F12, F18, F9, F19, F6, F13, F4, F3, F8, F1, F7, F17, and F5].
Negative features are [F2, F11, F16, and F10].
Lowest impact features are [F10, F1, F7, F17, and F5] with values [-0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model predicted class C1 for the case under consideration with a confidence level equal to 56.51%.
However, there is a 43.49% chance that label C2 could be the true label.
The classification decision above is mainly due to the influence of features such as F8, F1, and F6.
Among these top features, only F3 has a negative impact, shifting the verdict in the opposite direction.
On the other hand, all the remaining features are shown to have positive attributions, increasing the odds of being the correct label for this case.
In addition, some of the input features positively support labelling the given case as ""C1"".
This might explain why the model is not 100.0% certain about the assigned label since their respective degrees of influence are very close to zero when it comes to predicting the appropriate label here.
","Predicted class is C1, value of 56.51%.
Other classes and values are C2 43.49%.
Top features are [F8, F1, F6, F3, F2, F5, F4, F9, F13, F15, F16, F14, F10, F12, F7, and F11], with values [0.04, 0.04, 0.03, -0.03, -0.02, 0.02, 0.01, 0.01, -0.01, -0.01, -0.01, 0.01, 0.00, 0.00, 0.00, and -0.00].
Postive features are [F8, F1, F6, F5, F4, F9, F14, F10, F12, and F7].
Negative features are [F3, F2, F13, F15, F16, and F11].
Lowest impact features are [F14, F10, F12, F7, and F11] with values [0.01, 0.00, 0.00, 0.00, and -0.00]."
"The model is 51.69% certain that the correct label for the given data instance is C1, implying that there is a 48.31% chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F8, F9, and F1.
On the other hand, the least relevant features are F5 and  F4.
In terms of the direction of influence or contribution of each input feature, only F6 has negative attributions, shifting the verdict in the opposite direction.
However, when you take into consideration the very high degree of certainty associated with the assigned label, it is easy to see why the model's confidence level is moderately high.
","Predicted class is C1, value of 51.69%.
Other classes and values are C2 48.31%.
Top features are [F8, F9, F1, F7, F6, F3, F2, F5, and F4], with values [-0.24, 0.10, 0.07, 0.07, 0.05, 0.03, -0.02, -0.01, and -0.01].
Postive features are [F9, F1, F7, F6, and F3].
Negative features are [F8, F2, F5, and F4].
Lowest impact features are [F6, F3, F2, F5, and F4] with values [0.05, 0.03, -0.02, -0.01, and -0.01]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 85.13%, implying that there is only a 14.87% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F15, F1, and F2.
On the other hand, the least relevant features are F11,  F9,and F12.
Among these top-ranked features, only F16 has negative attributions, shifting the verdict in favour of an alternative label.
However, considering the direction of effect of each input feature, it is not surprising that the algorithm is very certain that this is the correct label for the case under consideration.
Other positive features increasing the likelihood of the assigned label include F14, which increases the odds of C3 being the true label here.
Finally, on the flip side, all the remaining features have values that contradict the classifier's labelling decision based on their respective degrees of influence.
In summary, we can see from the predicted probabilities across the classes that they are moderately low when you consider the degree of uncertainty associated with the aforementioned classification conclusion.
","Predicted class is C2, value of 85.13%.
Other classes and values are C1 14.87%.
Top features are [F15, F1, F2, F16, F14, F6, F3, F4, F11, F9, F12, F8, F10, F7, F13, and F5], with values [-0.31, 0.05, -0.04, 0.04, -0.03, 0.02, 0.02, 0.02, -0.01, 0.01, 0.01, -0.01, 0.01, -0.00, -0.00, and -0.00].
Postive features are [F1, F16, F6, F3, F4, F9, F12, and F10].
Negative features are [F15, F2, F14, F11, F8, F7, F13, and F5].
Lowest impact features are [F8, F10, F7, F13, and F5] with values [-0.01, 0.01, -0.00, -0.00, and -0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 95.09%.
This means that there is only a 4.91% chance that it could be C1.
The attributions analysis shows that F4, F5, and F10 are the most important driving factors resulting in the above classification decision.
On the other hand, some of the variables have negative contributions, shifting the verdict in favour of an alternative label.
Among these positive variables, only F9 has a negative contribution, increasing the odds of being the correct label for the case under consideration.
However, considering the direction of influence of each input variable, it is not surprising that the algorithm is very confident about the assigned label since their respective degrees of impact are quite close to zero.
","Predicted class is C2, value of 95.09%.
Other classes and values are C1 4.91%.
Top features are [F4, F5, F10, F2, F1, F3, F8, F7, F11, F6, and F9], with values [-0.52, 0.31, -0.10, 0.07, 0.03, -0.03, -0.03, -0.02, -0.01, 0.01, and 0.01].
Postive features are [F5, F2, F1, F6, and F9].
Negative features are [F4, F10, F3, F8, F7, and F11].
Lowest impact features are [F8, F7, F11, F6, and F9] with values [-0.03, -0.02, -0.01, 0.01, and 0.01]."
"The model is 100.0% certain that the correct label for the given data instance is C2 since there is little to no chance that it is any of the other labels.
The features with the most influence on the above classification are F2, F7, and F4.
Among these top features, only F12 has a negative contribution, shifting the prediction decision in the direction of C1.
Other positive features such as F8,  F9, or F11 have values that increase the classifier's response in favour of labelling the case as ""C2"".
On the flip side, all the remaining features positively support the assigned label, which can be attributed to the fact that their respective degrees of influence are very close to zero.
To sum up, we can see from the attributions analysis that each input feature contributes positively towards the assignment of an alternative label.
In contrast, the negative features decreasing the odds of this being the true label are referred to as 'negative features' since their contributions decrease the likelihood of assigning the appropriate label here.
Finally, considering the predicted likelihoods across the classes, one can conclude that they are not relevant when arriving at the aforementioned classification verdict.
","Predicted class is C2, value of 100.00%.
Other classes and values are C1 0.00%.
Top features are [F2, F7, F4, F6, F5, F12, F8, F10, F9, F14, F1, F3, F11, and F13], with values [0.43, 0.18, 0.10, 0.07, 0.07, -0.06, -0.05, 0.04, -0.04, 0.02, 0.02, 0.01, -0.01, and 0.01].
Postive features are [F2, F7, F4, F6, F5, F10, F14, F1, F3, and F13].
Negative features are [F12, F8, F9, and F11].
Lowest impact features are [F14, F1, F3, F11, and F13] with values [0.02, 0.02, 0.01, -0.01, and 0.01]."
"The classifier is 100.0% certain that the correct label for the given case is C1 since there is little to no chance that C2 is the right label.
The classification decision above is mainly due to the influence of features such as F8, F6, and F4.
Among these top features, only F9 has a negative contribution, shifting the verdict in the opposite direction towards either of the other labels.
Other positive features increasing the odds of this prediction are F3 and  F5.
On the flip side, all the remaining features have varying degrees of impact, ranging from moderate to low.
To sum up, it is important to take into account the attributions of each input feature when arriving at the predicted probabilities across the two classes.
Finally, considering the degree of uncertainty associated with the assigned label, we can conclude that they are not relevant when determining the appropriate label here.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F8, F6, F4, F10, F13, F9, F7, F12, F1, F2, F14, F11, F3, and F5], with values [0.43, 0.18, 0.10, 0.07, 0.07, -0.06, -0.05, 0.04, -0.04, 0.02, 0.02, 0.01, -0.01, and 0.01].
Postive features are [F8, F6, F4, F10, F13, F12, F2, F14, F11, and F5].
Negative features are [F9, F7, F1, and F3].
Lowest impact features are [F2, F14, F11, F3, and F5] with values [0.02, 0.02, 0.01, -0.01, and 0.01]."
"The classification algorithm labels the given case as C1 with an 85.0% confidence level, implying that there is about a 15.00% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F2, F10, and F13.
On the other hand, some of the input features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these influential features, only F15 has a positive impact, increasing the odds of being the correct label for the case under consideration.
However, compared to F11, all the remaining features have very little effect on the algorithm's final labelling decision for this case.
In summary, we can conclude that the uncertainty associated with the assigned label might be explained away by looking at the attribution analysis performed to understand the degree of influence or contribution of each feature.
To cut a long story short, the most relevant features considered here are F26,  F5,and F35 since their respective contributions decrease the likelihood of any of them being true.
Finally, considering the predicted likelihoods across the classes, it is reasonable to deduce that they are moderately low.
","Predicted class is C1, value of 85.00%.
Other classes and values are C2 15.00%.
Top features are [F2, F10, F13, F26, F5, F15, F35, F3, F7, F28, F9, F40, F39, F22, F23, F24, F42, F33, F19, and F11], with values [0.14, -0.13, 0.11, 0.08, 0.06, -0.04, 0.04, 0.03, -0.03, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, -0.01, 0.01, -0.01, and -0.01].
Postive features are [F2, F13, F26, F5, F35, F3, F28, F9, F40, F39, F22, F23, F24, and F33].
Negative features are [F10, F15, F7, F42, F19, and F11].
Lowest impact features are [F16, F18, F36, F1, and F41] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model is 100.0% certain that the correct label for the given data instance is C1 with a very high confidence level, which implies that there is little to no chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F4, F3, and F5.
On the other hand, all the remaining features are shown to have negative attributions, shifting the prediction verdict in the opposite direction.
Among these positive features, only F6 has a negative impact, decreasing the likelihood of the assigned label, while F7 does not.
In summary, we can see from the attribution analysis that each input feature contributes positively or negatively towards the final labelling decision made by the model regarding the case under consideration.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F4, F3, F5, F6, F2, F7, and F1], with values [0.47, 0.22, 0.20, 0.19, 0.05, 0.01, and 0.01].
Postive features are [F4, F3, F5, F6, F2, F7, and F1].
Negative features are [ ].
Lowest impact features are [F5, F6, F2, F7, and F1] with values [0.20, 0.19, 0.05, 0.01, and 0.01]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 99.84%, implying that there is only a 0.16% chance that it could be C2.
The most important features resulting in the above prediction are F10, F23, and F33.
On the other hand, some of the input features have negative attributions, shifting the verdict in favour of an alternative label for the case under consideration.
These include F7,  F13,and F30.
Among the top influential features, only F32 has a negative influence, decreasing the odds of being the correct label here.
Other positive features considered by the algorithm are such as F9, or F3.
However, unlike all the remaining features mentioned above, they can be ranked according to their contributions to the model's output decision based on the degree of influence of each feature.
In summary, we can conclude that the marginal uncertainty associated with the assigned label might be explained away by looking at the attribution analysis performed to understand the direction of effect of different features.
Finally, considering the predicted likelihoods across the classes, it is not surprising that this algorithm is very confident about the correctness of its class assignment.
","Predicted class is C1, value of 99.84%.
Other classes and values are C2 0.16%.
Top features are [F10, F23, F33, F29, F17, F32, F6, F19, F21, F7, F13, F30, F14, F2, F38, F20, F3, F28, F34, and F9], with values [4.06, 2.67, -1.18, -1.09, 0.93, -0.90, -0.74, 0.73, -0.62, 0.55, -0.44, 0.42, -0.40, 0.30, -0.24, -0.21, -0.17, -0.12, -0.10, and 0.10].
Postive features are [F10, F23, F17, F19, F7, F30, F2, and F9].
Negative features are [F33, F29, F32, F6, F21, F13, F14, F38, F20, F3, F28, and F34].
Lowest impact features are [F8, F11, F1, F22, and F31] with values [0.02, 0.02, -0.01, 0.01, and 0.00]."
"The classification algorithm labels the given case as C2 with a confidence level equal to 95.09%.
This implies that there is only a 1.06% chance that it could be C3, 3.85% and 0.0%, respectively.
The most important features resulting in the above prediction are F4, F1, and F9.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in favour of an alternative label.
These include F16,  F17,and F5.
Among the top influential features considered by the algorithm when determining the correct label for the case under consideration, four are shown to negatively contribute to the model's decision here.
Other positive features increasing the likelihood of labelling this case differently from C4 are: F7, but F2 also contributes positively.
In contrast, the marginal uncertainty surrounding the assigned label can be explained away by looking at the direction of influence of each input feature.
Positive features such as F6,, F13, or F10 increase the odds of being the true label among the three possible classes.
Finally, considering the attribution probabilities across the two classes, it is valid to conclude that the very strong positive contribution of F11 outweighs the negative contributions of all the others.
","Predicted class is C2, value of 95.09%.
Other classes and values are C4 1.06%& C3 3.85%& C1 0.00%.
Top features are [F4, F1, F9, F7, F3, F18, F20, F2, F11, F14, F13, F10, F15, F6, F19, F12, F16, F17, F8, and F5], with values [0.52, -0.06, -0.05, 0.04, -0.03, -0.03, -0.02, 0.02, 0.02, -0.01, 0.01, 0.01, -0.01, 0.01, 0.01, 0.01, 0.01, 0.00, 0.00, and -0.00].
Postive features are [F4, F7, F2, F11, F13, F10, F6, F19, F12, F16, F17, and F8].
Negative features are [F1, F9, F3, F18, F20, F14, F15, and F5].
Lowest impact features are [F12, F16, F17, F8, and F5] with values [0.01, 0.01, 0.00, 0.00, and -0.00]."
"The model is 99.72% certain that the correct label for the given data instance is C1, implying that there is only a 0.28% chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F12, F17, and F7.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
Among these top features, four are shown to have positive contributions, increasing the odds of being the true label here.
Other positive features include F20, which has a very strong positive impact on the final prediction made by the model with respect to this case.
In contrast, the least relevant features are F4,  F18, or F14 since their values support labelling the case as ""C2"".
To cut a long story short, we can look at the attribution analysis performed to understand why the confidence level across the two classes is so high.
Finally, considering the degree of uncertainty associated with the assigned label, it is important to note that not all the input features contribute positively, hence they are deemed irrelevant when determining the appropriate label under consideration.
","Predicted class is C1, value of 99.72%.
Other classes and values are C2 0.28%.
Top features are [F12, F17, F7, F20, F5, F4, F18, F14, F1, F22, F10, F8, F16, F2, F15, F13, F19, F3, F9, and F21], with values [0.38, -0.32, 0.11, 0.09, 0.08, -0.07, -0.07, -0.06, -0.06, 0.05, 0.05, 0.04, 0.04, -0.04, -0.04, -0.03, 0.03, 0.03, -0.02, and -0.02].
Postive features are [F12, F7, F20, F5, F22, F10, F8, F16, F19, and F3].
Negative features are [F17, F4, F18, F14, F1, F2, F15, F13, F9, and F21].
Lowest impact features are [F3, F9, F21, F6, and F11] with values [0.03, -0.02, -0.02, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level of 65.51%, implying that there is only a 34.49% chance that it could be C2.
F5, F1, and F4 are the most influential features resulting in the above-mentioned classification decision.
On the other hand, the least relevant features are F3 and  F7.
Considering the direction of influence of each input feature, it is not surprising that the algorithm is less certain about the correct label for this case since their respective attributions are very close to zero.
According to the attribution analysis, all the negative features have varying degrees of impact, ranging from moderate to low.
The top positive features increasing the likelihood of the assigned label include F9, which has a strong positive contribution, while the remaining negative ones favour labelling the case differently.
In simple terms, we can conclude that neither F8 nor F6 had any effect on the final prediction made here.
","Predicted class is C1, value of 65.51%.
Other classes and values are C2 34.49%.
Top features are [F5, F1, F4, F9, F2, F8, F6, F7, and F3], with values [0.34, 0.08, -0.03, -0.02, 0.02, 0.01, -0.01, -0.01, and -0.01].
Postive features are [F5, F1, F2, and F8].
Negative features are [F4, F9, F6, F7, and F3].
Lowest impact features are [F2, F8, F6, F7, and F3] with values [0.02, 0.01, -0.01, -0.01, and -0.01]."
"The classification algorithm labels the given case as C1 with a moderate degree of confidence since there is only a 44.0% chance that it could be C2.
F22, F26, and F18 are the most influential features resulting in the above classification decision.
The least relevant features considered by the algorithm to arrive at the label assignment here are F6, or F10.
On the other hand, some of the input features have negative attributions, shifting the verdict in favour of an alternative label.
These include F13,  F12,and F21.
Among the top positive features increasing the odds of being the correct label for the case under consideration, four are shown to negatively contribute to the final labelling decision; these are: F9, F29,Â F8, And F16.
Finally, considering the direction of influence of each input feature, it is not surprising that the model is quite confident about the correctness of this classification output.
","Predicted class is C1, value of 56.00%.
Other classes and values are C2 44.00%.
Top features are [F22, F26, F18, F5, F6, F10, F13, F12, F21, F29, F8, F16, F1, F9, F24, F23, F14, F20, F28, and F15], with values [-0.14, -0.08, 0.07, 0.04, -0.03, -0.03, 0.03, 0.02, -0.02, 0.01, 0.01, 0.01, -0.01, -0.01, -0.00, 0.00, -0.00, -0.00, -0.00, and -0.00].
Postive features are [F18, F5, F13, F12, F29, F8, F16, and F23].
Negative features are [F22, F26, F6, F10, F21, F1, F9, F24, F14, F20, F28, and F15].
Lowest impact features are [F3, F17, F30, F19, and F4] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model predicted class C1 with 100.0% certainty, implying that there is little to no chance that the correct label for the given data instance is C2.
The classification decision above is mainly due to the influence of features such as F3, F4, and F2 on the model.
Among these top features, only F6 is shown to have negative attributions, shifting the verdict in favour of a different label.
However, looking at the direction of influence or contribution of each feature, it can be concluded that all the input features are positive, increasing the odds of the assigned label (C1) being the right one.
On the other hand, the marginal uncertainty associated with the prediction made here could be explained away by the very strong positive attribution of F5.
","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F3, F4, F2, F1, F5, and F6], with values [0.28, 0.14, 0.12, 0.10, 0.08, and -0.07].
Postive features are [F3, F4, F2, F1, and F5].
Negative features are [F6].
Lowest impact features are [F4, F2, F1, F5, and F6] with values [0.14, 0.12, 0.10, 0.08, and -0.07]."
"The classification algorithm labels the given case as C1 with a 67.08% confidence level, implying that there is a 32.92% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F8, F6, and F5.
Among these top features, only F9 has a negative impact on the classifier's labelling decision for this case.
On the other hand, all the remaining features are shown to have positive attributions, increasing the odds in favour of the assigned label.
From the attribution analysis, we can see why the algorithm is not 100.0% certain about the correct label for the case under consideration.
To put it in simple terms, each input feature contributes negatively towards the assignment of any other label since their contributions decrease the likelihood of C3 being the true label here.
","Predicted class is C1, value of 67.08%.
Other classes and values are C2 32.92%.
Top features are [F8, F6, F5, F7, F2, F4, F9, F3, F11, F1, and F10], with values [-0.33, 0.14, 0.14, 0.12, 0.07, 0.06, -0.04, 0.03, -0.02, 0.01, and 0.01].
Postive features are [F6, F5, F7, F2, F4, F3, F1, and F10].
Negative features are [F8, F9, and F11].
Lowest impact features are [F9, F3, F11, F1, and F10] with values [-0.04, 0.03, -0.02, 0.01, and 0.01]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 66.11%.
This means that there is only a 31.78% chance that it could be C3 or C2.
The prediction decision above is mainly due to the influence of features such as F4, F6, and F2 which are shown to have varying degrees of influence on the algorithm's output decision for the case under consideration.
Among these top features, only F9 has a negative impact, shifting the verdict in the opposite direction towards one of the other possible classes.
Other positive features increasing the odds of this label being the correct label include F10,  F3,and F12.
On the flip side, all the remaining features have values that contradict the labelling decision made here.
In summary, we can see from the attributions analysis that the majority of input features positively support the assigned label (C1), while the least influential ones are F8, Analysing the attribution probabilities across the two classes indicate that they are less certain about the true label since their contributions tend to shift the final decision in favour of an alternative label.
","Predicted class is C1, value of 66.11%.
Other classes and values are C3 31.78%& C2 2.11%.
Top features are [F4, F6, F2, F8, F9, F7, F11, F10, F3, F1, F12, and F5], with values [0.41, -0.03, 0.03, -0.02, 0.02, 0.02, 0.01, 0.01, -0.00, 0.00, -0.00, and 0.00].
Postive features are [F4, F2, F9, F7, F11, F10, F1, and F5].
Negative features are [F6, F8, F3, and F12].
Lowest impact features are [F10, F3, F1, F12, and F5] with values [0.01, -0.00, 0.00, -0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with a confidence level of 98.17%, implying that there is only a 1.83% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F64, F72, and F66.
On the other hand, some of the input features are shown to have negative attributions, shifting the verdict in the opposite direction.
Among these influential features, only F5 has a positive impact, increasing the odds of being the correct label for the case under consideration.
However, compared to all the top features considered by the algorithm, we can conclude that the least relevant ones are F32,  F90, or F29.
Finally, considering the predicted likelihoods across the classes, it is not surprising that this algorithm is very confident about the assigned label.
","Predicted class is C1, value of 98.17%.
Other classes and values are C2 1.83%.
Top features are [F64, F72, F66, F73, F28, F67, F36, F5, F69, F24, F54, F85, F21, F83, F32, F90, F29, F1, F11, and F51], with values [-0.02, -0.02, -0.02, -0.02, -0.02, -0.02, -0.02, 0.02, -0.02, -0.02, -0.02, -0.02, 0.02, 0.02, -0.02, 0.01, -0.01, -0.01, -0.01, and -0.01].
Postive features are [F5, F21, F83, and F90].
Negative features are [F64, F72, F66, F73, F28, F67, F36, F69, F24, F54, F85, F32, F29, F1, F11, and F51].
Lowest impact features are [F56, F78, F65, F79, and F89] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The model is 51.62% certain that the correct label for the given data instance is C1, implying that there is a 48.38% chance that it could be C2.
The classification decision above is mainly due to the influence of features such as F8, F9, and F3.
Among these top features, only F6 has a negative impact on the prediction made by the model with respect to this case.
On the other hand, all the remaining features are shown to have positive contributions, increasing the odds in favour of the assigned label.
In addition, the values of F5,  F7, or F2 are considered less important when classifying the case under consideration since their respective degrees of influence are very close to zero.
Finally, considering the direction of effect of each input feature, it is valid to conclude that neither F4 nor F10 can be the true label here.
","Predicted class is C1, value of 51.62%.
Other classes and values are C2 48.38%.
Top features are [F8, F9, F3, F1, F4, F10, F5, F7, F2, and F6], with values [-0.10, 0.06, 0.01, -0.01, 0.01, 0.01, 0.01, 0.01, 0.00, and -0.00].
Postive features are [F9, F3, F4, F10, F5, F7, and F2].
Negative features are [F8, F1, and F6].
Lowest impact features are [F10, F5, F7, F2, and F6] with values [0.01, 0.01, 0.01, 0.00, and -0.00]."
"The model predicts class C2 with a 35.74% chance of being the correct label for the case under consideration.
The other two labels, C4 and C3, have a probability of 30.83%, while that of C1 is 33.42%.
According to the attribution analysis, F3 and F6 are the most influential features resulting in the prediction probabilities across the classes.
However, on the other hand, it is important to note that each input feature has a different degree of influence compared to all the remaining ones.
In this case, we can see why the model is not 100.0% certain about the correctness of the assigned label since their respective attributions are very close to zero.
","Predicted class is C2, value of 35.74%.
Other classes and values are C4 30.83%& C1 0.00%& C3 33.42%.
Top features are [F1, F2, F4, F5, F3, and F6], with values [0.08, 0.05, 0.02, 0.00, -0.00, and -0.00].
Postive features are [F1, F2, F4, and F5].
Negative features are [F3 and F6].
Lowest impact features are [F2, F4, F5, F3, and F6] with values [0.05, 0.02, 0.00, -0.00, and -0.00]."
"The classification algorithm labels the given case as C1 with a confidence level equal to 62.34%.
This implies that there is a 37.66% chance that it could be C2.
The above prediction decision is mainly due to the influence of features such as F1, F2, and F3.
On the other hand, the least important features are F4 and  F11.
Looking at the attributions of the input features since their respective degrees of influence are very close to zero, it is not surprising that the algorithm is confident in its label choice for this case.
Among the top influential features, only F7 has a negative contribution, decreasing the odds of being the correct label for the case under consideration.
In contrast, all the remaining positive features have a moderately high impact on the final labelling decision here.
","Predicted class is C1, value of 62.34%.
Other classes and values are C2 37.66%.
Top features are [F1, F2, F3, F7, F8, F9, F10, F5, F6, F4, and F11], with values [0.27, 0.13, -0.04, -0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, and 0.00].
Postive features are [F1, F2, F8, F9, F10, F5, F6, F4, and F11].
Negative features are [F3 and F7].
Lowest impact features are [F10, F5, F6, F4, and F11] with values [0.01, 0.01, 0.01, 0.01, and 0.00]."
"The classifier is 75.63% certain that the correct label for the given case is C1, however, there is a 5.86% chance that it could be C3 and 18.51%.
The classification decision above is mainly due to the influence of features such as F11, F6, and F8.
Among these top features, only four are shown to have negative attributions, increasing the likelihood of labelling the case as C2.
On the other hand, all the remaining features positively support the assigned label with respect to this case.
Other positive features pushing the prediction higher towards one of the two possible labels are F7, or F4.
To sum up, we can see from the attribution analysis that each feature has a different degree of impact on the model in terms of its respective direction of effect.
","Predicted class is C1, value of 75.63%.
Other classes and values are C3 5.86%& C2 18.51%.
Top features are [F11, F6, F8, F7, F4, F9, F2, F5, F10, F12, F3, and F1], with values [0.41, -0.10, -0.06, 0.05, 0.05, 0.04, 0.03, -0.02, 0.02, 0.02, 0.02, and -0.00].
Postive features are [F11, F7, F4, F9, F2, F10, F12, and F3].
Negative features are [F6, F8, F5, and F1].
Lowest impact features are [F5, F10, F12, F3, and F1] with values [-0.02, 0.02, 0.02, 0.02, and -0.00]."
"The model is very confident that the correct label for the given case is C3 since there is only a 23.0% chance that it could be either of the other labels, C1 and C2.
The features with the most influence on the above classification are F9, F6, and F2, whereas those with negative attributions include F7,  F1, or F5.
Considering the direction of influence of each input feature, it can be concluded that F3 has the strongest positive contribution, increasing the odds of being the right label in this case.
On the flip side, all the remaining features have moderate to low contributions, shifting the verdict in favour of an alternative label.
In summary, we can see from the prediction probabilities across the three possible classes that they are less relevant when determining the appropriate label here.
Finally, the confidence level associated with labelling the case as ""C3"" is higher than that of any other class.
","Predicted class is C3, value of 0.0%.
Other classes and values are C1 77.0%& C2 23.0%.
Top features are [F9, F6, F2, F4, F10, F8, F7, F1, F5, and F3], with values [0.25, 0.24, 0.16, 0.16, 0.13, 0.04, 0.03, 0.02, 0.02, and -0.02].
Postive features are [F9, F6, F2, F4, F10, F8, F7, F1, and F5].
Negative features are [F3].
Lowest impact features are [F8, F7, F1, F5, and F3] with values [0.04, 0.03, 0.02, 0.02, and -0.02]."
"The classification algorithm labels the given case as C2 with a confidence level of 90.58%, implying that there is only a 9.42% chance that it could be C1.
The most important features driving the above prediction are F5, F8, and F10.
On the other hand, some of the remaining features have negative attributions, shifting the verdict in the opposite direction.
Among these negative features, only F6 has a positive contribution, increasing the odds of being the correct label for the case under consideration.
However, compared to all the input features mentioned above, we can see why the algorithm is not 100.0% certain about the assigned label here.
Finally, the values of F11,  F9,and F12 are shown to have very little influence on the final labelling decision since their respective degrees of influence are closer to zero.
In summary, when you take into consideration the attribution probabilities across the classes, it is reasonable to deduce that the uncertainty associated with this classification may be due to the fact that each feature contributes negatively towards the least probable class.
","Predicted class is C2, value of 90.58%.
Other classes and values are C1 9.42%.
Top features are [F5, F8, F10, F7, F11, F2, F3, F1, F9, F12, F4, and F6], with values [-0.43, 0.27, -0.24, -0.20, 0.13, -0.08, -0.06, -0.03, 0.02, 0.01, -0.01, and -0.01].
Postive features are [F8, F11, F9, and F12].
Negative features are [F5, F10, F7, F2, F3, F1, F4, and F6].
Lowest impact features are [F1, F9, F12, F4, and F6] with values [-0.03, 0.02, 0.01, -0.01, and -0.01]."
"The classification algorithm labels the given case as C2 with a confidence level of 68.40%, implying that there is only a 31.60% chance that it could be C1.
The above prediction decision is mainly due to the influence of features such as F8, F6, and F2.
On the other hand, all the remaining features are shown to have varying degrees of impact on the algorithm's output verdict for this case.
Among these influential features, only F3 has a negative contribution, decreasing the likelihood of the assigned label in favour of an alternative label.
This can be explained away by looking at the attribution analysis performed to understand the direction of effect of each feature.
In summary, we can conclude that the most relevant features driving the labelling decision here are F7,  F9, or F5 since their respective attributions are very close to zero when determining the correct label for the case under consideration.
To cut a long story short, the positive features outweigh the negative ones, shifting the final decision in the opposite direction.
","Predicted class is C2, value of 68.40%.
Other classes and values are C1 31.60%.
Top features are [F8, F6, F2, F3, F4, F7, F9, F5, and F1], with values [0.02, 0.01, -0.01, -0.01, 0.01, 0.01, 0.00, 0.00, and 0.00].
Postive features are [F8, F6, F4, F7, F9, F5, and F1].
Negative features are [F2 and F3].
Lowest impact features are [F4, F7, F9, F5, and F1] with values [0.01, 0.01, 0.00, 0.00, and 0.00]."
"The classification algorithm labels the given case as C1 with an 83.33% confidence level, implying that there is only a 16.67% chance that it could be C2.
The most important features resulting in the above prediction are F7, F19, and F24.
On the other hand, some of the remaining features have moderate to low impact on the algorithm's decision for this case.
These include F8, or F13, but not all of them are considered relevant when deciding the correct label for the case under consideration.
Among the top influential features, four are shown to have negative attributions, increasing the odds of being the true label here.
Other positive features decreasing the likelihood of arriving at the assigned label can be identified as F6,  F4,and F25.
However, considering the direction of influence of each input feature, it is not surprising that the model is very confident about the correctness of its output verdict.
In summary, we can see from the predicted probabilities across the classes that they are mainly due to the fact that their respective values support labelling the situation differently.
","Predicted class is C1, value of 83.33%.
Other classes and values are C2 16.67%.
Top features are [F7, F19, F24, F1, F5, F16, F22, F18, F8, F13, F26, F3, F21, F2, F14, F10, F9, F6, F4, and F25], with values [0.17, 0.06, -0.04, 0.04, 0.03, 0.03, -0.03, 0.03, 0.03, -0.03, -0.02, -0.02, -0.02, 0.02, 0.02, -0.02, 0.01, 0.01, 0.01, and -0.01].
Postive features are [F7, F19, F1, F5, F16, F18, F8, F2, F14, F9, F6, and F4].
Negative features are [F24, F22, F13, F26, F3, F21, F10, and F25].
Lowest impact features are [F17, F12, F20, F23, and F15] with values [0.00, 0.00, 0.00, 0.00, and 0.00]."
"The classifier is 75.0% certain that C2 is the correct label for the given case since there is only a 25.00% chance that it could be C1.
The classification decision above is mainly based on the influence of features such as F2, F12, and F9.
On the other hand, the least important features are F5, which has a negative contribution to the model's labelling decision with respect to this case.
To cut a long story short, some of the features have positive attributions, while others have negative contributions, shifting the prediction verdict in the opposite direction.
Among all the input features mentioned above, only F10 negatively supports the assignment of an alternative label (C1).
In simple terms, we can see why the confidence level across the classes is higher than that of C3.
Finally, considering the degree of uncertainty associated with the assigned label, it is valid to conclude that the most relevant feature is F11.
","Predicted class is C2, value of 75.00%.
Other classes and values are C1 25.00%.
Top features are [F2, F12, F9, F3, F11, F8, F6, F5, F1, F7, F4, and F10], with values [-0.31, -0.30, 0.22, -0.21, 0.19, -0.18, -0.15, 0.12, -0.07, -0.05, -0.03, and -0.02].
Postive features are [F9, F11, and F5].
Negative features are [F2, F12, F3, F8, F6, F1, F7, F4, and F10].
Lowest impact features are [F5, F1, F7, F4, and F10] with values [0.12, -0.07, -0.05, -0.03, and -0.02]."
"The model predicts class C1 with 100.0% certainty, implying that there is little to no chance that the true label for the given case is C2.
The classification decision above is mainly due to the influence of features such as F4, F6, and F3.
On the other hand, all the remaining features are shown to have negative attributions, shifting the prediction verdict in a different direction.
In this case, we can see why the model is very confident about the correctness of the assigned label.","Predicted class is C1, value of 100.00%.
Other classes and values are C2 0.00%.
Top features are [F4, F6, F3, F5, F2, and F1], with values [0.32, -0.13, -0.11, -0.06, -0.03, and 0.01].
Postive features are [F4 and F1].
Negative features are [F6, F3, F5, and F2].
Lowest impact features are [F6, F3, F5, F2, and F1] with values [-0.13, -0.11, -0.06, -0.03, and 0.01]."

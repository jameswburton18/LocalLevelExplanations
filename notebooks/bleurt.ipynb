{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch length of `text`: 1 does not match batch length of `text_pair`: 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m candidates \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhi universe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbye world\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 12\u001b[0m   scores \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer(references, candidates, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m))[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(scores) \u001b[39m# tensor([1.0327, 0.2055])\u001b[39;00m\n",
      "File \u001b[0;32m~/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2518\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2519\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2520\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2521\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2522\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   2597\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwhen tokenizing batches of text, `text_pair` must be a list or tuple with the same length as\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2598\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `text`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2599\u001b[0m     )\n\u001b[1;32m   2600\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(text_pair):\n\u001b[0;32m-> 2601\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2602\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2603\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2604\u001b[0m     )\n\u001b[1;32m   2605\u001b[0m batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m   2606\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2607\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2608\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2623\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2624\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: batch length of `text`: 1 does not match batch length of `text_pair`: 2."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Elron/bleurt-base-512\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Elron/bleurt-base-512\")\n",
    "model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0327, 0.2055])\n"
     ]
    }
   ],
   "source": [
    "references = [\"hello world\", \"hello world\"]\n",
    "candidates = [\"hi universe\", \"bye world\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
    "\n",
    "print(scores) # tensor([1.0327, 0.2055])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0436)\n"
     ]
    }
   ],
   "source": [
    "references = [f'Given that the likelihoods of class C1 and class C2 are 36.38% and 63.62%, respectively, the case under consideration is labelled as C2 by the model. The top relevant features for this prediction decision are F9, F12, F16, and F6, while the least relevant features include F5 and F1. From the analysis performed, the values of F9, F12, and F16 are shown to have strong positive support for C2, while F6 and F13 are the top negative attributes, pulling the prediction towards C1. However, the combined magnitude of their impacts on the model is less than that of the positive input features mentioned above, hence the moderately strong push towards labelling the case as C2. The values of some features show a low influence on C2 prediction and these are F2, F10, F5, and F1.']\n",
    "candidates = ['The model predicted class C2 for the given case with a confidence level of 63.62%. However, there is a 36.38% chance that it could be C1 instead. The above prediction decision is mainly due to the influence of features such as F9, F6, and F12. On the other hand, the least important features are F5 and  F1. In terms of the direction of influence or contribution of each input feature, only F3 has a negative impact, shifting the prediction verdict in favour of an alternative label. Finally, on the contrary, all the remaining features have positive contributions, increasing the likelihood of labelling the provided example as \"C2\". All things considered, we can conclude that the uncertainty associated with the classification decision here may be attributed to some combination of negative features coupled with strong positive attributions from F11, F7, And F4.']\n",
    "\n",
    "with torch.no_grad():\n",
    "  scores = model(**tokenizer(references, candidates, return_tensors='pt'))[0].squeeze()\n",
    "\n",
    "print(scores) # tensor([1.0327, 0.2055])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.5, 'rouge2': 0.11510791366906475, 'rougeL': 0.2571428571428572, 'rougeLsum': 0.2571428571428572}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "# predictions = [\"hello there\", \"general kenobi\"]\n",
    "# references = [\"hello there\", \"general kenobi\"]\n",
    "results = rouge.compute(predictions=candidates,\n",
    "                         references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f53d471a03fb5b9741311ec5f82522ec5f217d64ed47634b801d3f5199a0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workbook I want to calculate the automatic metrics from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from src.utils import (\n",
    "    linearise_input, convert_to_features, form_stepwise_input, \n",
    "    simplify_feat_names,\n",
    "    label_qs,\n",
    "    simplify_narr_question,\n",
    "    nums_to_names\n",
    ")\n",
    "from evaluate import load\n",
    "from torchmetrics.text.infolm import InfoLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration james-burton--textual-explanations-702010-cac443d3271dff16\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-702010-cac443d3271dff16/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 673.13it/s]\n",
      "100%|██████████| 94/94 [00:00<00:00, 2725.87ex/s]\n",
      "100%|██████████| 328/328 [00:00<00:00, 2790.71ex/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 2837.74ex/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"james-burton/textual-explanations-702010\")\n",
    "\n",
    "# Form the linearised or stepwise (and linearised) input\n",
    "dataset = dataset.map(\n",
    "    lambda x: linearise_input(x, 'baseline_input', 20),\n",
    "    load_from_cache_file=False\n",
    "    ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/james/.cache/huggingface/metrics/bleurt/default/downloads/extracted/d6b01862d09a8feced08a9ee5a0c887edbafd7f450ddd83f5907da1bfbbc8754/bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 11:47:04.130256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-29 11:47:04.130461: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-29 11:47:04.143353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-29 11:47:04.143393: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-29 11:47:04.143717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/james/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/james/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/james/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the predictions\n",
    "bleurt = load('bleurt',checkpoint=\"bleurt-base-512\")\n",
    "bleu = load('bleu')\n",
    "meteor = load('meteor')\n",
    "\n",
    "results = {}\n",
    "# preds = dataset['input']\n",
    "# refs = dataset['narration']\n",
    "\n",
    "narrs_w_names = [nums_to_names(narr, eval(c2s), eval(f2s))\n",
    "                        for narr, c2s, f2s\n",
    "                        in zip(dataset['test']['narration'],\n",
    "                            dataset['test']['class2name'],\n",
    "                            dataset['test']['ft_num2name'])]\n",
    "input_w_names = [nums_to_names(inp, eval(c2s), eval(f2s))\n",
    "                        for inp, c2s, f2s\n",
    "                        in zip(dataset['test']['input'],\n",
    "                            dataset['test']['class2name'],\n",
    "                            dataset['test']['ft_num2name'])]\n",
    "\n",
    "preds = narrs_w_names\n",
    "refs = input_w_names\n",
    "        \n",
    "bleurt_results = bleurt.compute(predictions=preds,\n",
    "                                references=refs)\n",
    "bleu_results = bleu.compute(predictions=preds,\n",
    "                            references=[[r] for r in refs])\n",
    "meteor_results = meteor.compute(predictions=preds,\n",
    "                                references=[[r] for r in refs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.92it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-5.7852)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infolm = InfoLM('google/bert_uncased_L-2_H-128_A-2', idf=False)\n",
    "infolm(preds, refs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InfoLM but using the predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = '''radiant-snake-164\n",
    "        brilliant-paper-165\n",
    "        beaming-kumquat-166\n",
    "        dancing-goat-167\n",
    "        vivid-peony-175\n",
    "        prosperous-laughter-169\n",
    "        incandescent-goat-171\n",
    "        red-peony-172\n",
    "        alight-rat-173\n",
    "        red-laughter-174'''.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.88it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.87it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.81it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.80it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.86it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.78it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.79it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.84it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.85it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.77it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'radiant-snake-164': tensor(-5.5911), 'brilliant-paper-165': tensor(-4.5172), 'beaming-kumquat-166': tensor(-5.2534), 'dancing-goat-167': tensor(-4.8911), 'vivid-peony-175': tensor(-5.6877), 'prosperous-laughter-169': tensor(-4.3350), 'incandescent-goat-171': tensor(-4.7722), 'red-peony-172': tensor(-4.9977), 'alight-rat-173': tensor(-5.0640), 'red-laughter-174': tensor(-4.6484)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_dict = {}\n",
    "for i, run in enumerate(runs):\n",
    "    model = 't5-base' if i % 2 == 0 else 'bart-base'\n",
    "    with open(f'../models/{model}/{run}/test_predictions.txt') as f:\n",
    "        preds = f.readlines()\n",
    "    run_dict[run] = infolm(preds, refs)\n",
    "print(run_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Predicted class is \"Return\", value of 100.00%. Other classes and values are \"Go Away\" 0.00%. Top features are Perference(P2), Delay of delivery person picking up food, and Ease and convenient. Postive features are Ease and convenient. Negative features are Perference(P2) and Delay of delivery person picking up food. Lowest impact features are Educational Qualifications, Gender, and Good Quantity.',\n",
       " 'Predicted class is \"More\", value of 91.95%. Other classes and values are \"Less\" 8.05%. Top features are X24, X8, and X1. Postive features are X24 and X1. Negative features are X8. Lowest impact features are X13, X12, and X26.',\n",
       " 'Predicted class is \"Stay\", value of 89.16%. Other classes and values are \"Leave\" 10.84%. Top features are IsActiveMember, Age, and Geography. Postive features are Age and Geography. Negative features are IsActiveMember. Lowest impact features are Balance, EstimatedSalary, and HasCrCard.',\n",
       " 'Predicted class is \"Luxury\", value of 97.02%. Other classes and values are \"Basic\" 2.98%. Top features are isNewBuilt, hasYard, and hasPool. Postive features are isNewBuilt, hasYard, and hasPool. Negative features are  . Lowest impact features are cityPartRange, hasStorageRoom, and attic.',\n",
       " 'Predicted class is \"On-time\", value of 51.62%. Other classes and values are \"Late\" 48.38%. Top features are Discount_offered, Weight_in_gms, and Product_importance. Postive features are Weight_in_gms and Product_importance. Negative features are Discount_offered. Lowest impact features are Gender, Mode_of_Shipment, and Customer_care_calls.',\n",
       " 'Predicted class is \"Cleaning\", value of 82.81%. Other classes and values are \"Preparing meals\" 0.08%& \"Presence of smoke\" 1.60%& \"Other\" 15.52%. Top features are MQ5, MQ3, and MQ6. Postive features are MQ5, MQ3, and MQ6. Negative features are  . Lowest impact features are MQ1, MQ4, and MQ2.',\n",
       " 'Predicted class is \"Under 50K\", value of 97.82%. Other classes and values are \"Above 50K\" 2.18%. Top features are Capital Gain, Marital Status, and Relationship. Postive features are Capital Gain, Marital Status, and Relationship. Negative features are  . Lowest impact features are fnlwgt, Country, and Workclass.',\n",
       " 'Predicted class is \"High\", value of 68.12%. Other classes and values are \"Low\" 2.01%& \"Medium\" 29.87%. Top features are Trip_Distance, Cancellation_Last_1Month, and Customer_Rating. Postive features are Trip_Distance and Cancellation_Last_1Month. Negative features are Customer_Rating. Lowest impact features are Confidence_Life_Style_Index, Var2, and Type_of_Cab.',\n",
       " 'Predicted class is \"Return\", value of 80.00%. Other classes and values are \"Go Away\" 20.00%. Top features are Marital Status, Perference(P2), and Delay of delivery person picking up food. Postive features are Perference(P2). Negative features are Marital Status and Delay of delivery person picking up food. Lowest impact features are Monthly Income, Gender, and Good Quantity.',\n",
       " 'Predicted class is \"Medium\", value of 83.00%. Other classes and values are \"Low\" 3.00%& \"High\" 14.00%. Top features are Type_of_Cab, Destination_Type, and Trip_Distance. Postive features are Type_of_Cab. Negative features are Destination_Type and Trip_Distance. Lowest impact features are Gender, Var1, and Customer_Rating.',\n",
       " 'Predicted class is \"Low\", value of 100.00%. Other classes and values are \"Moderate\" 0.00%& \"High\" 0.00%. Top features are Duration_hours, Airline, and Total_Stops. Postive features are Duration_hours, Airline, and Total_Stops. Negative features are  . Lowest impact features are Arrival_hour, Duration_mins, and Dep_hour.',\n",
       " 'Predicted class is \"high quality\", value of 60.00%. Other classes and values are \"low_quality\" 40.00%. Top features are alcohol, sulphates, and total sulfur dioxide. Postive features are alcohol and total sulfur dioxide. Negative features are sulphates. Lowest impact features are citric acid, residual sugar, and free sulfur dioxide.',\n",
       " 'Predicted class is \"Fraud\", value of 100.00%. Other classes and values are \"Not Fraud\" 0.00%. Top features are Z4, Z14, and Z17. Postive features are Z4, Z14, and Z17. Negative features are  . Lowest impact features are Z12, Z28, and Z15.',\n",
       " 'Predicted class is \"r1\", value of 99.45%. Other classes and values are \"r2\" 0.47%& \"r3\" 0.04%& \"r4\" 0.05%. Top features are ram, battery_power, and px_height. Postive features are ram and battery_power. Negative features are px_height. Lowest impact features are talk_time, sc_h, and blue.',\n",
       " 'Predicted class is \"Go Away\", value of 97.94%. Other classes and values are \"Return\" 2.06%. Top features are Time saving, More Offers and Discount, and Perference(P2). Postive features are Time saving and More Offers and Discount. Negative features are Perference(P2). Lowest impact features are Educational Qualifications, Marital Status, and Long delivery time.',\n",
       " 'Predicted class is \"Medium\", value of 88.00%. Other classes and values are \"Low\" 2.00%& \"High\" 10.00%. Top features are Type_of_Cab, Destination_Type, and Trip_Distance. Postive features are Type_of_Cab. Negative features are Destination_Type and Trip_Distance. Lowest impact features are Gender, Var1, and Customer_Rating.',\n",
       " 'Predicted class is \"Not Fraud\", value of 93.32%. Other classes and values are \"Fraud\" 6.68%. Top features are incident_severity, incident_city, and injury_claim. Postive features are incident_severity and incident_city. Negative features are injury_claim. Lowest impact features are policy_csl, policy_state, and capital-loss.',\n",
       " 'Predicted class is \"High\", value of 75.63%. Other classes and values are \"Low\" 5.86%& \"Moderate\" 18.51%. Top features are Total_Stops, Airline, and Journey_day. Postive features are Total_Stops. Negative features are Airline and Journey_day. Lowest impact features are Duration_hours, Dep_minute, and Duration_mins.',\n",
       " 'Predicted class is \"high quality\", value of 94.37%. Other classes and values are \"low_quality\" 5.63%. Top features are volatile acidity, sulphates, and alcohol. Postive features are volatile acidity, sulphates, and alcohol. Negative features are  . Lowest impact features are free sulfur dioxide, pH, and density.',\n",
       " 'Predicted class is \"Low\", value of 100.00%. Other classes and values are \"High\" 0.00%. Top features are CRIM, LSTAT, and RAD. Postive features are CRIM and LSTAT. Negative features are RAD. Lowest impact features are RM, NOX, and INDUS.',\n",
       " 'Predicted class is \"Dissatisfied\", value of 58.75%. Other classes and values are \"Satisfied\" 41.25%. Top features are Communication with dr, friendly health care workers, and Exact diagnosis. Postive features are Communication with dr, friendly health care workers, and Exact diagnosis. Negative features are  . Lowest impact features are waiting rooms, parking, playing rooms, caffes, and hospital rooms quality.',\n",
       " 'Predicted class is \"Weak\", value of 90.65%. Other classes and values are \"Strong\" 9.35%. Top features are water, cement, and age_days. Postive features are water, cement, and age_days. Negative features are  . Lowest impact features are coarseaggregate, fineaggregate, and slag.',\n",
       " 'Predicted class is \"More than 5\", value of 80.35%. Other classes and values are \"Less than 5\" 19.65%. Top features are 3PointMade, 3PointAttempt, and FreeThrowMade. Postive features are 3PointMade and FreeThrowMade. Negative features are 3PointAttempt. Lowest impact features are FreeThrowPercent, Turnovers, and 3PointPercent.',\n",
       " 'Predicted class is \"Basic\", value of 99.30%. Other classes and values are \"Luxury\" 0.70%. Top features are isNewBuilt, hasYard, and hasPool. Postive features are isNewBuilt and hasPool. Negative features are hasYard. Lowest impact features are cityPartRange, garage, and hasStorageRoom.',\n",
       " 'Predicted class is \"Low\", value of 64.11%. Other classes and values are \"High\" 35.89%. Top features are LSTAT, TAX, and RM. Postive features are LSTAT and TAX. Negative features are RM. Lowest impact features are CRIM, NOX, and DIS.',\n",
       " 'Predicted class is \"High\", value of 100.00%. Other classes and values are \"Low\" 0.00%. Top features are fea_4, fea_8, and fea_5. Postive features are fea_4. Negative features are fea_8 and fea_5. Lowest impact features are fea_3, fea_7, and fea_10.',\n",
       " 'Predicted class is \"No\", value of 99.00%. Other classes and values are \"Yes\" 1.00%. Top features are  Net Income to Stockholder\\'s Equity,  Continuous interest rate (after tax), and  ROA(C) before interest and depreciation before interest. Postive features are  Net Income to Stockholder\\'s Equity,  Continuous interest rate (after tax), and  ROA(C) before interest and depreciation before interest. Negative features are  . Lowest impact features are  Quick Asset Turnover Rate,  Cash\\\\/Current Liability, and  Gross Profit to Sales.',\n",
       " 'Predicted class is \"Normal\", value of 100.00%. Other classes and values are \"Suspicious\" 0.00%. Top features are Z3, Z9, and Z4. Postive features are Z3. Negative features are Z9 and Z4. Lowest impact features are Z2, Z6, and Z7.',\n",
       " 'Predicted class is \"Leave\", value of 96.77%. Other classes and values are \"Leave\" 3.23%. Top features are OverTime, BusinessTravel, and JobSatisfaction. Postive features are OverTime. Negative features are BusinessTravel and JobSatisfaction. Lowest impact features are YearsAtCompany, TrainingTimesLastYear, and StockOptionLevel.',\n",
       " 'Predicted class is \"Leave\", value of 57.83%. Other classes and values are \"Leave\" 42.17%. Top features are OverTime, NumCompaniesWorked, and RelationshipSatisfaction. Postive features are OverTime, NumCompaniesWorked, and RelationshipSatisfaction. Negative features are  . Lowest impact features are HourlyRate, DistanceFromHome, and JobSatisfaction.',\n",
       " 'Predicted class is \"Invest\", value of 96.35%. Other classes and values are \"Ignore\" 3.65%. Top features are Feature7, Feature4, and Feature14. Postive features are Feature7, Feature4, and Feature14. Negative features are  . Lowest impact features are Feature20, Feature3, and Feature17.',\n",
       " 'Predicted class is \"Low\", value of 93.00%. Other classes and values are \"High\" 7.00%. Top features are fea_4, fea_10, and fea_8. Postive features are fea_4 and fea_8. Negative features are fea_10. Lowest impact features are fea_9, fea_6, and fea_11.',\n",
       " 'Predicted class is \"Good Credit\", value of 72.93%. Other classes and values are \"Bad Credit\" 27.07%. Top features are Checking account, Duration, and Housing. Postive features are Checking account. Negative features are Duration and Housing. Lowest impact features are Purpose, Job, and Credit amount.',\n",
       " 'Predicted class is \"Not Fraud\", value of 51.60%. Other classes and values are \"Fraud\" 48.40%. Top features are incident_severity, insured_hobbies, and authorities_contacted. Postive features are insured_hobbies. Negative features are incident_severity and authorities_contacted. Lowest impact features are total_claim_amount, injury_claim, and auto_year.',\n",
       " 'Predicted class is \"Not Placed\", value of 98.21%. Other classes and values are \"Placed\" 1.79%. Top features are specialisation, workex, and ssc_p. Postive features are specialisation, workex, and ssc_p. Negative features are  . Lowest impact features are mba_p, hsc_s, and ssc_b.',\n",
       " 'Predicted class is \"Skip\", value of 91.30%. Other classes and values are \"Watch\" 9.70%. Top features are Daily Time Spent on Site, Daily Internet Usage, and Age. Postive features are Daily Time Spent on Site and Age. Negative features are Daily Internet Usage. Lowest impact features are ad_day, Gender, and ad_month.',\n",
       " 'Predicted class is \"Class 1\", value of 100.00%. Other classes and values are \"Class 2\" 0.00%. Top features are A8, A9, and A10. Postive features are A8, A9, and A10. Negative features are  . Lowest impact features are A13, A1, and A2.',\n",
       " 'Predicted class is \"Return\", value of 82.00%. Other classes and values are \"Go Away\" 18.00%. Top features are Marital Status, Perference(P2), and Delay of delivery person picking up food. Postive features are Perference(P2). Negative features are Marital Status and Delay of delivery person picking up food. Lowest impact features are Monthly Income, Gender, and Good Quantity.',\n",
       " 'Predicted class is \"satisfied\", value of 100.00%. Other classes and values are \"neutral or dissatisfied\" 0.00%. Top features are Inflight wifi service, Type of Travel, and Customer Type. Postive features are Inflight wifi service, Type of Travel, and Customer Type. Negative features are  . Lowest impact features are Flight Distance, Food and drink, and Checkin service.',\n",
       " 'Predicted class is \"Low\", value of 65.07%. Other classes and values are \"High\" 34.93%. Top features are fea_4, fea_2, and fea_10. Postive features are fea_4 and fea_2. Negative features are fea_10. Lowest impact features are fea_8, fea_7, and fea_9.',\n",
       " 'Predicted class is \"Low\", value of 99.72%. Other classes and values are \"High\" 0.28%. Top features are Transmission, Fuel_Type, and Seats. Postive features are Transmission, Fuel_Type, and Seats. Negative features are  . Lowest impact features are Power, Mileage, and Kilometers_Driven.',\n",
       " 'Predicted class is \"Accept\", value of 65.51%. Other classes and values are \"Reject\" 34.49%. Top features are CD Account, Income, and CCAvg. Postive features are CD Account and Income. Negative features are CCAvg. Lowest impact features are Age, Family, and Extra_service.',\n",
       " 'Predicted class is \"Leave\", value of 71.87%. Other classes and values are \"Stay\" 28.13%. Top features are city, city_development_index, and experience. Postive features are city, city_development_index, and experience. Negative features are  . Lowest impact features are training_hours, last_new_job, and company_size.',\n",
       " 'Predicted class is \"Fraud\", value of 61.74%. Other classes and values are \"Not Fraud\" 38.26%. Top features are incident_severity, insured_hobbies, and insured_occupation. Postive features are incident_severity. Negative features are insured_hobbies and insured_occupation. Lowest impact features are policy_state, total_claim_amount, and auto_year.',\n",
       " 'Predicted class is \"r1\", value of 100.00%. Other classes and values are \"r2\" 0.00%& \"r3\" 0.00%& \"r4\" 0.00%. Top features are ram, battery_power, and px_width. Postive features are ram and px_width. Negative features are battery_power. Lowest impact features are blue, three_g, and n_cores.',\n",
       " 'Predicted class is \"Fraud\", value of 100.00%. Other classes and values are \"Not Fraud\" 0.00%. Top features are Z4, Z14, and Z17. Postive features are Z4, Z14, and Z17. Negative features are  . Lowest impact features are Z12, Z28, and Z15.',\n",
       " 'Predicted class is \"Acceptable\", value of 100.00%. Other classes and values are \"Unacceptable\" 0.00%. Top features are persons, buying, and safety. Postive features are persons, buying, and safety. Negative features are  . Lowest impact features are maint, lug_boot, and doors.',\n",
       " 'Predicted class is \"Invest\", value of 99.28%. Other classes and values are \"Ignore\" 0.72%. Top features are Feature7, Feature4, and Feature2. Postive features are Feature7 and Feature4. Negative features are Feature2. Lowest impact features are Feature11, Feature9, and Feature19.',\n",
       " 'Predicted class is \"r2\", value of 95.09%. Other classes and values are \"r1\" 1.06%& \"r3\" 3.85%& \"r4\" 0.00%. Top features are ram, touch_screen, and talk_time. Postive features are ram. Negative features are touch_screen and talk_time. Lowest impact features are sc_w, px_height, and battery_power.',\n",
       " 'Predicted class is \"Leave\", value of 63.62%. Other classes and values are \"Stay\" 36.38%. Top features are feature6, feature5, and feature4. Postive features are feature6 and feature4. Negative features are feature5. Lowest impact features are feature12, feature2, and feature8.',\n",
       " 'Predicted class is \"Acceptable\", value of 100.00%. Other classes and values are \"Unacceptable\" 0.00%. Top features are persons, buying, and safety. Postive features are persons, buying, and safety. Negative features are  . Lowest impact features are maint, lug_boot, and doors.',\n",
       " 'Predicted class is \"Good Credit\", value of 64.62%. Other classes and values are \"Bad Credit\" 35.38%. Top features are Saving accounts, Sex, and Duration. Postive features are Sex. Negative features are Saving accounts and Duration. Lowest impact features are Credit amount, Age, and Job.',\n",
       " 'Predicted class is \"High\", value of 73.58%. Other classes and values are \"Low\" 4.16%& \"Moderate\" 22.27%. Top features are Airline, Journey_month, and Duration_hours. Postive features are Airline. Negative features are Journey_month and Duration_hours. Lowest impact features are Journey_day, Dep_minute, and Destination.',\n",
       " 'Predicted class is \"Presence of smoke\", value of 35.74%. Other classes and values are \"Preparing meals\" 30.83%& \"Cleaning\" 0.00%& \"Other\" 33.42%. Top features are MQ5, MQ3, and MQ2. Postive features are MQ5, MQ3, and MQ2. Negative features are  . Lowest impact features are MQ6, MQ1, and MQ4.',\n",
       " 'Predicted class is \"Normal\", value of 100.00%. Other classes and values are \"Suspicious\" 0.00%. Top features are Z3, Z9, and Z4. Postive features are Z3. Negative features are Z9 and Z4. Lowest impact features are Z2, Z6, and Z7.',\n",
       " 'Predicted class is \"Above 50K\", value of 97.71%. Other classes and values are \"Under 50K\" 2.29%. Top features are Capital Gain, Marital Status, and Capital Loss. Postive features are Capital Gain and Marital Status. Negative features are Capital Loss. Lowest impact features are Workclass, fnlwgt, and Race.',\n",
       " 'Predicted class is \"Yes\", value of 100.00%. Other classes and values are \"No\" 0.00%. Top features are  Liability to Equity,  Net worth\\\\/Assets, and  Debt ratio %. Postive features are  Debt ratio %. Negative features are  Liability to Equity and  Net worth\\\\/Assets. Lowest impact features are  Operating profit per person,  Average Collection Days, and  Interest-bearing debt interest rate.',\n",
       " 'Predicted class is \"Class 1\", value of 97.03%. Other classes and values are \"Class 2\" 2.97%. Top features are A8, A14, and A9. Postive features are A8, A14, and A9. Negative features are  . Lowest impact features are A7, A3, and A2.',\n",
       " 'Predicted class is \"More than 5\", value of 80.35%. Other classes and values are \"Less than 5\" 19.65%. Top features are 3PointMade, 3PointAttempt, and FreeThrowMade. Postive features are 3PointMade and FreeThrowMade. Negative features are 3PointAttempt. Lowest impact features are FreeThrowPercent, Turnovers, and 3PointPercent.',\n",
       " 'Predicted class is \"Low\", value of 100.00%. Other classes and values are \"High\" 0.00%. Top features are Power, car_age, and Transmission. Postive features are Power, car_age, and Transmission. Negative features are  . Lowest impact features are Name, Kilometers_Driven, and Owner_Type.',\n",
       " 'Predicted class is \"< 10k\", value of 70.00%. Other classes and values are \"> 10k\" 30.00%. Top features are X6, X11, and X16. Postive features are X6 and X16. Negative features are X11. Lowest impact features are X12, X18, and X9.',\n",
       " 'Predicted class is \"Good Credit\", value of 87.50%. Other classes and values are \"Bad Credit\" 12.50%. Top features are Checking account, Saving accounts, and Purpose. Postive features are Checking account. Negative features are Saving accounts and Purpose. Lowest impact features are Age, Job, and Credit amount.',\n",
       " 'Predicted class is \"No\", value of 98.80%. Other classes and values are \"Yes\" 1.20%. Top features are X38, X32, and X22. Postive features are X38 and X32. Negative features are X22. Lowest impact features are X11, X8, and X29.',\n",
       " 'Predicted class is \"Less than 500\", value of 75.00%. Other classes and values are \"More than 500\" 25.00%. Top features are Snowfall (cm), Functioning Day, and Hour. Postive features are Hour. Negative features are Snowfall (cm) and Functioning Day. Lowest impact features are Visibility (10m), Holiday, and Wind speed (m\\\\/s).',\n",
       " 'Predicted class is \"Leave\", value of 84.87%. Other classes and values are \"Stay\" 15.13%. Top features are city, enrolled_university, and relevent_experience. Postive features are city and enrolled_university. Negative features are relevent_experience. Lowest impact features are company_size, company_type, and training_hours.',\n",
       " 'Predicted class is \"Preparing meals\", value of 83.08%. Other classes and values are \"Presence of smoke\" 16.87%& \"Cleaning\" 0.00%& \"Other\" 0.05%. Top features are MQ5, MQ3, and MQ1. Postive features are MQ3 and MQ1. Negative features are MQ5. Lowest impact features are MQ4, MQ6, and MQ2.',\n",
       " 'Predicted class is \"More than 5\", value of 83.98%. Other classes and values are \"Less than 5\" 16.02%. Top features are GamesPlayed, OffensiveRebounds, and FreeThrowPercent. Postive features are GamesPlayed, OffensiveRebounds, and FreeThrowPercent. Negative features are  . Lowest impact features are PointsPerGame, Steals, and FreeThrowMade.',\n",
       " 'Predicted class is \"Satisfied\", value of 56.51%. Other classes and values are \"Dissatisfied\" 43.49%. Top features are Quality\\\\/experience dr., Exact diagnosis, and Hygiene and cleaning. Postive features are Quality\\\\/experience dr., Exact diagnosis, and Hygiene and cleaning. Negative features are  . Lowest impact features are Check up appointment, lab services, and Time of appointment.',\n",
       " 'Predicted class is \"Bad Credit\", value of 51.69%. Other classes and values are \"Good Credit\" 48.31%. Top features are Checking account, Saving accounts, and Duration. Postive features are Saving accounts and Duration. Negative features are Checking account. Lowest impact features are Purpose, Age, and Job.',\n",
       " 'Predicted class is \"Stay\", value of 85.13%. Other classes and values are \"Leave\" 14.87%. Top features are feature3, feature15, and feature11. Postive features are feature15. Negative features are feature3 and feature11. Lowest impact features are feature2, feature8, and feature1.',\n",
       " 'Predicted class is \"Ignore\", value of 95.09%. Other classes and values are \"Promote\" 4.91%. Top features are avg_training_score, department, and recruitment_channel. Postive features are department. Negative features are avg_training_score and recruitment_channel. Lowest impact features are region, no_of_trainings, and gender.',\n",
       " 'Predicted class is \"Class 1\", value of 100.00%. Other classes and values are \"Class 2\" 0.00%. Top features are A8, A9, and A10. Postive features are A8, A9, and A10. Negative features are  . Lowest impact features are A5, A1, and A2.',\n",
       " 'Predicted class is \"Class 1\", value of 100.00%. Other classes and values are \"Class 2\" 0.00%. Top features are A8, A9, and A10. Postive features are A8, A9, and A10. Negative features are  . Lowest impact features are A5, A1, and A2.',\n",
       " 'Predicted class is \"Yes\", value of 85.00%. Other classes and values are \"No\" 15.00%. Top features are X32, X38, and X35. Postive features are X32 and X35. Negative features are X38. Lowest impact features are X9, X8, and X29.',\n",
       " 'Predicted class is \"Watch\", value of 100.00%. Other classes and values are \"Skip\" 0.00%. Top features are Daily Time Spent on Site, Area Income, and Age. Postive features are Daily Time Spent on Site, Area Income, and Age. Negative features are  . Lowest impact features are ad_day, Gender, and ad_month.',\n",
       " 'Predicted class is \"Not Fraud\", value of 99.84%. Other classes and values are \"Fraud\" 0.16%. Top features are  ERC20 uniq rec contract addr,  ERC20 uniq rec token name, and Sent tnx. Postive features are  ERC20 uniq rec contract addr and  ERC20 uniq rec token name. Negative features are Sent tnx. Lowest impact features are Unique Sent To Addresses,  ERC20 max val rec, and  Total ERC20 tnxs.',\n",
       " 'Predicted class is \"r2\", value of 95.09%. Other classes and values are \"r1\" 1.06%& \"r3\" 3.85%& \"r4\" 0.00%. Top features are ram, touch_screen, and talk_time. Postive features are ram. Negative features are touch_screen and talk_time. Lowest impact features are sc_w, px_height, and battery_power.',\n",
       " 'Predicted class is \"neutral or dissatisfied\", value of 99.72%. Other classes and values are \"satisfied\" 0.28%. Top features are Type of Travel, Customer Type, and Inflight entertainment. Postive features are Type of Travel and Inflight entertainment. Negative features are Customer Type. Lowest impact features are Gender, Flight Distance, and Leg room service.',\n",
       " 'Predicted class is \"Accept\", value of 65.51%. Other classes and values are \"Reject\" 34.49%. Top features are CD Account, Income, and CCAvg. Postive features are CD Account and Income. Negative features are CCAvg. Lowest impact features are Age, Family, and Extra_service.',\n",
       " 'Predicted class is \"Leave\", value of 56.00%. Other classes and values are \"Leave\" 44.00%. Top features are OverTime, BusinessTravel, and MaritalStatus. Postive features are MaritalStatus. Negative features are OverTime and BusinessTravel. Lowest impact features are MonthlyIncome, JobLevel, and DistanceFromHome.',\n",
       " 'Predicted class is \"Acceptable\", value of 100.00%. Other classes and values are \"Unacceptable\" 0.00%. Top features are persons, safety, and maint. Postive features are persons, safety, and maint. Negative features are  . Lowest impact features are buying, lug_boot, and doors.',\n",
       " 'Predicted class is \"Promote\", value of 67.08%. Other classes and values are \"Ignore\" 32.92%. Top features are KPIs_met >80%, avg_training_score, and previous_year_rating. Postive features are avg_training_score and previous_year_rating. Negative features are KPIs_met >80%. Lowest impact features are gender, length_of_service, and age.',\n",
       " 'Predicted class is \"Medium\", value of 66.11%. Other classes and values are \"Low\" 31.78%& \"High\" 2.11%. Top features are Type_of_Cab, Trip_Distance, and Destination_Type. Postive features are Type_of_Cab and Destination_Type. Negative features are Trip_Distance. Lowest impact features are Var1, Customer_Rating, and Var2.',\n",
       " 'Predicted class is \"Yes\", value of 98.17%. Other classes and values are \"No\" 1.83%. Top features are  Liability to Equity,  Total Asset Return Growth Rate Ratio, and  Continuous interest rate (after tax). Postive features are  . Negative features are  Liability to Equity,  Total Asset Return Growth Rate Ratio, and  Continuous interest rate (after tax). Lowest impact features are  ROA(C) before interest and depreciation before interest,  ROA(A) before interest and % after tax, and  Gross Profit to Sales.',\n",
       " 'Predicted class is \"On-time\", value of 51.62%. Other classes and values are \"Late\" 48.38%. Top features are Discount_offered, Weight_in_gms, and Product_importance. Postive features are Weight_in_gms and Product_importance. Negative features are Discount_offered. Lowest impact features are Gender, Mode_of_Shipment, and Customer_care_calls.',\n",
       " 'Predicted class is \"Presence of smoke\", value of 35.74%. Other classes and values are \"Preparing meals\" 30.83%& \"Cleaning\" 0.00%& \"Other\" 33.42%. Top features are MQ5, MQ3, and MQ2. Postive features are MQ5, MQ3, and MQ2. Negative features are  . Lowest impact features are MQ6, MQ1, and MQ4.',\n",
       " 'Predicted class is \"Promote\", value of 62.34%. Other classes and values are \"Ignore\" 37.66%. Top features are department, avg_training_score, and recruitment_channel. Postive features are department and avg_training_score. Negative features are recruitment_channel. Lowest impact features are education, previous_year_rating, and gender.',\n",
       " 'Predicted class is \"High\", value of 75.63%. Other classes and values are \"Low\" 5.86%& \"Moderate\" 18.51%. Top features are Total_Stops, Airline, and Journey_day. Postive features are Total_Stops. Negative features are Airline and Journey_day. Lowest impact features are Duration_hours, Dep_minute, and Duration_mins.',\n",
       " 'Predicted class is \"Low\", value of 0.0%. Other classes and values are \"High\" 77.0%& \"Medium\" 23.0%. Top features are Power, car_age, and Transmission. Postive features are Power, car_age, and Transmission. Negative features are  . Lowest impact features are Name, Kilometers_Driven, and Owner_Type.',\n",
       " 'Predicted class is \"Less than 500\", value of 90.58%. Other classes and values are \"More than 500\" 9.42%. Top features are Functioning Day, Solar Radiation (MJ\\\\/m2), and Rainfall(mm). Postive features are Solar Radiation (MJ\\\\/m2). Negative features are Functioning Day and Rainfall(mm). Lowest impact features are Dew point temperature, Seasons, and Wind speed (m\\\\/s).',\n",
       " 'Predicted class is \"Not Portable\", value of 68.40%. Other classes and values are \"Portable\" 31.60%. Top features are Sulfate, Hardness, and Turbidity. Postive features are Sulfate and Hardness. Negative features are Turbidity. Lowest impact features are Trihalomethanes, Organic_carbon, and Chloramines.',\n",
       " 'Predicted class is \"Less\", value of 83.33%. Other classes and values are \"More\" 16.67%. Top features are X24, X1, and X4. Postive features are X24 and X1. Negative features are X4. Lowest impact features are X12, X9, and X26.',\n",
       " 'Predicted class is \"Less than 500\", value of 75.00%. Other classes and values are \"More than 500\" 25.00%. Top features are Snowfall (cm), Functioning Day, and Hour. Postive features are Hour. Negative features are Snowfall (cm) and Functioning Day. Lowest impact features are Visibility (10m), Holiday, and Wind speed (m\\\\/s).',\n",
       " 'Predicted class is \"Unacceptable\", value of 100.00%. Other classes and values are \"Acceptable\" 0.00%. Top features are safety, persons, and buying. Postive features are safety. Negative features are persons and buying. Lowest impact features are maint, doors, and lug_boot.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleurt': -0.5074241114185846, 'bleu': 0.10726705757444158, 'meteor': 0.41644034699755955}\n"
     ]
    }
   ],
   "source": [
    "print({'bleurt': np.mean(bleurt_results['scores']),\n",
    "         'bleu': bleu_results['bleu'],\n",
    "            'meteor': meteor_results['meteor']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F40, F8, F5, and F22. Postive features are F5. Negative features are F40, F8, and F22. Lowest impact features are F31, F24, F34, F25, and F41.',\n",
       " 'Predicted class is C1, value of 91.95%. Other classes and values are C2 8.05%. Top features are F6, F5, F11, and F26. Postive features are F6 and F11. Negative features are F5 and F26. Lowest impact features are F8, F9, F22, F17, and F12.',\n",
       " 'Predicted class is C1, value of 89.16%. Other classes and values are C2 10.84%. Top features are F1, F5, F2, and F3. Postive features are F5 and F2. Negative features are F1 and F3. Lowest impact features are F6, F10, F4, F7, and F9.',\n",
       " 'Predicted class is C1, value of 97.02%. Other classes and values are C2 2.98%. Top features are F10, F11, F13, and F6. Postive features are F10, F11, F13, and F6. Negative features are  . Lowest impact features are F3, F17, F4, F12, and F14.',\n",
       " 'Predicted class is C2, value of 51.62%. Other classes and values are C1 48.38%. Top features are F4, F5, F6, and F9. Postive features are F5 and F6. Negative features are F4 and F9. Lowest impact features are F3, F1, F7, F2, and F8.',\n",
       " 'Predicted class is C3, value of 82.81%. Other classes and values are C2 0.08%& C1 1.60%& C4 15.52%. Top features are F1, F4, F3, and F5. Postive features are F1, F4, F3, and F5. Negative features are  . Lowest impact features are F4, F3, F5, F2, and F6.',\n",
       " 'Predicted class is C2, value of 97.82%. Other classes and values are C1 2.18%. Top features are F3, F4, F7, and F12. Postive features are F3, F4, F7, and F12. Negative features are  . Lowest impact features are F9, F5, F8, F14, and F1.',\n",
       " 'Predicted class is C3, value of 68.12%. Other classes and values are C2 2.01%& C1 29.87%. Top features are F10, F8, F9, and F2. Postive features are F10 and F8. Negative features are F9 and F2. Lowest impact features are F5, F12, F1, F4, and F6.',\n",
       " 'Predicted class is C1, value of 80.00%. Other classes and values are C2 20.00%. Top features are F27, F15, F2, and F21. Postive features are F15 and F21. Negative features are F27 and F2. Lowest impact features are F35, F22, F16, F26, and F42.',\n",
       " 'Predicted class is C1, value of 83.00%. Other classes and values are C3 3.00%& C2 14.00%. Top features are F2, F11, F10, and F1. Postive features are F2. Negative features are F11, F10, and F1. Lowest impact features are F5, F12, F8, F4, and F6.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%& C3 0.00%. Top features are F5, F6, F9, and F2. Postive features are F5, F6, F9, and F2. Negative features are  . Lowest impact features are F7, F4, F3, F12, and F1.',\n",
       " 'Predicted class is C1, value of 60.00%. Other classes and values are C2 40.00%. Top features are F11, F3, F6, and F1. Postive features are F11, F6, and F1. Negative features are F3. Lowest impact features are F4, F2, F10, F9, and F5.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F27, F1, F29, and F12. Postive features are F27, F1, F29, and F12. Negative features are  . Lowest impact features are F26, F13, F18, F17, and F24.',\n",
       " 'Predicted class is C3, value of 99.45%. Other classes and values are C1 0.47%& C2 0.04%& C4 0.05%. Top features are F2, F16, F11, and F12. Postive features are F2 and F16. Negative features are F11 and F12. Lowest impact features are F5, F4, F13, F6, and F1.',\n",
       " 'Predicted class is C1, value of 97.94%. Other classes and values are C2 2.06%. Top features are F38, F19, F46, and F40. Postive features are F38, F19, and F40. Negative features are F46. Lowest impact features are F15, F12, F2, F24, and F11.',\n",
       " 'Predicted class is C2, value of 88.00%. Other classes and values are C3 2.00%& C1 10.00%. Top features are F11, F9, F3, and F5. Postive features are F11. Negative features are F9, F3, and F5. Lowest impact features are F12, F8, F4, F10, and F2.',\n",
       " 'Predicted class is C2, value of 93.32%. Other classes and values are C1 6.68%. Top features are F17, F26, F25, and F2. Postive features are F17, F26, and F2. Negative features are F25. Lowest impact features are F22, F16, F29, F8, and F33.',\n",
       " 'Predicted class is C3, value of 75.63%. Other classes and values are C2 5.86%& C1 18.51%. Top features are F1, F7, F5, and F10. Postive features are F1 and F10. Negative features are F7 and F5. Lowest impact features are F2, F3, F11, F8, and F4.',\n",
       " 'Predicted class is C2, value of 94.37%. Other classes and values are C1 5.63%. Top features are F4, F6, F7, and F11. Postive features are F4, F6, F7, and F11. Negative features are  . Lowest impact features are F8, F9, F1, F3, and F5.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F7, F10, F9, and F1. Postive features are F7, F10, and F1. Negative features are F9. Lowest impact features are F6, F2, F5, F3, and F13.',\n",
       " 'Predicted class is C2, value of 58.75%. Other classes and values are C1 41.25%. Top features are F14, F8, F1, and F12. Postive features are F14, F8, and F1. Negative features are F12. Lowest impact features are F10, F15, F16, F5, and F7.',\n",
       " 'Predicted class is C1, value of 90.65%. Other classes and values are C2 9.35%. Top features are F1, F2, F3, and F4. Postive features are F1, F2, F3, and F4. Negative features are  . Lowest impact features are F4, F6, F7, F5, and F8.',\n",
       " 'Predicted class is C1, value of 80.35%. Other classes and values are C2 19.65%. Top features are F15, F2, F6, and F11. Postive features are F15 and F6. Negative features are F2 and F11. Lowest impact features are F7, F12, F14, F17, and F18.',\n",
       " 'Predicted class is C2, value of 99.30%. Other classes and values are C1 0.70%. Top features are F9, F5, F10, and F4. Postive features are F9, F10, and F4. Negative features are F5. Lowest impact features are F2, F11, F14, F17, and F12.',\n",
       " 'Predicted class is C2, value of 64.11%. Other classes and values are C1 35.89%. Top features are F11, F5, F6, and F12. Postive features are F11, F5, and F12. Negative features are F6. Lowest impact features are F4, F3, F9, F7, and F2.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F1, F10, F5, and F2. Postive features are F1. Negative features are F10, F5, and F2. Lowest impact features are F3, F7, F8, F9, and F11.',\n",
       " 'Predicted class is C2, value of 99.00%. Other classes and values are C1 1.00%. Top features are F29, F9, F88, and F53. Postive features are F29, F9, and F88. Negative features are F53. Lowest impact features are F41, F40, F72, F79, and F75.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F7, F8, F3, and F2. Postive features are F7 and F2. Negative features are F8 and F3. Lowest impact features are F9, F4, F5, F6, and F1.',\n",
       " 'Predicted class is C2, value of 96.77%. Other classes and values are C1 3.23%. Top features are F19, F11, F6, and F16. Postive features are F19 and F16. Negative features are F11 and F6. Lowest impact features are F30, F7, F23, F29, and F17.',\n",
       " 'Predicted class is C2, value of 57.83%. Other classes and values are C1 42.17%. Top features are F22, F24, F2, and F30. Postive features are F22, F24, and F2. Negative features are F30. Lowest impact features are F15, F23, F25, F12, and F16.',\n",
       " 'Predicted class is C1, value of 96.35%. Other classes and values are C2 3.65%. Top features are F2, F18, F19, and F11. Postive features are F2, F18, and F19. Negative features are F11. Lowest impact features are F5, F10, F1, F3, and F15.',\n",
       " 'Predicted class is C2, value of 93.00%. Other classes and values are C1 7.00%. Top features are F11, F2, F9, and F6. Postive features are F11 and F9. Negative features are F2 and F6. Lowest impact features are F4, F5, F8, F7, and F10.',\n",
       " 'Predicted class is C2, value of 72.93%. Other classes and values are C1 27.07%. Top features are F4, F3, F9, and F8. Postive features are F4. Negative features are F3, F9, and F8. Lowest impact features are F5, F1, F2, F7, and F6.',\n",
       " 'Predicted class is C1, value of 51.60%. Other classes and values are C2 48.40%. Top features are F21, F8, F26, and F11. Postive features are F8 and F11. Negative features are F21 and F26. Lowest impact features are F23, F5, F1, F16, and F7.',\n",
       " 'Predicted class is C2, value of 98.21%. Other classes and values are C1 1.79%. Top features are F2, F5, F8, and F7. Postive features are F2, F5, F8, and F7. Negative features are  . Lowest impact features are F11, F1, F12, F3, and F6.',\n",
       " 'Predicted class is C2, value of 91.30%. Other classes and values are C1 9.70%. Top features are F1, F3, F4, and F7. Postive features are F1, F4, and F7. Negative features are F3. Lowest impact features are F4, F7, F2, F6, and F5.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F2, F1, F7, and F11. Postive features are F2, F1, and F7. Negative features are F11. Lowest impact features are F6, F8, F9, F14, and F13.',\n",
       " 'Predicted class is C1, value of 82.00%. Other classes and values are C2 18.00%. Top features are F44, F8, F3, and F27. Postive features are F8 and F27. Negative features are F44 and F3. Lowest impact features are F31, F15, F24, F29, and F25.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F22, F19, F13, and F10. Postive features are F22, F19, and F13. Negative features are F10. Lowest impact features are F12, F15, F14, F5, and F8.',\n",
       " 'Predicted class is C2, value of 65.07%. Other classes and values are C1 34.93%. Top features are F5, F3, F2, and F4. Postive features are F5 and F3. Negative features are F2 and F4. Lowest impact features are F6, F8, F7, F1, and F10.',\n",
       " 'Predicted class is C2, value of 99.72%. Other classes and values are C1 0.28%. Top features are F9, F8, F4, and F5. Postive features are F9, F8, F4, and F5. Negative features are  . Lowest impact features are F3, F7, F1, F6, and F2.',\n",
       " 'Predicted class is C1, value of 65.51%. Other classes and values are C2 34.49%. Top features are F9, F1, F5, and F8. Postive features are F9 and F1. Negative features are F5 and F8. Lowest impact features are F7, F4, F6, F3, and F2.',\n",
       " 'Predicted class is C2, value of 71.87%. Other classes and values are C1 28.13%. Top features are F5, F1, F11, and F8. Postive features are F5, F1, F11, and F8. Negative features are  . Lowest impact features are F4, F12, F9, F2, and F6.',\n",
       " 'Predicted class is C2, value of 61.74%. Other classes and values are C1 38.26%. Top features are F1, F31, F14, and F10. Postive features are F1. Negative features are F31, F14, and F10. Lowest impact features are F24, F33, F27, F13, and F25.',\n",
       " 'Predicted class is C3, value of 100.00%. Other classes and values are C2 0.00%& C1 0.00%& C4 0.00%. Top features are F15, F4, F8, and F1. Postive features are F15, F8, and F1. Negative features are F4. Lowest impact features are F9, F11, F6, F14, and F19.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F28, F13, F1, and F20. Postive features are F28, F13, F1, and F20. Negative features are  . Lowest impact features are F19, F2, F17, F4, and F26.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F5, F1, F6, and F3. Postive features are F5, F1, F6, and F3. Negative features are  . Lowest impact features are F1, F6, F3, F2, and F4.',\n",
       " 'Predicted class is C1, value of 99.28%. Other classes and values are C2 0.72%. Top features are F17, F13, F4, and F12. Postive features are F17, F13, and F12. Negative features are F4. Lowest impact features are F18, F2, F16, F10, and F5.',\n",
       " 'Predicted class is C1, value of 95.09%. Other classes and values are C2 1.06%& C3 3.85%& C4 0.00%. Top features are F15, F3, F7, and F1. Postive features are F15 and F1. Negative features are F3 and F7. Lowest impact features are F5, F6, F16, F13, and F9.',\n",
       " 'Predicted class is C2, value of 63.62%. Other classes and values are C1 36.38%. Top features are F9, F6, F12, and F16. Postive features are F9, F12, and F16. Negative features are F6. Lowest impact features are F14, F2, F10, F5, and F1.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F2, F1, F3, and F4. Postive features are F2, F1, F3, and F4. Negative features are  . Lowest impact features are F1, F3, F4, F5, and F6.',\n",
       " 'Predicted class is C2, value of 64.62%. Other classes and values are C1 35.38%. Top features are F6, F3, F8, and F2. Postive features are F3. Negative features are F6, F8, and F2. Lowest impact features are F9, F5, F4, F1, and F7.',\n",
       " 'Predicted class is C2, value of 73.58%. Other classes and values are C3 4.16%& C1 22.27%. Top features are F12, F7, F5, and F8. Postive features are F12 and F8. Negative features are F7 and F5. Lowest impact features are F4, F2, F10, F3, and F6.',\n",
       " 'Predicted class is C4, value of 35.74%. Other classes and values are C1 30.83%& C2 0.00%& C3 33.42%. Top features are F1, F4, F6, and F5. Postive features are F1, F4, F6, and F5. Negative features are  . Lowest impact features are F4, F6, F5, F3, and F2.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F8, F3, F1, and F9. Postive features are F8 and F9. Negative features are F3 and F1. Lowest impact features are F7, F4, F5, F2, and F6.',\n",
       " 'Predicted class is C2, value of 97.71%. Other classes and values are C1 2.29%. Top features are F11, F4, F9, and F2. Postive features are F11, F4, and F2. Negative features are F9. Lowest impact features are F7, F3, F8, F14, and F5.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F81, F35, F89, and F42. Postive features are F89. Negative features are F81, F35, and F42. Lowest impact features are F7, F18, F3, F52, and F27.',\n",
       " 'Predicted class is C1, value of 97.03%. Other classes and values are C2 2.97%. Top features are F9, F14, F10, and F11. Postive features are F9, F14, F10, and F11. Negative features are  . Lowest impact features are F8, F5, F13, F3, and F4.',\n",
       " 'Predicted class is C1, value of 80.35%. Other classes and values are C2 19.65%. Top features are F18, F5, F7, and F2. Postive features are F18 and F7. Negative features are F5 and F2. Lowest impact features are F6, F1, F3, F12, and F19.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F10, F2, F5, and F9. Postive features are F10, F2, F5, and F9. Negative features are  . Lowest impact features are F4, F7, F8, F1, and F6.',\n",
       " 'Predicted class is C1, value of 70.00%. Other classes and values are C2 30.00%. Top features are F5, F9, F12, and F8. Postive features are F5 and F12. Negative features are F9 and F8. Lowest impact features are F2, F4, F13, F10, and F6.',\n",
       " 'Predicted class is C2, value of 87.50%. Other classes and values are C1 12.50%. Top features are F4, F5, F1, and F7. Postive features are F4. Negative features are F5, F1, and F7. Lowest impact features are F6, F9, F2, F8, and F3.',\n",
       " 'Predicted class is C2, value of 98.80%. Other classes and values are C1 1.20%. Top features are F22, F41, F16, and F17. Postive features are F22 and F41. Negative features are F16 and F17. Lowest impact features are F20, F15, F30, F34, and F24.',\n",
       " 'Predicted class is C1, value of 75.00%. Other classes and values are C2 25.00%. Top features are F12, F9, F1, and F7. Postive features are F1. Negative features are F12, F9, and F7. Lowest impact features are F2, F4, F8, F3, and F6.',\n",
       " 'Predicted class is C2, value of 84.87%. Other classes and values are C1 15.13%. Top features are F5, F7, F8, and F2. Postive features are F5, F7, and F2. Negative features are F8. Lowest impact features are F12, F6, F4, F11, and F1.',\n",
       " 'Predicted class is C1, value of 83.08%. Other classes and values are C3 16.87%& C4 0.00%& C2 0.05%. Top features are F3, F6, F4, and F5. Postive features are F6 and F4. Negative features are F3 and F5. Lowest impact features are F6, F4, F5, F2, and F1.',\n",
       " 'Predicted class is C2, value of 83.98%. Other classes and values are C1 16.02%. Top features are F14, F15, F12, and F18. Postive features are F14, F15, F12, and F18. Negative features are  . Lowest impact features are F10, F1, F7, F17, and F5.',\n",
       " 'Predicted class is C1, value of 56.51%. Other classes and values are C2 43.49%. Top features are F8, F1, F6, and F3. Postive features are F8, F1, and F6. Negative features are F3. Lowest impact features are F14, F10, F12, F7, and F11.',\n",
       " 'Predicted class is C1, value of 51.69%. Other classes and values are C2 48.31%. Top features are F8, F9, F1, and F7. Postive features are F9, F1, and F7. Negative features are F8. Lowest impact features are F6, F3, F2, F5, and F4.',\n",
       " 'Predicted class is C2, value of 85.13%. Other classes and values are C1 14.87%. Top features are F15, F1, F2, and F16. Postive features are F1 and F16. Negative features are F15 and F2. Lowest impact features are F8, F10, F7, F13, and F5.',\n",
       " 'Predicted class is C2, value of 95.09%. Other classes and values are C1 4.91%. Top features are F4, F5, F10, and F2. Postive features are F5 and F2. Negative features are F4 and F10. Lowest impact features are F8, F7, F11, F6, and F9.',\n",
       " 'Predicted class is C2, value of 100.00%. Other classes and values are C1 0.00%. Top features are F2, F7, F4, and F6. Postive features are F2, F7, F4, and F6. Negative features are  . Lowest impact features are F14, F1, F3, F11, and F13.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F8, F6, F4, and F10. Postive features are F8, F6, F4, and F10. Negative features are  . Lowest impact features are F2, F14, F11, F3, and F5.',\n",
       " 'Predicted class is C1, value of 85.00%. Other classes and values are C2 15.00%. Top features are F2, F10, F13, and F26. Postive features are F2, F13, and F26. Negative features are F10. Lowest impact features are F16, F18, F36, F1, and F41.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F4, F3, F5, and F6. Postive features are F4, F3, F5, and F6. Negative features are  . Lowest impact features are F5, F6, F2, F7, and F1.',\n",
       " 'Predicted class is C1, value of 99.84%. Other classes and values are C2 0.16%. Top features are F10, F23, F33, and F29. Postive features are F10 and F23. Negative features are F33 and F29. Lowest impact features are F8, F11, F1, F22, and F31.',\n",
       " 'Predicted class is C2, value of 95.09%. Other classes and values are C4 1.06%& C3 3.85%& C1 0.00%. Top features are F4, F1, F9, and F7. Postive features are F4 and F7. Negative features are F1 and F9. Lowest impact features are F12, F16, F17, F8, and F5.',\n",
       " 'Predicted class is C1, value of 99.72%. Other classes and values are C2 0.28%. Top features are F12, F17, F7, and F20. Postive features are F12, F7, and F20. Negative features are F17. Lowest impact features are F3, F9, F21, F6, and F11.',\n",
       " 'Predicted class is C1, value of 65.51%. Other classes and values are C2 34.49%. Top features are F5, F1, F4, and F9. Postive features are F5 and F1. Negative features are F4 and F9. Lowest impact features are F2, F8, F6, F7, and F3.',\n",
       " 'Predicted class is C1, value of 56.00%. Other classes and values are C2 44.00%. Top features are F22, F26, F18, and F5. Postive features are F18 and F5. Negative features are F22 and F26. Lowest impact features are F3, F17, F30, F19, and F4.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F3, F4, F2, and F1. Postive features are F3, F4, F2, and F1. Negative features are  . Lowest impact features are F4, F2, F1, F5, and F6.',\n",
       " 'Predicted class is C1, value of 67.08%. Other classes and values are C2 32.92%. Top features are F8, F6, F5, and F7. Postive features are F6, F5, and F7. Negative features are F8. Lowest impact features are F9, F3, F11, F1, and F10.',\n",
       " 'Predicted class is C1, value of 66.11%. Other classes and values are C3 31.78%& C2 2.11%. Top features are F4, F6, F2, and F8. Postive features are F4 and F2. Negative features are F6 and F8. Lowest impact features are F10, F3, F1, F12, and F5.',\n",
       " 'Predicted class is C1, value of 98.17%. Other classes and values are C2 1.83%. Top features are F64, F72, F66, and F73. Postive features are  . Negative features are F64, F72, F66, and F73. Lowest impact features are F56, F78, F65, F79, and F89.',\n",
       " 'Predicted class is C1, value of 51.62%. Other classes and values are C2 48.38%. Top features are F8, F9, F3, and F1. Postive features are F9 and F3. Negative features are F8 and F1. Lowest impact features are F10, F5, F7, F2, and F6.',\n",
       " 'Predicted class is C2, value of 35.74%. Other classes and values are C4 30.83%& C1 0.00%& C3 33.42%. Top features are F1, F2, F4, and F5. Postive features are F1, F2, F4, and F5. Negative features are  . Lowest impact features are F2, F4, F5, F3, and F6.',\n",
       " 'Predicted class is C1, value of 62.34%. Other classes and values are C2 37.66%. Top features are F1, F2, F3, and F7. Postive features are F1 and F2. Negative features are F3 and F7. Lowest impact features are F10, F5, F6, F4, and F11.',\n",
       " 'Predicted class is C1, value of 75.63%. Other classes and values are C3 5.86%& C2 18.51%. Top features are F11, F6, F8, and F7. Postive features are F11 and F7. Negative features are F6 and F8. Lowest impact features are F5, F10, F12, F3, and F1.',\n",
       " 'Predicted class is C3, value of 0.0%. Other classes and values are C1 77.0%& C2 23.0%. Top features are F9, F6, F2, and F4. Postive features are F9, F6, F2, and F4. Negative features are  . Lowest impact features are F8, F7, F1, F5, and F3.',\n",
       " 'Predicted class is C2, value of 90.58%. Other classes and values are C1 9.42%. Top features are F5, F8, F10, and F7. Postive features are F8. Negative features are F5, F10, and F7. Lowest impact features are F1, F9, F12, F4, and F6.',\n",
       " 'Predicted class is C2, value of 68.40%. Other classes and values are C1 31.60%. Top features are F8, F6, F2, and F3. Postive features are F8 and F6. Negative features are F2 and F3. Lowest impact features are F4, F7, F9, F5, and F1.',\n",
       " 'Predicted class is C1, value of 83.33%. Other classes and values are C2 16.67%. Top features are F7, F19, F24, and F1. Postive features are F7, F19, and F1. Negative features are F24. Lowest impact features are F17, F12, F20, F23, and F15.',\n",
       " 'Predicted class is C2, value of 75.00%. Other classes and values are C1 25.00%. Top features are F2, F12, F9, and F3. Postive features are F9. Negative features are F2, F12, and F3. Lowest impact features are F5, F1, F7, F4, and F10.',\n",
       " 'Predicted class is C1, value of 100.00%. Other classes and values are C2 0.00%. Top features are F4, F6, F3, and F5. Postive features are F4. Negative features are F6, F3, and F5. Lowest impact features are F6, F3, F5, F2, and F1.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 169.11ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model_name', 'predicted_class', 'task_name', 'narration', 'values', 'sign', 'narrative_id', 'unique_id', 'classes_dict', 'narrative_questions', 'feature_nums', 'ft_num2name', 'old2new_ft_nums', 'old2new_classes', 'predicted_class_label', 'class2name', 'input'],\n",
       "    num_rows: 1\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda x: x['unique_id'] == 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-702010-cac443d3271dff16/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a4ca6c3359638a88.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The case is labelled as \"high quality\" by the classifier, with the likelihood of this being correct equal to 94.37%, suggesting that there is a slight chance of about 5.63% that this decision could be wrong. The above prediction by the classifier is mainly based on the values of the features volatile acidity, sulphates, total sulfur dioxide, and alcohol, which, according to the analysis performed, offer very strong positive support for the prediction. The other variables with a positive influence on the decision are citric acid, fixed acidity, and density, further cementing the belief in the decision made here. The 5.63% likelihood of the \"low_quality\" can be blamed on the negative influence of chlorides, residual sugar, free sulfur dioxide, and pH, decreasing the likelihood of the \"high quality\" label assigned to the case under consideration. In summary, the confidence level of 94.37% in the \"high quality\" assignment is mainly due to the strong positive influence of sulphates, volatile acidity, and alcohol.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils import nums_to_names\n",
    "\n",
    "x = dataset.filter(lambda x: x['unique_id'] == 429)[0]\n",
    "nums_to_names(x['narration'], eval(x['class2name']), eval(x['ft_num2name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration james-burton--aug-text-exps-a7fb5fbf61784010\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--aug-text-exps-a7fb5fbf61784010/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 791.58it/s]\n"
     ]
    }
   ],
   "source": [
    "aug = load_dataset(\"james-burton/aug-text-exps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'C1': 'low_quality', 'C2': 'high quality'}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['class2name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicted class is C2, value of 94.37%. Other classes and values are C1 5.63%. Top features are F4, F6, F7, and F11. Postive features are F4, F6, F7, and F11. Negative features are  . Lowest impact features are F8, F9, F1, F3, and F5.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-29 11:45:48.466388: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-29 11:45:49.030709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-29 11:45:49.030758: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-29 11:45:49.030763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Downloading: 100%|██████████| 382/382 [00:00<00:00, 365kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 676kB/s] \n",
      "Downloading: 100%|██████████| 17.7M/17.7M [00:00<00:00, 20.1MB/s]\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1/1 [00:00<00:00, 32.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 48.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.1784)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%    - SVM 96\n",
    "%    - LogReg 90\n",
    "%    - RF 82\n",
    "%    - KNN 59\n",
    "%    - Gradient Boosting 38\n",
    "%    - Decision Tree 25\n",
    "%    - MLP 15\n",
    "%    _ DNN 13\n",
    "%    - Adaboost 5\n",
    "%    - GaussianNB 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([96, 90, 82, 59, 38, 25, 15, 13, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.33962725482034"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([96, 90, 82, 59, 38, 25, 15, 13, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['model_name', 'predicted_class', 'task_name', 'narration', 'values', 'sign', 'narrative_id', 'unique_id', 'classes_dict', 'narrative_questions', 'feature_nums', 'ft_num2name', 'old2new_ft_nums', 'old2new_classes', 'predicted_class_label', 'class2name', 'input'],\n",
       "    num_rows: 375\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration james-burton--textual-explanations-702010-cac443d3271dff16\n",
      "Found cached dataset parquet (/home/james/.cache/huggingface/datasets/james-burton___parquet/james-burton--textual-explanations-702010-cac443d3271dff16/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 267.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"james-burton/textual-explanations-702010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The prediction probability associated with class C2 and class C1, respectively, is 35.34% and 64.66%. Based on these probabilities, the model labels the given case as C1 since it is the most probable class. According to the attribution analysis, the most relevant features considered by the model here are F5, F1, and F8, while the least relevant features are F12, F2, and F4. Regarding the direction of influence of the features, F5, F1, F8, and F7 are the top positively supporting features, driving the decision higher in favour of C1. Further increasing the probability that C1 is the true label are the values of other positive features such as F16, F3, F15, and F14. To explain why the likelihood of C2 is 35.34%, we have to look at the negative contributions from F11, F6, F13, F2, F12, and F4. The abovementioned negative features contradict the model's decision with respect to the classification outcome.\",\n",
       " \"The classifier is very uncertain about the correct label for the case given.  Regarding the classifier's decision, there is close to an even split on the probability of either of the possible labels is the correct label but the classifier chooses the label as C2. The prediction verdict above is attributed to the contributions of mainly the following features: F4, F11, F3, and F15, however, the lowest ranked features are F7, F19, and F1. Analysing the direction of influence of the features shows that there are ten positive and ten negative features.  Positive features such as F3, F15, F8, and F20 increase the response of the classifier in favour of the assigned label. Conversely, negative features such as F4, F11, F12, and F2 decrease the likelihood of C2 being the correct label given that their values support the alternative label, C1. The uncertainty concerning the label assignment can be due to the fact that the top negative features F4 and F11 have very high attributions shifting the classifier's verdict away from the C2 class.\",\n",
       " 'The classification algorithm believes that C2 is the output label that was generated with 100% certainty and that C1 is unlikely to be the correct label in this case. According to the attribution investigations, the following input features are ranked from most relevant to least relevant: F2, F4, F6, F1, F3, and F5. As shown by the attribution plot, F2 is the only one shown to positively contribute to the above classification decision, while the others contribute negatively. The contributions of negative features such as F4, F6, and F1 result in the decision being driven in a different direction. From the prediction confidence level, we can conclude that the very strong influence of F2 overshadows the contributions of the negative features hence the very high confidence level.',\n",
       " \"Of the three possible labels, there is 100.0% confidence that C3 is the most probable label for the given case. The features that heavily influence the classification verdict presented here are F5, F10, and F6, and they have a very strong positive contribution, increasing the odds of the C3 prediction. Other features with a positive influence on the model are F3, F1, F12, F7, and F4. On the contrary, F9, F2, and F8 make the model's decision fluctuate negatively towards selecting an alternative label. All of the negative features mentioned above have a low to moderate impact on the classification verdict presented here compared to F6, F10, and F5. Finally, F11 with its very low positive impact is the least ranked feature marginally pushing the decision towards the assigned label.\",\n",
       " \"Per the model, class C2 has a prediction probability of 10.50 percent, whereas class C1 has a predicted probability of 89.50 percent. As a result of the model, it can be determined that C1 is the most likely label for the given scenario. All of the input features are shown to contribute to the above conclusion, with F1, F7, and F3 having the most influence on the classification decision. The least influential features with regard to this classification are F5, F2, F10, and F4, whereas, the impact of F8, F6, and F9 can be classified as modest. The large positive contributions of F7 and F1 are responsible for the model's high confidence which further supported by the positive contributions of F8, F5, and F2. In conclusion, the negative features F3, F6, F10, F9, and F4 favour labelling the case as C2 hence the associated predicted probability.\",\n",
       " \"The classifier says that C2 is the most likely label for the provided data with relatively high confidence. It is crucial to remember, however, that there is a 21.80% possibility that it is C1. F16 and F19 are the major driving variables for the aforementioned classification or prediction choice. The remaining variables  F1, F12, F17, and F6 have a modest to minor impact on the selection made above. Among the input variables, F1, F6, F4, F9, and F10 are the subset that have a negative influence or contribution whereas all of the remaining variables have a positive impact. In essence, the substantial positive contributions of F16 and F19, together with the contributions of additional positive variables such as F12, F17, F8, and F7, account for the classifier's confidence in this classification.\",\n",
       " 'C1 has an 83.0% chance of being the correct label for the case under consideration, making C2 the least likely class with a predicted likelihood of 17.0%. F24, F14, and F16 features have a significant impact on class selection here while on the other hand, the remaining features are shown to have marginal to no contribution to the classification verdict here. In actual fact, the values for F3, F9, F7, F4, F12, and F23 may have been ignored by the classifier because their respective influences are almost zero. Of the important features, only F26, F11, F22, F20, F6, and F19 are negative and this is mainly because their contribution to selection tends to reduce the chance that C1 is the correct label, preferring that the case is classified as C2. The remaining features such as F24, F14, F16, F21, F1, and F13 strongly contribute positively, increasing the chances of C1 which explains the level of certainty associated with C1.',\n",
       " 'C2 is the label picked by the algorithm with about 82.06% certainty, since the prediction likelihood of C1 is only 17.94%. F20, F5, F10, and F3 all contribute significantly to the above classification output and among them, the features that support the most positive contribution to the C2 prediction are F3, F20, and F5, while F10 drives the final prediction against assigning C2 in support of C1. F1 also contributes positively to the classification here, but F18 contributes negatively and like F10 favours C1. Finally, according to the analysis, F11, F13, F7, and F8 all have little effect on the final prediction made by the algorithm for this case.',\n",
       " 'The model is confident in its prediction, as it predicted class C1 with a likelihood of 90.48% and hence, for the given case, there is a smaller chance of it being any other class label. F6 and F10 are deemed the most important features whereas on the other hand all the other features have moderate to minimal amounts of influence. Both F6 and F10 have the same direction of impact, increasing the odds of the predicted label, C1. While F4 and F1 are both encouraging the model to make a prediction of C1, the others F9, F12, and F8 is pushing the model towards a different label. Many features have moderately low impact on the final prediction, but the features F3, F8, and F2 are those with the smallest influence.',\n",
       " \"Based on the probability distribution across the classes, the classifier is shown to have a moderately high confidence level in the C2 label assignment, with its likelihood equal to 65.0%, whereas that of C1 is only 35.0%. The prediction decision above is predominantly due to the influence of the variables F22, F12, F23, and F9. On the lower end are the least relevant variables, F11, F18, F16, F19, F2, and F4, with little to no influence on the classifier when assigning a label to the given instance. On the one hand, the top positive variables are F22, F12, and F23, increasing the probability that C2 is the correct label. Also, the top negative variables are F9, F26, F21, and F3, decreasing the classifier's response and consequently shifting the prediction verdict in the opposite direction towards C1. Other variables with a positive direction of influence are F5, F1, F8, F10, F17, F20, F24, F6, and F7.\",\n",
       " \"The model's output labelling judgement for the case under consideration is as follows: C2 cannot be the label for the given case; C1 is the most likely class label with a 100.0% confidence level. The key driving factors resulting in the aforementioned classification are the values of the input features: F38, F51, F13, F46, F28, F71, and F44. F70, F61, F85, F20, F59, F93, F14, F66, F24, F89, F30, F65, and F54 are the features that have a modest effect on the decision. Aside from the aforementioned input features, all others, such as F47, F10, F7, and F43, are revealed to be irrelevant to the conclusion reached here. Not all of the influential features support labelling the current instance as C1, and they are referred to as negative features. F44, F61, F30, F65, and F54 are the negative attributes that diminish the likelihood that C1 is the correct label in this case. F38, F51, F13, and F46 are important positive features that strongly increase the likelihood that C1 is the correct label.\",\n",
       " \"For the case under consideration, the model assigned C1 with very high confidence, since the likelihood of C2 being the right label is only 0.52% which is very small. F11, F7, F3, and F10 have a large positive impact on the model's output prediction. F3 and F10 have a moderately positive impact on the prediction of C1, while F2 has a similar impact but in the opposite direction. F5, F12, and F4 have a very low impact on classification. F6, F8, F1, and F9 have a larger but still insignificant effect. Examining the attributions indicates that there are only two features, F2 and F12, with values that contradict the prediction made here but, their impact on the model is smaller when compared to positive features such as F7, F3, and F11, which explains why the confidence level associated with this classification is high.\",\n",
       " 'Since the likelihood of C3 being the true label is shown by the prediction algorithm outputs to be equal to 93.02 percent, there is only a small chance that the true label for the given data instance is any of the other class labels, C2 and C1. The features F12, F1, F11, and F10 are the most important ones driving the label assignment verdict above, and on the other hand, the least relevant features are shown to be F9, F6, and F3. Considering the direction of influence of each input feature, as shown by the attribution analysis, it can be concluded that the positive features steering the prediction higher towards C3 are F12, F1, F10, F11, F2, F4, and F6. The marginal doubt in the predicted output decision is attributed to the negative contributions of F8, F7, F3, F9, and F5. Considering the attributions of the features and predicted probabilities across the classes, it can be concluded that the joint positive contribution outranks the negative contributions; hence, the algorithm is confident that C3 is likely the true label.',\n",
       " 'Between the three possible classes, there is an 88.0% probability that the correct label for this case is C1. This means that there is a 12.0% chance that the label could be one of the other possible labels, C2 or C3. Increasing the odds of the predicted label are the variables F7, F5, F2, and F10. The next set of variables, F6, F1, and F9, have values that moderately decrease the likelihood of C1 being the correct label. F11, F4, and F3 are the other negatively contributing features, and given that they are lowly ranked, they have a marginal impact when determining the correct label for this case. The other positive features further increasing the probability that C1 is the right label are F12 and F8. Overall, we can conclude that the decision to label the case as C1 is largely due to the strong positive influence of F5, F7, F10, and F2.',\n",
       " 'Based on the input variables, the model is moderately confident that the C2 is the appropriate label for the data under consideration. As a matter of fact, the prediction likelihood associated with class C1 is about 30.42%. The preceeding classification verdict can be largely blamed on the contributions of variables F4, F11, F1, and F6, whereas those with marginally lower contributions are F2, F3, and F7. The variables with moderate contributions are F5, F8, F9, and F10. Considering their respective contributions, F4, F1, F6, and F10 are the variables with positive influence that increase the chances of C2 being the correct label for the given data. The little doubt in the label choice here could be attributed to the negative variables, mainly F11, F5, F9, and F8, which decrease the chances of the model labelling the data given as C2 since these negative variables favour selecting the alternative label, C1 over C2. Given that majority of top variables contribute positively, it is not unexpected that C2 is the picked label with reasonably high confidence.',\n",
       " 'The classifier is 69.02% certain that the given case is under the class label C1, implying that the likelihood of C2 is only 30.98%. Analysis performed to understand the contribution of each input feature revealed that: F7, F9, and F3 are the most influential features when assigning a label to the given case. Features F4, F6, F8, and F10 have moderate contributions, whereas the F1, F2 and F5 have lower relevance to the final classification decision. F7 and F3 push the class assignment towards C1, whereas F9 does the opposite, decreasing the likelihood of C1. Similar to F9, F4, and F6 negatively impact the C1 classification, whereas F10, F1, and F8 positively push the decision towards the C1 class. Features F2, and F5 all have little impact on the final decision, with F5 having the least impact.',\n",
       " 'The classifier trained on this prediction problem assigns a label to a given case based on the information supplied. The class assigned by the classifier to the case under consideration is C1. The probability that C2 is the correct label is around 25.28%; therefore, it is less likely to be the true label. The above classification decision is mainly based on the influence of the features F7, F1, F5, F3, F4, F6, and F2. Of the above stated features, F3 and F1 are the ones shown to have a negative impact, decreasing the odds of C1 being the accurate label for the given case and encouraging the classifier to select C2 instead. Finally, it can be concluded that there is a moderately high level of confidence in the assigned label, which can be attributed to the strong positive contribution of F7 combined with other positive features such as F5 and F4.',\n",
       " 'The case given is labelled as C1 by the classifier with a confidence level equal to 82.07%. Therefore, the probability of C2 being the correct label is only 17.93%. The classification above is mainly due to the contributions of features such as F30, F12, F11, and F32. F17, F24, and F28 are the next three with moderate influence. However, not all the features are considered by the classifier when determining the correct label for the given case. F46, F2, F40, and F8 are notable irrelevant features. With regards to the direction of influence of the relevant features, F30, F12, F11, and F32 are the top features with strong positive contributions favouring the assignment of label C1. The top negative features that shift the classification in a different direction are F17, F24, F33, and F29. Considering the fact that a number of the relevant features have positive attributions, it is not surprising that the classifier is quite certain that the appropriate label is C1 instead of C2.',\n",
       " 'According to the algorithm, there is little to no chance that the correct label for the given data instance is any of the following classes: C4, C1, and C3. It is very confident that the proper label is C2.  This label assignment is largely due to the parts played by the features F19, F12, and F17. On the lower end are the input features F1, F9, F16, and F10, which are shown to be less relevant when it comes to this labelling assignment task.  Finally, among the top features identified during the attribution investogation, only F15 and F4 are features with a negative influence, decreasing the odds of C2 being the appropriate label here.',\n",
       " \"The model predicts that the label for this case is C2 with a high degree of certainty of about 99.19% and the probability of the other label is only 0.81%. From the analysis, the variables with the strongest attributions to this classification decision are F8, F11, and F14. The attributions of these variables increased the response of the model in favour of labelling the case as C2. Other variables that positively supported the label decision include F15, F6, and F16. Not all the variables support the model's prediction of C2 and this is because the values of F2, F10, F3, F7, and F1 are driving the prediction towards C1. The joint attribution from these variables is weaker than that from F8, F11, and F14, so the model is biased toward predicting C2. Finally, F13, F9, F5, and F17 are the least important positive features, given that they have minimal attributions in favour of C2.\",\n",
       " \"According to the output prediction probabilities across the two classes, the output decision for the given data is C2 with a very high confidence level. C1 has a prediction probability of about 0.00%. The variables contributing most to the abovementioned classification are F10, F2, and F5, whereas F4 and F7 are the least influential variables. The very high confidence level associated with the classification decision here could be attributed to the fact that a greater number of the input variables have attributions that increase the model's response towards label C2. F3, F9, and F4 are the variables with negative contributions that attempt to push the model to label this case as C1. To  put it in a nutshell, the joint contribution of the negative variables is very low unlike that of the positive variables, hence the model's certainty in the decision here.\",\n",
       " 'With a labelling confidence level of 99.50%, the classifier predicts the label C2 in this situation. Hence, it is correct to conclude that the classifier is less certain that C1 is the proper label for the case here. The analysis indicates that five features contradict the decision above, while four features support the classifier. The features contradicting the prediction are usually referred to as negative features while those supporting it are referred to as positive features. The negative features decreasing the odds of C2 being the correct label are F6, F9, F4, F2, and F8. Conversely, the positive features increasing the odds of C2 are F7, F3, F1, and F5.',\n",
       " 'The class assigned by the model is C2 with a close to 97.67% confidence level, implying that the likelihood of C1 is only 2.33%. Based on the analysis, the most important features considered during the classification are F25, F4, F1, and F28 but among these features, F4 and F1 are the only ones with negative attributions, decreasing the likelihood of C2 being the label for the given case. Furthermore, moderately influencing the decision are F12, F27, F23, and F18. F12, F27, and F23 have positive attributions, while F18 has a negative impact, shifting the prediction in a different direction. Finally, the features with insignificant impact on the model when it comes to this case include F17, F10, F9, and F11.',\n",
       " 'With a higher degree of confidence, the model labels this given case as C1 since there is a zero chance that it is C2. The classification here can be attributed to all the features having positive contributions, decreasing the odds of C2 being the correct label. The features can be ranked based on their degree of influence from the most relevant to the least relevant as follows: F5, F7, F3, F2, F6, F4, F1. This implies that F5 is the most influential feature, while F1 is the least influential among the input features.',\n",
       " 'According to the model, C1 has a prediction probability of 99.45 percent, C2 has a prediction probability of 0.47 percent, C4 has a prediction probability of 0.04 percent, and C3 has a prediction probability of 0.05 percent, therefore, the most likely class is C1. F8 and F5 positively influence the above-mentioned label decision in favour of C1, but F7 has the opposite effect, favouring a different label. F9 and F18 both have a similar negative impact on the C1 prediction, whereas F16 has a positive impact. In this case, F4, F1, F13, and F20 have little influence on the labelling result. All in all, the model is confident in its assignment of the C1 class as shown by the predicted probabilities across the classes.',\n",
       " \"Per the model employed here, the prediction probability of C2 is only 17.93%, and that of C1 is equal to 82.07%. Given the information provided to the model, the most valid conclusion regarding the true label is that C1 is without a doubt the most likely one. The attributions analysis indicates that F11, F46, F8, F36, and F31 are the major drivers resulting in the prediction probabilities across the classes under consideration. At the tail end are features such as F17, F9, F13, and F19 that have very little influence on the decision made with respect to the given case. Among the influential features, only F11, F46, F36, F7, F29, F35, F6, F26, and F44 have positive contributions in support of labelling the given case as C1. On the other hand, the negative features such as F8, F31, F15, F10, F38, F39, and F16, suggest C2 could likely be the true label in this case. Overall, the marginal doubt in the correctness of assigning C1 to the case under consideration is attributed to the negative features driving the model's decision in the direction of C2 away from C1. But the higher influence of positive features such as F11 and F46 ensures that C1 is assigned as the most probable label.\",\n",
       " \"The model is about 90.0% certain or sure that the correct label based on the input features of the given case is C1. The features with the most significant influence on the decision are F8, F2, F4, and F9. The influence of the features can be categorised as positive or negative traits depending on the direction of the effect on the model. Positive features increase the likelihood of the most likely class (i.e., C1), whereas negative features reduce the model's responsiveness to the assigned label, favouring the less likely class (i.e., C2). From the attribution analysis, F5, F7, and F1 are the negative features here. Overall, the negative features are shown to have moderate to low influence compared to the positive features, hence explaining why the model is very confident about the assigned label C1.\",\n",
       " \"With an 81.01% chance of being correct, C1 is the most likely label, consequently, the C2 class's prediction probability is only 18.99%. The algorithm or classifier got the above prediction mostly due to the influence of features like F7, F5, F4, and F2. F6, which is found to have very little impact with regard to the label choice here, is the least relevant feature for the algorithm. F5, F1, F4, and F2 have a positive direction of influence, pushing the algorithm higher towards the C1 label. Negative features like F7, F9, and F8 favour choosing or labelling the case as C2.\",\n",
       " 'The model makes classification decisions based on the information provided to it and for the case here, the prediction probabilities across the two class labels, C2 and C1, are 49.32% and 50.68%, respectively. Based on these prediction probabilities, the label assigned is C1, since it has the highest likelihood, however, the model is not very certain about the correctness of the assigned label since its probability is marginally higher than the average. The uncertainty in the classification here can be blamed on the fact that only F1, F6, F4, F12, and F5 have positive attributions, shifting the decision higher towards C1. On the other hand, features F3, F2, F10, F7, F9, F11, and F8 have negative contributions that decrease the prediction likelihood of C1 while increasing that of C2. To cut a long story short, the most positive features are F1 and F6, whereas the most negative ones are F3 and F2. Finally, F9, F5, and F11 are not as important as all the previously mentioned features hence received little attention from the model.',\n",
       " \"The most likely label for the given case is C1 since the predicted probability of C2 is only 34.27% and this means that the likelihood of C1 is 65.73%. The most relevant features that led to the C1 classification verdict are F5, F30, F26, F17, and F15. However, some of the features are deemed irrelevant to the above verdict and these include F3, F4, F13, and F27. Among the relevant features with some degree of impact, seven are shown to drive the model's class assignment towards the C2, while the remaining support the C1 prediction. Notable negative features swinging the prediction towards C2  are F5, F30, and F26, while the notable positive features are F17 and F15. The small uncertainty associated with the prediction decision for the given case could be attributed to the fact that all the three most important features are negative features whose values contradict assigning the label C1.\",\n",
       " \"Per the classifier for the given data, the most plausible label is C2. F2, F19, F24, and F8 are the main features pushing for the above-mentioned outcome. F3, F18, F14, F16, F13, and F22, on the other hand, have little contribution to the classifier employed here. F25, F17, F7, and F9 have a moderate contribution to the assignment of C2. The classifier's confidence in the label decision above can be attributed to larger positive attributions of F17, F25, F24, and F19 compared to the negative attributions of F7, F21, F2, F4, F8, and F5.\",\n",
       " \"The least probable class, according to the classification algorithm, is C2, with a prediction probability of 25.12%, therefore, we can conclude that the algorithm is quite confident that the correct label for this data is C1. Analysing the attributions revealed that F1, F6, F8, and F2 are the most relevant features, whereas F5, F7, and F4 are the least relevant features. Increasing the algorithm's response in favour of C1 are the positive features F1, F8, F2, F7, F5, and F9. On the contrary, all the other features, F6, F3, F12, F11, F10, and F4, drive the algorithm towards labelling the given data as C2, hence they are considered negative features. Furthermore, the negative influence on the algorithm is the reason why the confidence level in the C1 is reduced to 74.88%.\",\n",
       " \"According to the classification algorithm, neither C1 nor C3 nor C2 is the correct label for the given case. It is 100.0% certain that C4 is the right label. The higher degree of certainty in the above prediction can be attributed to the positive contributions of F18, F11, and F12. The other positive features include F20, F15, F6, and F14, however, unlike F18, F11, and F12, these features have a moderately low impact on the algorithm's decision. The remaining positive features, F7, F19, F13, and F9, are among the least influential input features considered by the algorithm. There are other features such as F16, F10, F3, and F4 whose contributions only serve to decrease the odds of C4 being the correct label for the given case. Regarding the high confidence of the algorithm with respect to this classification, one can conclude that the negative features have little influence on the algorithm's label decision here.\",\n",
       " \"The prediction likelihoods across the two classes are 15.35% for class C1 and 84.65% for C2, it can be concluded that C2 is the most probable class label for the given data instance. According to the attribution analysis conducted, the different input variables have varying degrees of influence on the model's decision here. The most influential set of variables is F18, F31, F19, F36, F35, F20, and F38, while the variables with the least influence include F37, F3, F25, F16, F33, and F6. The following or subsequent analysis performed to understand the direction of contribution of of the features  will focus on the most influential ones controlling the label selection here. Among the top influential features, F18, F31, F19, F36, and F38, only F18 and F31 have negative contributions, decreasing the probability that C2 is the correct label, and they strongly support labelling the case as C1 instead. Pushing the classification decision in favour of C2 are the positive variables such as F19, F36, and F38. The contributions of the remaining variables, including F35, F20, and F5, have moderate to low influence. All in all, the marginal uncertainty in the decision here is mainly due to the negative influences of F18, F31, F9, and F23, but the positive contributions of F19, F36, F5, F35, F20, and F38 drive the decision higher towards C2.\",\n",
       " \"According to the machine learning model, it is more likely that the case's label is C2, with a certainty of 100.0%, and this prediction decision is mainly based on the effects of the following features: F8, F10, F6, F9, and F1 on the model. Apart from F1 and F9, all the other variables mentioned above have a strong positive influence, improving the odds of the prediction class, C2. Together with F1 and F9, the values of variables F3 and F2 indicate that C1 could be the correct label instead. Unlike the top positive variables, F8, F10, and F6, each of these negative variables has a moderate contribution to the final decision. The features F5, F7, F11, and F4 are shown to have made minor contributions to the model's decision in this case. In summary, with only the positive contributions from F8, F10, F6, F11, and F5, the model is very certain of the classification output as indicated by the predicted probabilities across C2 and C1.\",\n",
       " 'The classification findings by the model for the case here are as follows: there is a 97.67% chance that C1 is the correct label hence only a marginally low chance of 2.33% that C1 is not the correct label but C2 is. From the above findings, it is valid to conclude that the right class for the given case is C1, and the model is very certain of this decision. The features with the most control and influence on the classification above are F27, F4, F17, F22, and F7 but the influence of the remaining features is either moderate or low or negligible. Some of the features with moderate impact include F14, F11, F15, and F23. Those with low influence are F18, F10, F25, F9, and F8. Finally, those with negligible impact are F19, F3, F1, F6, F5, F20, F29, F28, F2, and F13 since their values are shown to have no impact on the classification made by the model here. The top positive features increasing the prediction likelihood of class C1 are F27, F7, and F24. Conversely, the negative features decreasing the odds in favour of C2 are primarily F4, F15, and F17.',\n",
       " 'The model is very confident that C3 is the most probable class for the given case, with a probability of 90.48% which means that the other labels are very unlikely. F12 and F1 are the most important variables with respect to this classification verdict while all other variables are shown to have a medium or low impact. Fortunately, the top variables, F12 and F1, have the same direction of influence, increasing the likelihood of C3. Furthermore, while F4 and F11 push the model to predict C3, those pushing for the assignment of a different label are F3, F8, and F2. Finally, many features have a fairly small impact on the final prediction made by the model here, but F7, F8, and F6 have the least impact.',\n",
       " \"Despite the reasonably high confidence in the assigned label, the prediction probabilities across the two classes indicate that C2 might be the correct label. F7, F2, F9, and F8 are the factors whose major contributions resulted in the labelling choice mentioned above. According to the analysis, the top two factors, F7 and F2, have a negative influence, leading the classifier to classify the data as C2 rather than C1. F1 is the only other negative variable with a moderate effect when compared to the other two negative variables. Nevertheless, there are several factors, F9, F8, F3, F6, F4, and F5, that favourably support and encourage the classifier to assign C1. All in all, the degree of uncertainty in this classification instance might be explained by just looking at the negative factors' rather strong pull on the classifier towards C2.\",\n",
       " 'C1 has a probability estimate of only 6.80%, while that of C2 is 93.20%; consequently, the most likely class for the given case is C2. The important or relevant features considered by the classifier are F12, F17, F8, F16, F9, F24, F14, F34, F23, F29, F1, F38, F37, F18, F6, F22, F25, F7, F10, and F4. Not all input features are relevant when determining the appropriate label and these irrelevant features include F32, F13, and F28. Furthermore, F12 and F17 have a strong positive effect, increasing the odds in favour of C2. In contrast, the F8, F9, and F16 are the negative features, lowering the odds of C2. Comparing the attributions of F12, F24, and F17 features to those of the negative features mentioned above, it is not surprising that the classifier is convinced that C2 is the most likely label here.',\n",
       " \"The model is not 100% convinced that the correct label for the data under consideration is C2 since there is a 26.27% chance that labelling the data as C1 is correct.  All the input variables are shown to have some degree of influence on the classification decision, with the most influential variables being F9, F2, and F7, whereas F6 and F1 are the least influential. The impact of F5, F3, F4, and F8 can be considered moderate compared to the F9, F2, and F7. The uncertainty surrounding the above classification can be blamed on the fact that the majority of input variables have values suggesting that C1 could be the appropriate label. The negative features that decrease the prediction likelihood of C2 are F9, F7, F4, and F8. However, given that the prediction probability is about 73.73%, it can be said that the influence of positive features, F2, F5, F3, and F6, is enough to swing the model's verdict in favour of C2.\",\n",
       " \"The model predicted class C1 with an 81.98% prediction likelihood. F24 had the largest impact, followed by F23, F9, F18, F14, F10, F11, F2, F8, F21, F20, F27, F4, F12, F15, F19, F13, F16, F30, and finally, F29, which had the smallest non-zero impact. F24, the feature with the largest impact, contributed against the direction of the prediction, whereas F23, F9, F18, and F14 all contributed positively towards the prediction. Other features that had a negative influence on the prediction included F11 and F2, whereas F10 had a positive influence on the prediction. F22, F6, F1, and F5 are shown to have close to zero attribution in the model's prediction verdict in the given case.\",\n",
       " \"This case's label has a 70.83 percent chance of being C3 and per the predicted likelihoods across the alternative labels, C1 has a 29.71 percent chance of being the correct label, however, the model is certain that C2 is not the true label. The most important variables are F1, F7, F3, and F2, whereas the remaining influential variables are listed in order of the magnitude of their contributions: F8, F6, F4, F9, and F5. Three of the nine variables have values that push towards the prediction of label C1 while the other attributes are referred to as positive since their values inspire the prediction of class C3. F1, F7, and F3 are the three attributes that have a negative influence on the prediction judgement, pushing it away from C3 towards the label C1. Finally, it is essential to highlight that the cumulative effect of positive attributes is greater than that of negative attributes, F3, F7, and F1.\",\n",
       " \"First of all, the classification decision is solely based on the information or data supplied to the prediction model. According to the model, there is a 61.61% chance that C1 is the true label, and a 38.39% chance that C2 is the true label. Since the predicted probability of C1 is higher than that of C2, it is valid to conclude that C1 is most likely the true label. The main feature responsible for this classification is F14, with a very strong positive influence, driving the model's decision higher towards C1. The next set of relevant features are F7, F17, F18, F15, F24, F32, F30, and F10. Among all the features mentioned above, F7, F18, F15, F32, and F30 have negative contributions that are responsible for the decrease in the probability that C1 is the true label. This implies that the contributions of F17, F24, and F10 combined with that of F14 explain why the model is moderately certain that C1 is the true label.\",\n",
       " 'Because the prediction probability of C2 is barely 0.70 percent, the classifier outputs the label C1 with near 100 percent confidence based on the values of the input attributes. The effects of F8, F4, and F2 on the aforementioned classification decision are significant. The values of these features are given greater emphasis by the classifier than the others. F2 is has a negative impact among these top features, pushing the prediction judgement towards the least likely class, C2 whereas on the other hand, F8 and F4 are referred to as positive features since they improve the likelihood of the C1 label rather than the C2 label. Finally, unlike the others, the values of F1, F9, F6, and F15 have only a little influence on the label selection made here.',\n",
       " \"The classification algorithm arrived at the prediction output based on the variables or information supplied about the case under consideration. The prediction probabilities across the three-class labels, C2, C3, and C1, respectively, are 28.17%, 50.21%, and 21.62%, making C3 the label assigned by the algorithm, judged based on the prediction probabilities. The attributions analysis suggests that F4, F3, F9, and F8 are the positive features that increase the algorithm's prediction response in favour of C3. On the other hand, F7, F1, F6, F10, F2, F12, F11, and F5 have negative contributions in support of labelling the case as either C2 or C1. Overall, judging by the degree of contributions of the positive features, it is not surprising that the algorithm is moderately certain that neither C2 nor C1 is the most probable label for the case under consideration here.\",\n",
       " \"Judging based on the information provided on the case under consideration, the model outputs that the prediction probability of C2 is only 0.48%, indicating that with about 99.52% certainty, the true label here is C1 and in simple terms, the model is very confident that the true label for the case under consideration is C1. The higher degree of certainty in the above classification can be attributed solely to the positive contributions of influential features F4, F12, and F16. Analysis indicates that all the remaining features such as F1, F8, F17, F7, and F14 have moderate to low contributions towards the prediction conclusions above, whereas F3, F11, F15, and F13 are the least relevant features here. The very marginal decrease in the C1's prediction likelihood could be attributed to the influence of negative features F8, F7, F5, F13, and F11 since their contributions support labelling the case as C2 instead. Moderate positive features further driving the model to label this case as C1 are F1, F17, F10, and F14.\",\n",
       " 'Judging based on the values of the variables passed to the model with respect to the case under consideration, the output labelling decision is as follows: there is about an 83.98% chance that C2 is the correct label, whereas the likelihood of C1 is only 16.02%, hence the label choice with a higher confidence level is C2. The top-variables influencing this decision are F2, F4, F3, and F14, while the least important variables are F9, F12, and F13. According to the variable contributions analysis performed, only the input variables F8, F16, F19, and F15 exhibit negative attributions, pushing the prediction decision towards the alternative label, C1. The other variables positively support the C2 prediction, shifting the verdict strongly away from the C1 class. In conclusion, positive variables such as F2, F4, F3, F14, F7, and F17 have a higher joint contribution compared to the negative features, which can explain why the model is certain that C2 is the most probable label.',\n",
       " 'The model predicts the class label C4 for the given test instance with a likelihood of about 69.23%. However, there is about a 30.77% chance that the true class label is C2, while the others, C3 and C1, have a 0.0% likelihood. The top features contributing to this prediction decision are F1, F15, F9, and F2, whereas the least important are F12, F11, and F6. Among the top features, while F1 and F15 have values that shift the prediction decision towards the C4 class label, the values of F9 and F2 suggest that the true label could likely be C2. For the features with moderate influence on the decision, F4, F14, F18, and F5 have negative contributions, further decreasing the confidence level in the C4 assignment. On the other hand, the moderate positive influences of F17, F8, F20, F19, and F16 drive the decision further towards the C4 label. Considering the attributions of the input features, it is surprising that the confidence level is just 69.23% since the top feature, F1, has the highest contribution among all the input features. Finally, the values of F10, F11, and F6, though shown to be less important when deciding the correct label for the given case, have positive contributions to the prediction with respect to the given case.',\n",
       " 'The given case is likely C2 with a confidence level of 87.50% judged based on the values of the input features supplied to the classifier and according to the attributions analysis, F9 and F2 have a high degree of impact. F6, F8, F3, F4, and F5 have a moderate degree of impact while on the contrary F7 and F1 have little impact. Examining further, the values of F9, F2, F6, and F8 all have a positive influence on the classifier supporting the label assignment decision for the given test case. F3 and F5 are also positively supporting features, whereas F4 has a negative influence on the final classification. Finally, F7 and F1 both have very little contributions, though F1 has significantly less than even F7.',\n",
       " \"The label for this example is estimated to be C2 among the four possible classes, with a 73.08 percent chance of being true. C1 is the next most likely label, with a probability of roughly 26.92 percent. The above prediction assessment is mostly dependent on the values of the variables F3, F9, F8, F5, and F12. F3 had the greatest influence, followed by F8, F9, F12, and F5. The positive variables F3, F9, F10, and F17 outnumber the negative variables F8, F12, F5, and F19. Twelve of the twenty variables have values that tilt the prediction towards one of the three other probable classifications. As a result, it is not unexpected that the model is not completely certain of the C2 assigned. Given that the chance of C2's being accurate is 73.08 percent, the model appears to be relatively confident in its final judgement for the data instance under review.\",\n",
       " \"The model has classified the instance as C2 due to the effects of the following features: F5, F8, F6, and F2. Based on the values of these variables, the likelihood of the C2 label is 65.51 percent. F2 and F6 are the top positively contributing variables, whereas F5 and F8 are the most adversely contributing variables. Unlike F2 and F6, which have greater influences on the model's prediction choice in this situation, F3 and F9 have fairly modest positive influences. Finally, F1, F7, and F4 show negative predictive effects, however, as compared to F5, their attributions are modest.\",\n",
       " 'Considering the predicted likelihoods across the classes,  C2 is confidently chosen as the true label since its likelihood is 93.27%, implying that the likelihood of C1 is only about 6.73%. F6 and F15 are the two features with a very strong positive influence, favouring the prediction of class C2. The following features have a moderate effect and are listed in descending order of influence: F5 and F10 have a negative effect, while F4 and F13 have a positive effect on the prediction of C2. Similar to F5 and F10, the features F9 and F8 also negatively affected the prediction decision. Finally, the values of F2, F16, F1, and F7 are the least important to the model decision for this case.',\n",
       " 'The classification output is C1, however, the classifier is somewhat unsure about this prediction decision because the corresponding predicted probability is only 55.19%. F11 is by far the most influential feature whereas F4, F6, and F17 have been recognised as having the biggest effect on prediction output here after F11. The combination of F11, F4, F6, F17, and F3 features has resulted in the classification choice being altered from C1 to C2. While F5, F2, and F16 all have a minor influence on the classification, F5 is the only one that has a positive impact on the C1 classification. In this case, many features had lower influence on the prediction, with F15, F8, F13, F19, and F10 having a marginal effect.',\n",
       " 'Judging based on the values of the input variables, the classification algorithm labels the case as C2 since its prediction likelihood is equal to 88.69%. The prediction decision is primarily based on the contributions of F2, F1, and F9, however,   F7, F4, and F10 are shown to be the least important variables. Regarding the direction of influence of the variables, F2, F9, F6, F7, and F4 are the positive variables that increase the odds of C2 being the correct label. Driving the prediction toward the alternative label, C1, are the variables F1, F3, F8, F5, and F10. Owing to the fact that the most influential variables, F2 and F9, have strong positive attributions, outweighing the contributions of the negative variables, it is not surprising that the algorithm is certain about the decision made.',\n",
       " 'The classification algorithm predicts class C1 with a confidence level of 61.55% and this implies that the probability of the alternative label is only 38.45%. In this case, the top features driving the prediction decision are F7, F9, F1, and F2, followed by F4, F5, F8, F3, and finally F6. Based on the inspections performed to understand the direction of influence of the input features, it can be concluded that F7 has the strongest positive contribution, while F1 has the strongest negative contribution and conversely, all the remaining features have moderate contributions. The other positive features are F9, F4, F5, and F3, whereas the remaining negatives are F2, F8, and F6. All things considered, the influence of the negative features indicates that the likelihood of the C2 label is 38.45% while the positive contributions push the prediction higher towards C1 resulting in the 61.55% prediction confidence.',\n",
       " \"The classification model's decision about the true label for the case is based on the information provided to it. Among the three labels, C2, C3, and C1, the model shows without a doubt that neither C3 nor C1 is the true label, given that the probability of C2 being the true label is 100.0%. F12, F6, and F2 are the main contributing factors or variables in the final verdict here since their respective influence outranks the remaining variables. In fact, analysis indicates that F3, F11, and F5 are the least influential variables since they receive little emphasis from the model when making the labelling decision here. In between F12, F6, and F2, and F3, F11, F8, and F5, are the variables such as F9, F10, F1, and F7 with moderate influence on the classification decision here. Among the variables passed to the model, only F9, F7, and F3 are shown to have negative contributions, which suggests that perhaps the true label could be either of the remaining labels. However, given the 100.0% predicted likelihood of C2, it is reasonable to deduce that the positive variables, such as F12, F6, F2, F10, F4, and F1, significantly influence the model's judgement towards C2.\",\n",
       " \"According to the model, C1 is the class with the higher probability, which is equal to 52.57 percent, of being the label for this selected instance or case. Conversely, there is a 47.43 percent chance that C2 is the correct label showing that the model is less certain about the classification verdict in this case. This uncertainty can be linked to the fact that the majority of variables have values that favour assigning C2. The only variables increasing the model's response to prediction C1 are the positive variables namely: F6, F16, F7, F1, F8, F11, and F5. The top negative variables decreasing the likelihood of C1 are F14 and F19 supported by other negative variables, F2, F18, and F9, that further shift the verdict towards C2.\",\n",
       " 'According to the predicted likelihoods across the classes, C1 has a 17.0% chance of being the true label for the given data or case, implying that C2 is the most likely label. F12, F19, and F20 are the most important factors that led to the classification judgments above. The remaining factors have a minor or non-existent impact on the classifier. The classifier most likely ignored the values of F18, F3, F1, F8, F14, and F5 when giving a label to this case since their relative degrees of impact are extremely near to zero. F9, F15, F24, F11, F21, and F4 are considered negative factors among the significant factors because their contributions to the choice tend to reduce the chance that C2 is the correct label. These negatives features lend themselves to the case being classified as C1 but the remaining features contribute positively, raising the likelihood of the C2 classification.',\n",
       " 'According to the ML model, C1 is the most likely class label, and we can conclude that the model is quite confident about the decision given that the probability of having C2 as the correct label is only 7.0%. For the case under study, analysis indicates that F4, F8, F3, and F6 are essentially the negative set of features that push the forecast higher towards C2 instead of C1, while F7, F11, F2, and F1 increase the odds of the prediction being equal to C1. In general, the most relevant feature is F7, while F5 and F9 are the least relevant features, with marginal influence on the above classification verdict. In summary, given the very strong positive influence of F7 together with the moderate influence of the other positives, F11, F1, and F2, it is not strange that the model chose to label the case as C1 instead of C2.',\n",
       " 'Because the confidence level associated with the other class, C1, is just 2.29%, the model predicts that the given example is likely C2 and to be specific, the model is quite certain that the right label for the given case is C2. All the features are shown to have some degree of influence on the decision above, with F14 and F6 being the least relevant features, while F5 and F9 are the top features. From the analysis performed to understand how each feature contributes to the above prediction assertion, only  the features F13, F12, F1, F8, F7, and F6, have negative influences, shifting the prediction verdict towards C1.  The remaining features all contribute positively, strongly shifting the prediction towards the assigned label which could explain the prediction confidence level associated with label C2. The most positive features are F9, F2, and F5 with stronger push in favour of the output label and they are supported by other positive features such as F4, F10, F3, and F11 have a moderate degree of influence.',\n",
       " 'The classifier made the prediction here based on the information provided about the case under consideration, and according to the classifier, the prediction probabilities or likelihoods across the labels C1 and C2 are 100.0% and 0.0%, respectively. All the input features are shown to have different degrees of influence on the final decision here by the classifier. The most influential features are F5 and F3, with F2 and F6 ranked as the least contributing factors. The values of F4 and F1 suggest that perhaps the true label could be C2 since they are the negative features. However, considering the confidence in C1, it is valid to conclude that the joint influence or contribution to the classification of the negative features with respect to the given case is outmatched by the joint positive attribution of F5, F3, F2, and F6.',\n",
       " 'For the selected case, the model assigns the label C1. The prediction probability distribution across the classes C2 and C1 is 2.40% and 97.60%, respectively. The most important features considered for this prediction are F18, F3, F12, and F15, while on the other hand, the least relevant features with little contributions to the decision based on the analysis are F1, F8, F19, and F9. The top positive features Increasing the likelihood of the prediction being made are F18, F3, and F15. Pushing the prediction towards the alternative class C2, the top negative features are F12, F14, and F4. F17, F11, F13, F7, and F2 are some of the features that have a moderate impact on the classification decision in this case.',\n",
       " \"The given instance was labelled as C1 by the model based on the values of its features. The model is about 79.64% certain about this prediction decision, hence, there is a slight chance that the label could be C2. Among the different features, the ones with the most impact on the model are F8, F25, F9, F2, and F27. The most negative feature is F8, and it is significantly pushing the narrative toward the prediction of C2. From this, it is foreseeable that there is a chance that the true label could be C2 which is about 20.36%. The influence of F8 and F9 is somewhat counterbalanced by the values of the features F25, F2, and F27. Other attributes that shift the decision in favour of C2 are F7 and F24. F3 shifts the decision further in the direction of C1 and in addition, F31 supports the model's prediction while the values of F5 and F32 of the given test case contradict the model's decision, decreasing the likelihood of C1. Among the features not relevant to this prediction decision for this case are F10, F20, F28, and F26.\",\n",
       " \"According to the classification algorithm, the best label for the given case is C2, because there is little to no chance that C1 is the correct label. Not all of the features are found to contribute to the label given here. The following significant features are ordered in order of their effect on the algorithm's output: F36, F8, F26, F35, F3, F12, F24, F9, F21, F6, F20, F5, F4, F25, F19, F27, F7, F23, F37, F31. F30, F33, and F13, on the other hand, are unimportant features since they have almost no influence. Among the most influential features F36, F8, F26, F35, and F3, F26 is considered the most negative, dragging the verdict in a different direction, while the others have positive contributions, increasing the possibility that C2 is correct in this case. F24 is recognised as a positive feature with modest effect, whereas F12 and F9 are identified as negative features. Given that the majority of the top five attributes have positive contributions, boosting the likelihood that C2 is the correct label, it is not unexpected that the algorithm is quite confident in the assigned label's accuracy.\",\n",
       " 'The classifier labbelled the given case as C2 with a confidence level of 98.89%, implying that the chance of C1 being the correct label is only about 1.11%. The classification output decision is solely based on the information supplied to the classifier about the case under review. We can rank the contributions of the features as follows: F12, F6, F8, F3, F13, F15, F5, F1, F10, F2, F9, F14, F7, F11, and F4. Among the top features, F12 is the only negative feature, increasing the probability of predicting the alternative label, C1. Other top features that are shifting the prediction towards C2 are F6, F8, and F3. Similar to F12, the features F5, F11, and F2 have negative contributions, supporting the generation of C1. By comparing the strong joint positive attribution to the joint negative attribution, it is evident why the classifier is very certain that C2 is the right label for this instance.',\n",
       " \"The prediction probability associated with class C1 is 10.50%, while that of class C2 is 89.50%, therefore, it can be concluded that C2 is the most probable label for the given case according to the model. All the input features are shown to contribute to the above decision, and the ones with the strongest influence on the classification decision are F8, F7, and F1, but F10, F4, F6, and F3 are shown to be the least relevant features . Finally, the degree of influence of F9, F5, and F2 can be described as moderate. The model's high confidence can be attributed to the strong positive contributions of F7 and F8 which are supported by the contributions of the remaining positive features F9, F10, and F4. Conversely, shifting the prediction in favour of C1, the negative features F1, F5, F6, F2, and F3.\",\n",
       " 'The model labels the case as C2 with fairly high confidence equal to 89.73%, whereas the likelihood of C1 is only 10.27%. Analysis shows that only 20 of the 46 input variables contribute to the prediction assertion above. The prediction judgement C2 is mainly based on the variables F17, F9, F18, and F19. F43, F23, F32, F33, F29, and F20 also contribute to the decision, however, their degree of influence is only moderate. According to the direction of influence analysis, F17, F19, F29, and F33 positively support the decision of the model to assign the label C2. However, F9, F23, F20, F18, F43, and F32 reduce the likelihood or chance that C2 is the true label for this particular test instance. The main variables with less influence on the above classification decision are F14, F31, F7, and F38.',\n",
       " 'The label predicted for this case is C1 with very high confidence of approximately 97.71% which insinuates that there is a marginal possibility that C2 could be the label. The above classification decision is largely due to the values of F4, F8, F3, and F14. On the other hand, F6 and F1 are less relevant when the model is deciding the correct label for the case here. Digging deeper revealed that each feature either positively or negatively contribute to the prediction made here. Six features contradicted the classification decision, while the remaining ones positively supported the C1 prediction. The negative features driving the prediction towards C2 are F3, F5, F13, F2, F9, and F1 and countering their influence are  the top positive features are F4, F8, F7, and F14.',\n",
       " 'The likelihood of C2 being the correct label for the selected case or instance is 67.54% according to the classifier. This means, there is a 32.46% chance that C1 could be the label and the classification assertion above is influenced mainly by the variables F11, F4, F7, and F9. On the contrary, F10, F2, and F5 are deemed less important when deciding the correct label for this given case.   Decreasing the likelihood of the predicted label , C2, are the variables  F9, F8, F2, and F5, therefore, these negative variables support the alternative class C1. However, the collective or joint attribution of the top positive variables, F4, F11, and F7 is strong enough to tilt the classification in favour of C2.',\n",
       " 'The confidence level score with respect to each class label suggests that this case should be labelled as C2. Specifically, there is about an 80.0% chance that C2 is the correct label. However, this implies that there is also about a 20.0% chance that it should be C1. The above prediction decision is based predominantly on the influence of the following features: F8, F1, F7, F5, F3, F11, and F2. According to the analysis, the features F8, F1, and F7 have a very strong positive influence, swinging the prediction decision towards C2. In contrast, the value of F5 also suggests the decision should be the alternative class, C1. Similar to F5, the values of F10, F3, and F11 indicate the label could be C1. However, the influence of these features is very small compared to F8, F1, F7, and F5. Finally, the attributes with a moderately low influence on the final prediction decision for this case include F2, F6, F4, and F9. The values of F2 and F9 have a negative attribution, while F6 and F4 have positive attributions.',\n",
       " 'The classification algorithm is pretty confident that the correct label for the data under consideration is C1, wowever, it is noteworthy to consider that C2 has about a 15.13% chance of being the correct label.  The predicted probability of each label is assigned based on the influence of features such as F6, F3, F10, and F8. However, the analysis shows that the values of F11, F2, F12, and F5 are less relevant when classifying the data.   Only the features F10, F9, F11, F2, F12, and F5 have negative attributions, decreasing the predicted probability of the assigned label and one can say these features are shifting the prediction decision towards the label C2.',\n",
       " 'The case, despite having features with considerable negative impact, also has numerous and measurable positive features, so the assignment of the label C2 by the model is very likely since the predicted probability is 91.95% which is very higher than that of C1. The F26, F6, and F16 were the most important features driving the model to arrive at the labelling assignment of C2. F11 and F12 have nearly identical positive attributions, while F19 and F4 has negative impacts, swinging the prediction towards a different label. However, the joint positive impact of F11, F26, F16, and F12 stands out over the impact of F6, F18, F4, and F19, favouring the prediction of the C2 model. All things considered, there are more features with a positive impact than those with negative impact; the mean attribution of the positive attributes is much larger which somewhat explains why the confidence level is very high. Above all, it is important to note that the prediction is made with less emphasis on the values of F15, F5, F10, and F9 hence they are practically irrelevant when it comes to labelling this case.',\n",
       " 'The predicted label is C2 given the predictability of C3 is 28.96% and that of  C1 is 23.41%. Considering the probabilities of the classes, the model can be described as being moderately confident. The prediction of C2 can be attributed to the varying degree of contributions of the input features. Attribution analysis indicates that F9, F7,  and F3 are considered the most influential. Those with moderate influence are F10, F5, F11, F4, F2, and F6, whereas on the contrary, the least influential ones are F8, F1, and F12. The analysis also revealed that not all the features contribute positively to the prediction decision and amongst the input features, the ones with negative attributions decreasing the likelihood of the C2 prediction are  F7, F3, F11, F4, and F2 whereas conversely, the top positive features are F9, F10, and F5.',\n",
       " \"The model outputs a predicted probability of 2.55% for the C1 label and 97.45% for the C2 label. Judging from above, the most probable class is C2. Hence, C2 is the assigned label by the model, with a very high confidence level. The top features contributing to the prediction assessment above are F51, F26, F4, F55, and F9. However, the values of about twenty features are deemed relevant while the remaining are regarded as irrelevant when classifying the given case. These irrelevant features include F67, F45, F11, and F71. Among the relevant features, F4, F6, F32, F28, F10, and F57 are shown to be the only positive features that increase the model's response in favour of the assigned label C2. In contrast, the majority of the relevant features, mainly F51, F26, F55, and F9, have negative contributions, decreasing the odds of the label C2, hence supporting the assignment of C1 to the given case.\",\n",
       " \"The algorithm identifies the provided data or case as C1 with a greater level of certainty since the prediction probability of class C2 is just 0.07 percent as a result, C2 is less likely than C1. The influence of input features such as F7, F21, F33, F8, and F27 is mostly responsible for the classification verdict above with only F27  having a negative influence among them, slightly pulling the decision in favour of C2. F7, F21, F33, and F8, on the other hand, make considerable positive contributions in favour of assigning C1 to the data. F10, F37, F34, F23, F25, F17, F1, and F14 are some more features that have a modest effect on the algorithm's decision. But, not all features are demonstrated to influence the classification decision either negatively or positively to the aforementioned classification outcome and in reality, a number of these are demonstrated to be irrelevant for determining the suitable label for this case and these include F13, F18, F35, and F19. All in all, the most important features for this classification instance are F7 and F21, whereas F38 and F32 are the least important.  \",\n",
       " 'For the given dataset instance, the label assigned by the classifier is C1 since it has a predicted probability of about 89.16%. On the other hand, there is a 9.0% chance that C2 could be the appropriate label, whereas C3 only has a 1.84% chance of being the true label. The classifier arrived at this classification verdict chiefly due to the influence and contributions of variables such as F5, F4, F10, and F6. However, there is less emphasis on the values of F1, F8, and F2, since their impact on the classifier with respect to the given case is smaller compared to the other variables, hence they are the least ranked features. From the attribution analysis, there are four variables with negative contributions, pushing the verdict in the direction of C2. These negative variables are F5, F6, F7, and F3, and their influence on the classifier could explain why there is a little bit of doubt about the correctness of the C1 class assigned and the notable positive variables are F4, F9, F1, and F10.',\n",
       " 'The case is labelled as C2 by the model but looking at the predicted probabilities across the different classes, there is a 33.63% chance that the label could be C1. To explain the above prediction conclusion, the analysis revealed that the majority of the features have negative influences or attributions, pushing the prediction away from C2 in favour of C1. The negative features include F8, F6, F17, F4, and F10 and the values of these features are ranked higher than any of the positive features. Shifting the prediction in the direction of C2 are the positive features F5, F19, F15, and F16. The analysis also revealed that the values of F11, F12, and F3 are less relevant to the prediction for the case under consideration.',\n",
       " 'The most likely label chosen by the model in this case is C1. The decision above is based on the prediction probabilities for the two possible labels, C1 and C2, which are 94.25% and 5.75%, respectively. The following variables can be ranked from most important to least important based on their contribution to the model when it comes to this instance: F4, F10, F1, F2, F7, F9, F8, F6, F3, and F5. F10 and F4 turned out to be the most important positive variables, supporting the model towards assigning the class C1. The least positive variables are F6 and F8, which have less effect on the model. In fact, most of the input features have negative contributions towards the assignment of class C1, leading to a decision change in favour of the other label, C2. The most negative variables are F7, F1, and F2, and the least negative are F3 and F5.',\n",
       " 'As per the classification algorithm employed, the most probable label for the data under consideration is C2 since the chances of C1 is very slim and negligible.  The main driver behind the labelling decision above is F1. The features with moderate influence are F5, F6, F2, F8, F9, and F4, while those with very small or marginal impact are F3 and F7.  The direction of influence of the input features could be used to explain why the algorithm is very confident here. Most of the features have a positive impact, increasing or improving the chances of C2 being the correct label and the feature with a significantly higher contribution, F1, is a positive feature which when coupled with other positives F6, F2, F4, and F9 encourages the prediction or assignment of the C2 label. Furthermore, aside from F5 and F8, the other two negative features, F3 and F7, are shown to have a significantly lower impact on the algorithm and the very marginal doubt in the decision can be attributed to the influence of the negative features.',\n",
       " \"The ML algorithm classifies the provided data or case as C2 with a likelihood of 80.70%, hinting that the likelihood of C1 being the correct label is only 19.30%. This classification decision above is mainly based on the influence or contributions of the input features. The most relevant features driving the classification algorithm to arrive at the above decision are F11, F16, F22, F6, F32, F27, and F25. On the other side, not all of the input features are considered relevant when deciding the appropriate label for the given data instance, and these irrelevant features include F24, F18, F31, F3, and F19. Among the top influential features, F32, F27, and F25 are regarded as negative features since their contributions push the algorithm's decision towards the less likely class, C1, although F11, F16, F22, F29, and F6 have positive contributions, increasing the probability that C2 is the right label here.\",\n",
       " 'The predicted probability of class C2 is 12.81% and that of class C1 is 87.19%. Therefore, the label chosen by the model is C1, which is the most probable class. The top two features with significant influence on the prediction verdict above are F5 and F20. These features have positive attributions, shifting the decision higher in support of label C1. Other positive features are F28, F8, F3, and F37. Decreasing the likelihood of the assigned label are the negative features such as F31, F17, F16, and F6. Finally, the values of features such as F39, F4, F22, F30, F14, and F1 are considered irrelevant to the prediction decision above.',\n",
       " \"The C1 has a predicted probability of just 3.10% while that of the C2 is 96.90%, therefore, the most likely class selected by the classifier for the given data is C2. The relevant features contributing to this classification are mainly F23, F4, F31, F9, F35, F14, F5, F25, F3, F8, F18, F36, F34, F1, F22, F10, F33, F29, F16, and F11. As per the attribution analysis, F23 and F4 have a very strong joint positive contribution, increasing the classifier's response higher in favour of C2 than C1. In contrast, F31, F35, and F9 are the top negative features, degrading the classifier's response in favour of C1. Comparing the attributions of F23, F14, and F4 to those of the negative features mentioned above, it is not surprising that the classifier is quite confident that C2 is the most probable label here.\",\n",
       " 'There is only a 17.0% chance that C1  is the correct label which implies that the most probable label for the given data or case is C2 given its predicted likelihood of 83.0%.  The main influential features resulting in the classification conclusions above are F25, F5, and F3 whereas the remaining features have either a moderate or negligible influence on the classifier. When it comes to assigning a label to this case, the classifier likely ignored the values of F1, F13, F12, F24, F14, and F19 since their respective degrees of influence are very close to zero.  Among the influential features, only F26, F18, F2, F16, F23, and F10 are considered negative features mainly due to the fact that their contributions towards the decision here only serve to decrease the likelihood that C2 is the correct label and it can be said that these features favour labelling the case as C1. The remaining features such as  F25, F5, F3, F7, F4, F20, and F15, offer positive contributions, increasing the likelihood of the C2 class.',\n",
       " \"The classification algorithm's decision on the true label for the given case is solely dependent on the information presented to it. Per the algorithm, the accurate label for the case under consideration is most likely C1, and the 12.47% possibility of C2 reflects only a minor uncertainty in the classification algorithm's certainty. The marginal doubt mentioned above can be blamed on the negative contributions of F11, F6, F7, F1, and F5, supporting the assignment of C2 instead of C1. Conversely, the positive contributions of F2, F10, F3, F4, F8, and F9 are shifting the algorithm's decision higher in favour of label C1, hence the high certainty of its correctness. Overall, F11 and F6 are the most influential negative features, whereas F2 and F10 are the most positive features. Also, F12 is shown to have a negligible influence on the classification decision with respect to the case here.\",\n",
       " 'The model indicates that the label for this case is likely C2, with an 83.33% chance that it is correct, implying that it is unlikely that C1 is the appropriate class. This predictive assertion is chiefly influenced by the values of the input variables F13, F24, and F1. While the F1 and F24 values positively control the model towards the prediction of C2, the F13 value biases the decision towards C1. However, the combined effect of F1 and F24 outweighs the contribution of F13. In addition, the variables F7, F18, and F21 also positively support the output predictions of the model. F17 has similar direction of contribution that of F13, further decreasing the odds of the C2 label. Unlike all the variables above, F25, F26, F4, F9, F15, and F8 are shown to have very little effect on model predictions with respect to the given case and we can say that their values receive very low consideration from the model.',\n",
       " \"The algorithm's forecast for the data instance under consideration is C2, and the decision's confidence level is about 91.36 percent. We can observe from the plot that the variables F13 and F11 are moving the prediction judgement towards the other label, C1. The F12, F8, F2, and F7, on the other hand, have values that have a favourable influence, pushing the data classification choice towards label C2. While F1 and F3 contradict the prediction, F10 and F14 have values that confirm the algorithm's prediction output verdict.\",\n",
       " 'The model classifies this case as C1 and it is noteworthy that there is, however, a 38.26% chance that the true label could be class C2. The uncertainty associated with the classification decision above is higher than expected, which could be attributed to the values of the different input features. The most influential feature is F9, which has a positive effect on the class C1 prediction by the model here. All other features are much less influential, with contributions from F20, F2, F22, and F12 shifting the prediction towards C2. Supporting the model in assigning the label choice, F1 is the next most influential feature. The impacts of the F21 and F17 are moderate, ranking seventh and eighth, respectively. Unfortunately, values of features such as F33, F23, F32, and F6 do not matter when determining the correct label in this instance.',\n",
       " 'The classification or prediction algorithm indicates that the most probable label for the given data is C1 since there is only a 25.47% chance that C2 could be the correct label. The major factors resulting in the above decision are F10, F7, and F5, while the set of features with moderate influence are F9, F2, F1, and F8. The least vital features are shown to be F3, F11, F4, and F6. In conclusion, it is very surprising to see the uncertainty surrounding the classification here given that only F9 and F1 have a negative impact, driving the algorithm to label the data as C2. To be specific, the contributions of F9 and F1 result in a decrease in the likelihood of C1 being the right label, as indicated by the prediction probabilities across the two possible classes but the influence of these negatives are moderated by the major positive features which are F10, F7, and F5.',\n",
       " 'The classification verdict is as follows:  the most probable label for this case is C1, and the classifier is certain that neither C2 nor C3 is the correct label. The main drivers for the above classification are F4, F1, and F7, all of which have a strong positive influence, pushing the classifier to choose C1. Other positive features pushing the classification further higher towards C1 include F9, F6, F11, and F2. Not all the input features support the assigned label and the negative features F12, F5, and F3 indicate that the most probable class for this case could different from the assigned label. However, considering the confidence level in the above classification, it is valid to conclude that the classifier paid little attention to the negative features, hence selecting class C1.',\n",
       " \"The best choice of label for the given case is C2 according to the classification algorithm, since there is little to no chance that C1 is the right class. Not all the features are shown to contribute either positively or negatively towards the label assigned here. The influential features can be ranked according to the associated degree of impact on the algorithm's output as follows: F8, F21, F27, F24, F14, F25, F28, F17, F26, F15, F22, F12, F20, F4, F19, F7, F16, F35, F6, F30. On the other hand, the irrelevant features include F36, F9, and F38 since they have close to zero impact. Among the top influential ones, F8, F21, F27, F24, and F14, the input feature F27 is regarded as the most negative, dragging the verdict in a different direction, while the others have positive contributions, improving the likelihood that the choice of C2 is appropriate in this case. The features with moderate influence are F28, F25, F17 where  F28 is identified as a positive feature, while F25 and F17 considered negative features. Since a large number of top features have positive contributions that increase the probability that C2 is the right label, it is not surprising that the algorithm is very confident about the correctness of the assigned label.\",\n",
       " 'For the given case, the prediction decision is as follows: The probability of C2 being the correct label is only 18.57%, the probability of C1 is 81.43% making it the most probable label for the case here. The certainty of the prediction can be attributed to the influence of variables such as F8, F12, F4, F6, and F7. The least relevant variables considered to arrive at the classification verdict are F2, F5, F10, and F11. F9, F3, and F1 have moderate contributions to the classification here. The attribution analysis performed indicates that F6, F7, F3, F1, F5, and F11 are the negative variables, decreasing the likelihood of C1 in favour of labelling the given case as C2. The variables F8, F12, and F4 have the highest positive influence, which increases the odds of label C1 being the correct label.',\n",
       " 'The output decision of the classifier with respect to the given case is: C1 is the most probable label, followed by C2 and C3. To be specific, the predicted likelihood across the classes are as follows: 86.54% for C1, 13.46% for C2, and finally a 0.0% probability with respect to C3.  The moderately high classification confidence could largely be due to the impact of certain input features supplied to the classifier. F9, F12, F7, F6, and  F8 are the top-ranked variables whereas the least ranked are F1, F10, F2, F5, F3, F11, and F4. The marginal uncertainty in the classification verdict is due to the negative attributions of F12, F8, F2, F3, and F4 which prefer labelling the case differently. In conclusion, we can see that F9, F7, F6, F10, and F1 are among the positive variables pushing the classification in favour of C1.',\n",
       " \"C2 is the predicted label assigned to this case or instance. This is based on the fact that there is only a 0.68% chance that C1 is the correct label. The most relevant variables that increase the prediction's probability are F8, F2, F20, and F7. Conversely, F13 is the only important feature driving the classification decision in the direction of C1. Other negative features include F14, F12, F1, and F4. Other positive features increasing the chances of the C2 prediction are F19, F5, and F17. Unlike F8, F2, F20, and F7, these positive variables have moderate contributions to the model's decision. The least ranked among all the relevant features are F21, F3, F10, and F6, with lower attributions to the C2 prediction, however, F11 and F18 are shown to have no impact when determining the correct label for the case under consideration.\",\n",
       " 'The following classification decisions are largely based on the factors or attributes of this particular case. The class label, in this case, is projected to be C1 out of the potential classes, which is 97.50% likely. The next possible label is C2, which has an approximate probability of 2.50%.  The confidence level with respect to this classification is very high, and the features with the most contributions are F3, F6, and F2. However, F9, F8, and F1 are shown to be the least relevant features. The attribution analysis shows that the only positive features whose contributions favour labelling the case as C1 are F6, F2, F7, and F5. However, the negative attributions of F3, F4, F8, F9, and F1 also indicate that perhaps C2 could be the true label. Judging based on the confidence level coupled with the attributions, it can be concluded that the values of the positive features F6, F2, F7, and F5 are good enough to steer the classification in the direction of C1, but the strong negative attribution of F3 casts about 2.50% of doubt on the decision.',\n",
       " \"The prediction probabilities for classes C2 and C1, respectively, are 15.35% and 84.65%. Based on the aforementioned, C1 is the most likely class label for the presented data instance, and according to the attribution analysis, the various input variables had varying degrees of impact on the model's classification judgement. F33, F8, F17, F37, F29, F32, and F4 are the most influential factors, whereas F25, F18, F26, F15, F13, and F3 have the least impact. The subsequent analysis will concentrate on the most relevant factors influencing the label selection in this case. Looking at the attributions of the input features, only F33 and F8 exhibit negative contributions among the top influential features, F33, F8, F17, F37, and F4, lowering the chance that C1 is the right label, and they strongly favour labelling the instance as C2 instead. Positive variables such as F17, F37, and F4 influence the classification choice in favour of C1. The remaining variables, including F29, F32, and F2, have a moderate to low impact. In essence, the marginal uncertainty in this decision is mostly owing to the negative impacts of F33, F8, F31, and F11, while the positive contributions of F17, F37, F2, F29, F32, and F4 push the decision much closer to C1.\",\n",
       " \"The model assigns the class C1 with near perfect certainty or confidence level since the predicted likelihood of C2 is only 1.0%. F63, F90, F49, F64, and F93 have the greatest cumulative beneficial influence on the model's choice to create C1. F67 also had a significant influence, but it shifted the choice away from C1. Furthermore, F45 and F20 had a modest influence on C1 decision making, which was still bigger than features F30 and F50, which had a moderate impact and contributed to C2 class prediction. Furthermore, F20, F48, and F23 have minimal positive impact on the final result, further increasing the chances of C1 being the  appropriate label for the given case. However, a number of input features, notably F38, F65, F72, and F87, appear to be less essential to predictions here. All in all, the very high confidence level could easily be explained away by considering the fact that the joint influence of the positive variables such as F63, F90, F49, F64, and F93 far outshines the joint contribution of the negative variables such as F67, F30, F50, and F3.\",\n",
       " 'Between the two classes, the model labelled this case as C1 with a likelihood of about 97.0% since there is only a marginal chance that it belongs to label C2. The most relevant features influencing this decision are F6, F1, F7, and F12. In this case, F6, F1, and F12 have a considerable positive influence on the prediction of C1. In contrast, the values of F7 and F13 throw a bit of doubt on the C1 prediction. However, compared to F6, F1, and F12, this shift is very small. Finally, there are some attributes with limited impact on the prediction of C1 and these are F5, F3, F11, F2, F9, and F10 since their values are less important to the model in terms of determining the label for this case.',\n",
       " 'Probably C1 is the right label for this case since the probability of the alternative label, C3 and C2, are only 1.03% and 0.0%. The order of importance of the features for the above classification verdict is F1, F4, F2, F6, F5, F7, F3, and F8. Analysis conducted shows that only the features F4, F5, and F7 have negative contributions, hence reducing the probability of assigning label C1 to the given case. Positive features that increase the likelihood that C1 is the valid label are F1, F2, F6, F3, and F8. The co-attribution of the positive variables is stronger than that of the negative ones, so it is not surprising that we see the level of confidence associated with the prediction of class C1.',\n",
       " 'The model labels the given data as C2 since it has a higher predicted probability equal to 51.42% compared to that of C1 which is equal to 48.58%. The input variables with higher contributions to the above classification decision are F21, F4, F9, F30, and F25, while those with little influence are F16, F7, F27, F22, and F19.  Positively supporting the choice of the label, in this case, are mainly F21, F4, F25, and F30. However, the main negative variables are F9, F6, and F1. Judging based on the degree of influence as well as the direction of influence of the variables, it is not surprising that the model is only 51.42% confident in the assigned label which is marginally above average.',\n",
       " 'The following is the classification for the provided data:  C1 is the most likely class label and C2 cannot possibly be the correct label given the likelihood is 0.0%. F12, F38, and F75 are the key variables that contributed to the classification choice. However, the classifier does not consider all features while making this conclusion, and these irrelevant features include F58, F24, F57, and F63. Revealed to have positive contributions to the prediction made here among the top features are  F75, F19, F64, and F23, but all of the others, F12, F38, F61, F72, F7, F26, and F92, argue against labelling the present scenario as C2 and despite the fact that the bulk of relevant features are pointing in the opposite direction, the classifier is extremely certain that the proper label for the current scenario is C1, not C2.',\n",
       " \"The prediction probability of C1 is 17.93% and that of C2 is 82.07%. Therefore, the most probable class for the given case is C2. The above classification assertion statements are based on the information supplied to the classifier about the case given. The top features with significant attributions leading to the decision made above are F6, F26, F32, F42, F43, and F25. Conversely, F38, F24, F14, F28, and F33 are among the features deemed irrelevant to the classification decision here since their contributions are almost negligible and much closer to zero. The attribution analysis suggests that not all the relevant features positively contribute to the classifier's arriving at the verdict here. Those with positive attributions that push the classifier towards generating C2 as the label are F6, F26, F32, F42, F4, F7, F16, and F21. Decreasing the likelihood of the correctness of C2 are the negative features such as F43, F29, F25, F13, F39, F5, F41, and F12, which could be blamed for the little uncertainty in the classification output, as indicated by the prediction probability of C1.\",\n",
       " \"The classification algorithm classifies the given case as C1 with a confidence level equal to 99.99%, suggesting that there is little chance that the C2 label could be the true label. The classification confidence level can be attributed to the influence and contributions of the features F29, F8, F10, F26, and F23. Positively supporting the model's decision are values of F29, F8, F10, and F26. On the contrary, the values of F23, F4, F3, and F7 are shifting the model towards producing the C2 label, which results in a marginal decrease in the certainty associated with the C1 label. The other positively supported features further improving the odds in favour of C1 include F25, F12, F16, and F5. Overall, it is not farfetched to accept that C1 is the correct label for the case under consideration since the strong positive influences of F29, F8, and F10 far outweigh the influence of any of the other input features. In other words, as mentioned above, there is only a small chance that the true label is not C1 considering the attributions of the top influential input features.\",\n",
       " \"The data is marked as C2 by the classifier based on the input features, with a moderate degree of confidence since the prediction probability of the other label, C1, is only 44.0%. The most influential features driving the classification above are F27, F12, F24, F14, F30, F18, F26, F2, F20, F22, F13, F21, F17, F23, F10, F25, F5, F6, and F8. Strongly reducing the chance of C2 being the true label for the given case are the negative features F12 and F27. Actually, these negative features, along with other features such as F30, F18, and F20, are responsible for the uncertainty in the classification decision here. On the contrary, the input features F24, F14, F26, F2, F22, and F13 positively contribute to the classifier's decision to choose C2 as the label here. Finally, it is important to note that not all the features are shown to be relevant when making the labelling decision regarding the case under consideration, and these irrelevant features include F15, F4, F1, and F29.\",\n",
       " \"The model determined that this case belongs to C1 of the three possible labels, with an 83.0% likelihood. It is important to note, however, that there is about a 14.0% chance that it could be C2 and a 3.0% chance that it is rather C3. The most relevant feature driving this prediction is F12, with a very strong positive attribution, increasing the odds of the label C1. The following attributes have values pushing for a different prediction: F11, F4, F7, and F6, however, their attributions are very low when compared to that from F12. Other features positively contributing to the model's decision for this test case are F5, F2, F9, F1, F3, F10, and F8, with F3, F10, and F8 being the least relevant features considered by the model for the given case.\",\n",
       " 'The classifier assigned the label C1, given that there is merely a 2.18% chance that C2 is the correct label. Influencing this classification decision are mainly the values of the variables F5, F12, F11, and F4 which are also commonly referred to as positive variables since they increase the response in favour of the predicted label. Other variables supporting the prediction of C1 are F1, F14, F6, and F8. However, unlike F5, F12, F11, and F4, these variables have a moderate impact on the classifier. The  variables that decrease the likelihood that C1 is the correct label are F3, F2, F13, and F7 since they have values that swing the classification verdict in the direction of C2.',\n",
       " \"Because the chance that C2 is the right label is around 42.17 percent, the example under review is labelled as C1 with a moderate degree of confidence. F4, F1, F7, F15, F6, and F14 have the most influence on the above forecast, whereas F25, F21, F11, F13, F29, F10, and F26 have small contributions. F20, F22, F5, F8, F17, F12, and F24 all have a relatively modest impact. However, the classifier does not take into account all of the attributes while making a judgement in a specific case and the attributes F9, F2, F16, and F23 are all irrelevant features. F4, F1, F7, F6, F21, F29, and F12 are the positive features pushing the prediction in support of the forecasted label. We can see from the attributions map that the bulk of the influential features exhibit negative attributions that reduce the likelihood that C1 is the correct label, justifying the uncertainty associated with the classifier's prediction choice.\",\n",
       " \"All features are shown to have a positive impact on the classification to class C1 or to have no impact at all. F12, F1, F17, and F28 are the four features with the most impact.  Some of the remaining features, in order of feature importance, are F5, F7, F10, F27, F21, F18, F2, F23, F24, F29, F11, and F6. F12 and F1 both have the highest positive impact on the final classification, pushing the classification towards class C1. All of F17, F28, F5, and F7 influence the model's classification to C1. In terms of the features which have a positive impact on the classification, features F10, F27, F21, and F18 are all ranked to have a medium degree of influence on the final classification. F10 and F27 both have a similar importance attribution, which is higher than that of F21 and F18. All the other features not listed above are irrelevant to the decision above and among them are F22, F25, and F30.\",\n",
       " 'Label C2 has a lower probability than label C1, so C1 is the most likely option in this case. C1 has a probability of approximately 96.25 percent, which can be attributed to variables such as F7, F2, F8, and F6. According to the attributions assessment, the least relevant variables are F3, F4, and F9. Inspection of the direction of influence of the features showed that F10 and F1 present negative contributions that push the model somewhat away from producing C1 because they support the label C2. Considering that the combined impact of the negative variables is quite minimal in comparison to the combined impact of the positive variables such as F7, F2, F8, F5, and F6, it is not surprising that the model is very certain that C2 is not the accurate label for the given case.',\n",
       " 'According to the prediction made here, the most likely label for the given case is C2, with a prediction probability of 97.02%, indicating that the prediction probability of C1 is only 2.98%. The classification above is mainly due to the influence of F9, F13, and F5. The next set of features with moderate contributions includes F1, F12, and F6. However, those with little consideration from the classifier are F7, F4, F10, and F15. In consideration of the fact that all the top four features have a strong positive contribution, it is foreseeable why the classifier is relatively confident that the correct label for this case is C2. Additionally, the negative attributes with moderate to low impact are F12, F3, and F16.',\n",
       " 'The classification conclusion is as follows: C1 is the most likely label for this case and the classifier is certain that neither C3 nor C2 are the right labels since their likelihoods are equal to zero. The driving factors for the above classification are F6, F11, and F12, all of which have a substantial positive impact, causing the classifier to select C1. F1, F4, F2, and F9 are also positive features. The assigned label is not supported by all of the input features since the negative features F10, F8, and F3 support the decision that the most likely class for this instance could be any one of the other labels, C2 and C3. Nevertheless, given the confidence level in the aforementioned classification, it is reasonable to assume that the classifier paid little attention to the negative features, resulting in the selection of class C1.  ',\n",
       " 'The classifier states that there is a 50.0% chance that the true label of this test observation is C2. This indicates that the classifier is less certain in its prediction decision regarding the case under consideration. The label assigned is mainly due to the values of the features F9, F6, F4, F1, F2, and F7. The top features, F9 and F6, have very strong positive contributions pushing the prediction higher towards the most probable label. Among the remaining features stated above, F4, F1, F2, and F7, only F7 demonstrates some level of contradiction, forcing the labelling decision in a different direction. Finally, the features with marginal impact on the prediction made here are F5, F8, and F3. While F5 and F3 positively influence the decision made, F8 suggests that the label assigned by the classifier might not be the true label.',\n",
       " 'According to the attribution analysis, the each input variables contributes differently to the decision. For the case under consideration, there are variables that have negative influence on the decision here, but it also has numerous quantifiable variables that are positive. Per the model, C2 is 91.95% certain to be the correct label and C1 has a predicted probability of only 8.05%. The most essential input variables are F1, F7,  F8, and F2, which allow the model to effectively compute the likelihoods across the classes, C2 and C1. F26 and F17 have nearly comparable positive effects, but F15 and F3 have a negative influence, altering the output decision in favour of a different label. The cumulative positive contribution of F26, F1, F2, F4, F19, and F17 was greater than that of F7, F8, F3, and F15, hence the positive variables succeed at improving the predictability odds in favour of the C2 class. Furthermore per the variable attributions, the contributions of F22, F5, F20, and F9 has very little to do with the classification decision since their attributions are negligible and closer to zero than all the above-mentioned variables.',\n",
       " \"Deciding the most probable label for the given case on the basis of the values of the input variables, the classification algorithm's output decision is that:  the probability of C2 being the correct label is 79.78%, the probability of C1 is 20.22%. Therefore, the most likely label is identified as C2 and the attribution analysis shows that all the variables contributed to some extent to the final decision by the algorithm with respect to the given case. The most influential variables are F9, F8, F14, and F7, but F13, F6, and F11 are the least influential ones. The analysis also indicates that F9, F7, F13, and F11 are responsible for the marginal doubt in the classification decision here hence they are commonly referred to as negative variables since their contributions only tend to shift the verdict in a different direction than the assigned label. Finally, the variables such as F8, F14, F4, F12, F2, and F1 are the positive variables that increase the algorithm's response in favour of outputting the C2 label.\",\n",
       " 'The reliability of the classification verdict for this case is 71.57%, implying there is a 28.43% chance that the correct label could be C1. F6 has a significant negative impact on classification output since its contribution contradicts the labelling of the case as C2, hence favours labelling the case as C1. The values F5, F8, F7, F1, F4, F2, and F3 have a positive effect on the results, but still contributes less than the effect of F6. The analysis shows that F6 has an overwhelming negative impact or influence on the predictive decisions of the model here. F8, F7, F1, and F4 have a positive effect on model predictions. Due to the power of the F6 function, all other functions have little effect on the results. In summary, the uncertainty of the predictions can be explained by the control on the model by feature F6, which drags the classification decision favourably towards C1.',\n",
       " \"This model predicted class label C1 with about 93.32% certainty, while there was about a 6.68% chance of the correct class being identified as a different label. Seven features, F30, F5, F33, F10, F11, F9, and F6, have higher impacts on the model prediction decision above. But the feature F30 has the largest positive impact on the result and on the contrary, F33, F11, F9, and F6 show the potential to shift the narrative to a different label since their contributions reduce the likelihood of the predicted label for this case. In addition, features F25, F12, and F15 have moderate impacts on the model's prediction but each of them is increasing the responses, and finally, the features shown have negligible influence include F17, and F3.\",\n",
       " \"Considering the prediction likelihoods, this case is labelled as C2 by the model, that is, the model states that there is about an 83.33% chance that the case is under C2 and about a 16.67% chance that it is not. The most relevant features influencing the decision made here are: F20, F7, F17, and F5. Among the feature set mentioned above, F20 and F7 offer a very strong positive contribution to the prediction of C2. Conversely, F5 suggests the alternative label C1 could be the true label for this case, but this attribution is weak when compared to F7 and F20. Other features that are moderately pushing for this classification decision include F8, F24, F25, and F16. However, the values of F21, F2, F11, and F26 advocate for the assignment of a different label. Finally, the features F6, F4, F14, and F22 have very little impact on the model's prediction for this case.\",\n",
       " \"For the given instance, the model generated the label C1 with a very high predicted probability equal to 99.66% which implies that the model is very confident that C2 is not the correct label. Ranking the contributions of the features to the prediction above, from the most relevant to the least relevant, is as follows: F2, F5, F1, F3, F4, F6, and F7. Among the seven features, only F4 and F6 have negative contributions, pushing the prediction towards the C2 label. However, given that these features have very low contributions, their impact on the model's decision is close to negligible when compared to the contributions of the positive features F2, F5, and F1.\",\n",
       " 'The final prediction given by the model was C1 with almost 100% certainty, showing the model is confident about its decision. F10 had significantly more influence on the prediction than any other feature with F3 and F5 having the next highest attribution values. All the top features, F10, F3, and F5, encouraged the model to output class C1. F2, F12, and F9 are the features that had the least positive impact on the final classification. The features F4, F6, F8, and F11 have moderate impacts, pushing the model slightly away from a C1 classification.',\n",
       " \"Based on the information provided to the classifier, the true label for the given case is likely C1, with a confidence level of 76.26%. Each input variable has a different degree of influence on the classifier's final labelling decision with respect to the case under consideration. Whilst F10, F8, and F7 have lower contributions to the classifier's decision, F16, F15, and F5 are identified as the major contributors resulting in the assignment and classification probabilities across the two classes. There is a 23.74% chance that perhaps C2 is the true label and the features responsible for this are the negative features, F5, F11, F1, F2, F14, F10, and F8. Driving the classifier's decision in favour of C1 are the positive features such as F16, F15, F3, F12, F9, F4, and F6.\",\n",
       " \"According to the model, C3 is the least probable class, while the most probable class for the given case is identified as C2. The top two variables with the greatest control over the model in terms of this case's label assignment are F11 and F4 but on the contrary, the rest of the variables have moderate-to-lower influence. The contribution of F4 is negative, reducing the chances of selecting the label C2. F11, F2, and F1 drive the model to classify the given case as C2. Furthermore, both F7 and F6 have values that increase the predicted probability of C2, but F9 and F3 decrease the model's response in favour of any of the remaining classes. When choosing a label in this instance, the model pays little attention to the respective values of F8, F12, F5, and F10 hence they are the least ranked features.\",\n",
       " \"Judging by the prediction probabilities, the most probable or likely class assigned by the classifier is C1, with the associated confidence level of 90.97%. The features with the most influence on the prediction above include F9, F13, and F14, while the least important features are F10, F3, and F6. Beside some of the features are shown to negatively contribute to the prediction made here and these negative features, F5, F13, F2, F4, and F10, reduce the classifier's response to generating label C1, consequently pushing the verdict towards C2. The joint impact of the negatives is smaller compared to that of positive features such as F9, F14, F8, and F1, hence the greater drive on the classifier to assign C1 as the correct label.\",\n",
       " 'According to the model employed, the label for the case is more likely to be C1. This assessment decision is mainly based on the inpacts of features such as F2, F7, F4, F3, and F8. Among these top features, F2, F7, and F4 have positive contributions to the prediction above, while F8 and F3 are identified as negative features which decreases the likelihood associated with class C1 for this case. Furthermore, the values of F10, F9, F5, and F6 also indicate that the other label, C2, may be the correct label but luckily, the influence of the above-mentioned negative features can be classified as only moderate when compared to F2, F7, and F4. In conclusion, with such a strong positive influence from F2, F7, F4, and F1, it is safe to say that the model is very accurate in its classification judgments, with 100.0% certainty.',\n",
       " \"From the prediction likelihood of each class label, the most probable label for the given case based on the values of its features is C2. The likelihood of C1 is negligible, hence we can conclude that the classifier is very confident that C2 is the correct label.  Analysing the attributions of the input features showed that the most relevant feature with a strong influence on the classifier's decision here is F4. However, the classifier likely disregards the values of the irrelevant features, F6 and F9, when arriving at the classification above.  The confidence level of the classifier employed to make the classification decision above is higher, mainly because the majority of the influential features have positive contributions. Positive features such as F4, F1, and F8 increase the classifier's response higher in favour of C2. F3 and F5 are the main negative features, but compared to F4, their influence on the above classification is very small.\",\n",
       " \"The correct label for the given data instance, according to the machine learning algorithm, is C2 and this is mainly because the probability that C1 is the right label is only about 3.08%.  From the analysis, the ranking of the input features based on their respective degree of influence is F3, F11, F12, F5, F2, F10, F8, F9, F6, F7, F4, and F1. This implies the most relevant features are F3, and F11 whereas F4 and F1 are the least relevant ones.  Given that F6, F7, and F4 are the features that have a negative impact on the algorithm's selection in this case, it's no wonder that it's quite confident in the chosen class. The arguement towards labelling the case as C2 is also supported by the fact that the joint negative contributions of F6, F7, and F4 is very small when compared to that of the top positive features F3, F11, F12, F5, and F2.\",\n",
       " 'Based on the values of the input variables, the prediction model labels the case given as C2 with very high certainty. Specifically, there is only about a 5.59% possibility that C1 is the correct label according to the model. The most influential factors leading to the above prediction decision are the values of F10, F2,  and F14 whereas F9, F3, and F6 are deemed less relevant by the model. In between the two ends (most influential and least influential) are the features such as F15, F8, and F11 with moderate contributions. According to the attribution investigation performed, F10, F8, F11, F4, F5, and F6 have positive contributions, increasing the model\\'s response to favour labelling the case as \"C2\". Conversely, features such as F2, F14, F12, and F15 provide negative contributions, resulting in a small shift toward selecting C1 as the correct class. In conclusion, given that the prediction likelihood of C1 is only 5.59%, it is obvious that the positive features outweigh the negative ones in terms of the considerations they receive from the model, hence the model\\'s decision to assign the C2 label.',\n",
       " 'The data under consideration is labelled as C1 since it is the most probable label, with a prediction likelihood equal to 99.97% therefore classifier employed here is very confident that C2 is not the right label. The top features with the greatest influence on the classifier in terms of the above classification are F13, F2, F19, and F20. Conversely, the values of F1 and F12 have inconsiderable or insignificant influence on the decision made by the classifier. The input features with moderate to low influence but higher than F1 and F12 on the classifier include F10, F14, F3, and F15. The analysis also shows that the majority of the input features have positive attributions, explaining the level of confidence of the classifier as demonstrated by the prediction probabilities across the classes. The positive features increasing the odds of being labelled C1 include F13, F2, F19, F10, F14, F3, and F17. The marginal doubt in the prediction made here could be attributed to the influence of negative features such as F20, F15, F11, and F9. The negative features support classifying the given data as C2, but since their collective influence is smaller compared to that of the positives, the classifier is shifted more towards labelling the data as C1.',\n",
       " 'C1 was the predicted category for the given case and the classifier is shown to be very certain about the above prediction verdict, given that the probability of C1 being the label is about 99.72%. The following five features all contributed positively towards the prediction of the C1 class with increasing levels of impact: F4, F5, F10, F6, and F9. F7 and F2 both had similar levels of impact on the prediction of C1, with F7 having a marginally stronger impact. F7 contributed towards the prediction of C1, while F2 contributed against it, in favour of an alternative label. F3 and F1 are the least relevant features, with very little impact, both with negative attributions, driving the prediction decision or verdict away from C1. From the analysis, only the features, F3, F1, and F2, are shown to have negative attributions, shifting the prediction away from C1. However, the collective attribution of F3, F1, and F2 is very low when compared to that of the positive features, so the classifier is motivated strongly by the positive features, leading to the prediction decision above for the case under consideration.',\n",
       " \"The ML model or algorithm employed here predicted the class C2 with 100.0% confidence level, clearly implying that the case belongs under the class C2 and not C1 since its associated likelihood is 0.0%. Analysis of the contributions of the features indicated that only features F4 and F7 have negative influence, shifting the classification decision away from C2. However, these features are shown to be the least significant ones when it comes to assigning a label to the case under consideration. Therefore, it is a little surprising to see that the model's confidence level is very high with respect to the prediction made here. Among the remaining positive features, F3, and F2, have the strongest impact or influence, increasing the odds of C2 being the label for the case under consideration and the least positive features are F1, F5, and F6.\",\n",
       " 'Judging from the values of the input variables, the label predicted for the case under consideration is C2 with a high confidence level of 98.89%, implying that the probability of C1 being the actual label is just 1.11%. The attribution analysis suggests that F11, F10, and F3 are the most impactful features controlling the label selection. In contrast, F8, F13, and F1 are the least important variables whose values contribute marginally to the label selection. While the variables F11, F13, F6, and F9 contribute towards labelling the given case as C1, the remaining variables such as F10, F3, and F15 strongly support the C2 selection. The variables supporting the assignment of C2 are the positive variables whereas negative variables are those shifting the decision in favour of C1 and are against the C2 labelling decision.',\n",
       " 'There is little to no doubt that C3, among the three classes, is the proper label for this example since its associated predicted probability is 100.0%. F5, F12, and F4 are the variables with the most influence on the labelling output produced here. Furthermore, these variables have a stronger positive influence on the C3 prediction. Similarly, F8, F9, F6, F7, and F10 are some of the variables favouring the selection of C3 as the correct label. F2, F3, and F1, on the other hand, have a negative and opposing impact on the model, increasing the odds in favour of the other labels. When compared to F4, F5, and F12, all of these negative variables have a moderately low impact on the prediction given here. Finally, the lowest ranked essential input variable is recognised as F11, with a very low positive attribution.',\n",
       " 'Here, the model assigned C1 the highest probability, equal to 99.48%, implying that the predictability of C2 is only 0.52%. Per the attribution analysis, only F1 and F9 have negative contributions that decrease the likelihood of the C1 label in favour of the C2 label. F12, F10, F8, and F2 have the highest positive contributions that improve the odds in favour of the C1. The contributions of the other positive features, such as F4, F6, and F7, have moderate contributions, whilst F3, F11, and F5 are the lowest ranked positive features. All in all, the model is very certain that C2 is not the true label, and this highlighted by the fact that the joint negative contribution of F9 and F1 is only marginal when compared with the very strong influence of positive features such as F12, F8, and F2.',\n",
       " 'The assigned label or class by the prediction algorithm is C2, which happens to be the most probable class predicted with a probability of around 56.0%, consequently, there is a 44.0% chance that perhaps C1 could be the true label instead. The classification assertion above is attributed to the contributions of mainly F23, F1, F18, F12, F30, F10, F11, F16, F29, F24, F8, F15, F7, F28, F9, F17, F14, F6, F26, and F25. However, not all of the features are considered relevant when determining the correct label for the given case. F5, F4, F27, and F2 are examples of irrelevant features. Among the influential features, F23 and F1 are regarded as the most negative, dragging the verdict in a different direction, while the top features, F18 and F12, have positive contributions, increasing the likelihood that C1 is the right label here. Actually, the reason for the 44.0% prediction likelihood of C1 can be attributed to the strong negative influence of F23 and F1. The other negative features include F30, F10, and F29, while the other positive features are F11, F16, F24, and F8.',\n",
       " \"For the given case, the model predicts C1 as the label. The probability that the label could be the alternative class, C2, is only about 1.94% which implies that the model is very confident in this classification decision or output. F5 and F1 are the top features pushing for the C1 prediction for this case. Other features with a positive impact on this prediction include F22, F6, F12, F29, and F11. On the other hand, the values of F17, F35, F15, and F14 make up the set of features with negative attributions on the prediction decision above. However, compared to F9, F22, F6, and F1, the features above have a very marginal influence on the model. This might explain why the model is highly confident that the true label is likely C1. Finally, there were some features with insignificant impact on the model's prediction decision for the case under consideration and these include F21, F36, F42, and F13.\",\n",
       " 'The predicted likelihood of C1 based on the information supplied to the model is 51.62%, whereas there is a 48.38% likelihood that C2 is the correct label. The uncertainty of the model in terms of this case or instance can be attributed mainly to the direction of influence of the variables F5, F2, and F8. Decreasing the chances of C1 being the correct label are the variables F5, F8, F7, and F4. While F5, F8, and F7 have strong negative attributions, F4 is the least negative variable. Increasing the likelihood of C1 prediction are mainly the variables F2, F1, and F9. The features F3, F6, and F10 also have a weak positive influence on the classification decision arrived at by the model for this case under consideration.',\n",
       " 'In this case, the classifier indicates that there is a 99.50% chance that the C1 class is the true label, so it is correct to conclude that the classifier is not sure that C2 is the correct label for the case here. According to the study, five input variables contradict the label choice, while four variables support the classification made above. The variables that contradict the prediction are known as negative features while positive features are those that support the classification verdict. F1, F2, F7, F8, and F5 are the negative variables that reduce the likelihood of C1 being the correct label. F3, F6, F9, and F4 are the positive variables that increase the likelihood of C1.',\n",
       " 'Considering the values of features such as F9, F8, and F11, the model is very certain (about 99.65% certain) that C1 is the right label for the given case. While F9, F8, and F11 are the most important features, the model paid little attention to F1, F5, and F7 when deciding on the appropriate label here.Overall, driving down the odds of C1 are the negative features F11, F6, F3, and F5, which are shown to support the other label. However, the very high confidence in the above-mentioned decision is chiefly attributed to the positive contributions of F8, F9, F10, F4, and F2.',\n",
       " 'With a certainty of 100.0%, the model labels this case as C2 and from the predicted likelihoods across the classes, it can be inferred that the model verdict is that there is a zero chance that the case is under C1. The most significant feature is F1, while the least important attributes are F8, F7, and F3. The moderate features are F4, F9, F6, F5, and F2, ranked in order of their respective attributions on the label predicted. With regards to the direction of influence of each feature, some of the input features have positive attributions in favour of the assigned label and increasing the response of the model in favour of the C2 label, while the remaining ones contradict. F1, F6, F5, and F8 are the positive features, while F4, F9, F2, F7, and F3 are the negative ones, shifting the prediction verdict in the direction of C1.',\n",
       " \"The case under consideration can be labelled as either C2 or C1 or C3, and based on values for features such as F1, F6, F12, F4, and F5, the model labelled this test case as C2 with a confidence level equal to 62.29%. However, there is a 28.41% chance that the label could be C1 and a 9.3% chance that it could be C3. All the features used to make the prediction decision have different influences on the model with respect to this test case. That is, while some features positively support the prediction, others have values suggesting any of the alternative labels could be the true label. According to the analysis, F1, F12, F6, and F4 are the top features with the highest impact on the prediction made. The features F1, F12, F4, and F6 are the top attributes positively supporting the prediction of C2. In contrast, F5 and F11 are the features with the most negative attributions, pushing for the prediction of an alternative class. Further decreasing the likelihood of C2 are the features F10, F9, F3, and F8, which all negatively contribute to the model's final decision with respect to the given case. Finally, features F2 and F7 are shown to be less relevant, with positive contributions to the above classification.\",\n",
       " 'Tasked with labelling a given case as either class C1  or class C2 , the model assigns C1 as the most probable true label, with a confidence level of approximately 99.90%. This confidence level suggests that the probability of C2 being the correct label is only 0.10%. Attribution analysis conducted indicates that all the variables have a different degree of influence or contribution to the model arriving at the above mentioned classification verdict. The features responsible for the very high certainty of the model with respect to the case under consideration are F2, F6, F3, and F1. Actually, the only input variables with a negative contribution also happen to be the least relevant variables, F4 and F7.',\n",
       " \"Even though there is moderately high confidence in the assigned label, the prediction probabilities across the two classes indicate that C1 could be the correct label for this data instance.  The variables with primary contributions resulting in the labelling decision above are F3, F2, F5, and F4. As per the attribution analysis, the top two variables, F3 and F2, have a negative impact, influencing the classifier to label the given data as C1 instead of C2. The only other negative variable is F6, with moderate influence compared to the other two negative variables. On the other hand, there are many variables, specifically F5, F4, F8, F1, F7, and F9, that positively support and influence the classifier to assign C2. To a greater degree, the level of uncertainty with respect to this classification instance could be explained away by just looking at the negative variables' fairly strong pull on the classifier towards C1.\",\n",
       " 'According to the classification model employed here, there is a marginal chance that the true label for this test example is C1. Undoubtedly, the model estimated that the likelihood of the true label being equal to C2 is 99.92%. The above prediction decision is based on the influence of features such as F3, F1, F2, F5, and F7. All these features have significant positive support for the prediction decision here, with the top features being F1 and F2. Furthermore, the features with a moderate influence on the prediction of C2 are F6, F8, F9, and F4. While F4 positively supports labelling the case under consideration as C2, the features F6, F8, and F9 indicate otherwise. Finally, the features with marginal impact are F10, F12, and F11.',\n",
       " 'Considering the values of the features, the prediction from the model for the case under consideration is C2 and this labelling decision is not 100% certain given that there is a 27.27% probability that it could be C1. For the case under consideration, the assigned label is mainly due to the values of the features F3, F9, F7, and F2 while the least important is F1. The direction of the contributions of the relevant features is summarised in the following sentences: F3 and F9 have a very strong joint positive contribution in favour of class C2 coupled with moderately positive input features F7, F2, and F4, however unlike them, F1 has a very low positive impact on the model for the case here. All of F6, F10, F8, and F5 have a negative impact on the prediction made here, however, their pull is not enough to shift the prediction in the direction of the other class label, C1.',\n",
       " \"For this case, the classification model's confidence is only about 69.40%, implying that the likelihood of label C2 is about 30.60%. According to the classification attribution analysis, F1 and F2 are the most relevant features, whereas F4 and F6 are the least influential. When the attributions of the features were carefully analysed, only F8, F5, and F7 are identified as negative features since their contributions drive down the prediction likelihood of the assigned label, C1. Conversely, F1, F2, F3, F4, and F6 have a positive influence on the model in support of labelling the given case as C1 instead of C2.\",\n",
       " 'The model prediction for the test case is C2 and the confidence level of this prediction decision is 91.36%, while the predicted probability of C1 is only 8.64%. According to the attribution analysis, we can see that the features F10 and F8 have negative attributions, pushing the prediction decision towards the alternative label, C1. Conversely, the F9, F13, F1, and F2 have values with a positive impact, shifting the classification decision towards label C2. Furthermore, while the attributes F14 and F15 contradict the prediction made, F6 and F12 have values that support the prediction from the model for the test case under consideration. Finally, F7, F3, F5, and F4 are the least ranked features, and among them, only F4 has a negative influence that contributes marginally to the shift away from labelling the case as C2.',\n",
       " 'In summary, the model predicted an 87.14% likelihood of the class label C2 for the test example under consideration, therefore, there is a chance of about 12.86% that the correct class label could be a different label. The features with the highest impact on the model are F7, F3, F4, and F10, whose values are attributing most to the labeling decision here and among these features, only F10 shows the potential to shift the narrative toward a different label. On impact comparison, features F7, F3, F4 and F10 have higher impact on the model prediction than F5 and F9. Features F7, F3, F4, F5, and F9 show a positive impact shifting towards the prediction of C2. F10 is the most negative of all the set of features passed to the model, F1, F8, and F6 have moderate negative influence, whereas the feature F11 has very little negative impact on the prediction.',\n",
       " \"Mainly based on the values of the features F23, F13, F24, and F30, the model classifies the given case as C1 with a prediction confidence level of 90.15%. This means that there is only a 9.85% chance that the correct label could be C2. The features that positively contribute to the prediction include F23, F30, F43, and F18, since their influences increase the model's response in favour of assigning the label C1. On the flip side, features dragging the final decision higher towards C2 include F13, F24, F27, and F21, since their values contradict the assigned label here. Finally, the prediction was made with less emphasis on the values of features such as F4, F26, F7, and F25, given that they are shown to have very close to zero influence.\",\n",
       " '90.58% it the predicted chance that C2 is the correct label for the given case, indicating that the predicted probability of C1 is only 9.42%. Per the feature-attributions, the top-ranked features are F12, F7, and F6, whereas the smallest important or least ranked features are F3, F11, F10, and F9. The influence of intermediate input features like F4, F5, and F2 is considered moderate. The features with positive contributions to the classification above are F6, F5, F3, and F11, while on the other hand, all the remaining features are shown to negatively contribute to the decision above. The main negative features that decrease the probability that C2 is the true label, considering the likelihood of label C1 for this case, are F12, F7, and F4.',\n",
       " \"The most likely label for the given scenario, according to this prediction, is C1, which has a prediction probability of 97.02 percent, whereas C2 has a prediction probability of just 2.98 percent. The impact of F14, F7, and F1 is mostly responsible for the aforementioned classification. F13, F12, and F17 are the following groups of features with moderate contributions. F16, F9, F2, and F8, on the other hand, receive minimal attention from the classifier. Given that all four top features have a substantial positive contribution, it's easy to see why the classifier is quite certain that C1 is the correct label in this case. F12, F11, and F6 are also negative features, having a moderate to low influence.\",\n",
       " 'According to the prediction algorithm employed here, the most probable label for the given data instance is C2. The confidence level associated with the prediction decision above is 64.62%, meaning there is about a 35.38% likelihood that C1 is the right choice.  The input features can be ranked according to their respective degrees of influence in decreasing order as follows: F4, F3, F1, F8, F2, F9, F5, F7, and F6. Therefore, when classifying the given case, the algorithm places little emphasis or consideration on the values of F3 and F4,  however, the values of F7 and F6 are the most important here.  F6, F9, F5, and F1 are regarded as negative features since their contributions decrease the likelihood of C2 being the correct label. However, positive features such as F7, F8, and F2 drive the algorithm higher towards assigning C2 to the case under consideration here.',\n",
       " 'The true label has a 50.0% chance of being one of the two classes and based on the predicted likelihoods mentioned above, it can be concluded that the model is very unsure about the correctness of the classification. The above prediction decisions are mainly influenced by the features F2, F3, F4, F1, F5, and F7, while the least important are F8, F6, and F9. Overall, since the predicted likelihood is evenly split between the two classes, it can be concluded that the model is very uncertain as to which label is the right one. The variables with contributions that support the assignment of C2 include F2, F5, F6, and F9, but on the other hand, the ones with contributions towards the assignment of C1 are F3, F7, F4, F1, and F8. With respect to the assignment of the C2 label, F3, F7, F4, F1, and F8 are the negative variables, while F2, F5, F6, and F9 are the positive variables.',\n",
       " 'The model is assigned the label C1 for the given example. F10, F2, and F4 are the most important features that influence the above-mentioned estimate decision, however unlike them, F6, F1, and F11 are less important. The majority of features have values that swing the judgement towards the other label, C2. The only input features that increase the likelihood that C1 is the correct label are F10, F5, and F1, therefore it is very surprising that the model has 100.0% confidence in its estimate for the given example.',\n",
       " 'Since the probability that C2 is the correct label is only 2.18%, the classifier assigns the label C1 in this labelling instance. The main factors influencing this classification decision are the values of the variables F4, F11, F9, and F7. From inspecting the direction of influence of the above-mentioned variables, they can be referred to as the positively contributing variables because they increase the response of the classifier, increasing the odds in favour of the assigned label, C1. Other positive variables that support the prediction of C1 are F2, F10, F6, and F8, however, unlike the top positive ones, these variables have only moderate control on the classifier. Just four of all the input variables are shown to reduce the probability that C1 is the correct label and these variables are F3, F1, F12, and F5 since their respective values cause the classification judgement to shift in the direction of C2. In summary, given that the confidence level in the C1 prediction is 97.82%, it is obvious that the negative contributions of F3, F1, F12, and F5 result in only a marginal decrease in the certainty or confidence level.',\n",
       " 'The selected case is labelled as C2 with close to an 85.0% confidence level, hinting that there is a smaller chance that it could be C1. The most important variables when determining the label for this case are F15, F13, F42, and F21. The variables with moderate influence include F10, F25, F11, and F27. However, the last three ranked variables according to their respective impacts on the model for the case under consideration are F38, F41, and F35. Significantly increasing the odds of the predicted label are the variables F15 and F42. Conversely, the F13 has the strongest impact, driving the classification verdict towards C1. Other features with similar direction of influence as F13 are F17, F11, F30, F12, and F16.',\n",
       " 'The probable label for the given case is C1 since its associated predicted probability is 91.85% compared to the 8.15% of C2. The input variables mostly responsible for the above prediction verdict are F11, F10, and F6, however, the values of F8, F14, and F3 are deemed less relevant by the model in this case. The attributions of the input variables can be either positive or negative, depending on the direction of influence on the model. Among the variables, the ones with negative attributions that decrease the probability that C1 is the correct label are F5, F4, F1, F8, F14, and F3. On the contrary, F11, F10, F6, F9, F7, F13, and F12 are some of the remaining variables that increase the likelihood of C1 being the correct label. Based on the attributions of the variables, we can conclude that the collective impact of the negative variables is not strong enough to shift the prediction verdict away from C1, resulting in only a marginal uncertainty in the assigned label.',\n",
       " \"The classification verdict of the model for the case under consideration has a 50.10% chance of being C1. But based on the estimated likelihoods indicated above, it is possible to deduce that the model is extremely doubtful about the classification's validity. The following variables have the most attributions to the aforementioned prediction decisions: F1, F6, F2, and F9. F8 and F5 are the least important, whereas the values of the variables F4, F7, and F3 had only a moderate impact. Regarding the direction of influence of the variables, F1, F7, F8, and F5 are the ones driving the classification higher towards the C1 label and away from C2. However, factoring the likelihood of the C2 label, the negative variables, F6, F9, F2, F4, and F3, successfully cast doubt on the validity of the assigned label. In simple terms, the negative contributions from F6, F9, and F2 can easily explain the uncertainty associated with the class label assignment for the case under consideration here.\",\n",
       " 'The algorithm classified the given data as C2 with close to 99.32% certainty since the prediction likelihood of C1 is only 0.68%.  The abovementioned prediction verdict is largely due to the influence of F10, F2, and F1 while the other influential features include F7, F3, and F4. However, F5, F8, F9, and F6 are shown to have smaller contributions to the decision made here.  Not all the features have positive contributions, and F1, F3, and F4 are known as negative features since for the given case, they reduce the likelihood of the assigned label and hence they favour or support labelling the case as C1 instead.',\n",
       " \"The model is quite certain that C1 is the most likely class for the current scenario. C1 has a 90.48% chance of being correct, implying that any of the other labels is highly unlikely. F10 and F4 are the most relevant variables influencing the abovementioned classification decision but all other factors or variables are proven to have a moderate or minor influence. Fortunately, the top variables, F4 and F10, have an impact on the model that is positive, boosting the chance of C1. Furthermore, whereas F1 and F5 force the model to forecast C1, the variables F8, F11, F9, and F6 are forcing the model to assign a different label. Finally, several variables have a very minor influence on the model's final forecast here, but F12, F9, and F6 are shown to have the least contributions.\",\n",
       " 'The label assigned by the model is C2 with a higher predicted confidence level of 99.99%, meaning the probability of C1 being the correct label is virtually equal to zero. The classification decision above is mainly due to the influence of the features F5, F3, F8, and F4, however, the remaining features have very marginal contributions to the decision. Among the features, only F1 and F6 are shown to have a negative impact, reducing the likelihood of the assigned label. However, this negative influence is very weak compared to that of the top positive features, F5, F3, F8, and F4.',\n",
       " \"With respect to the given case, the classification algorithm employed here generates C1 as the most probable class since the probability of C2 is 41.63% while that of C1 is 58.37%. F1, F7, and F4 are the most influential features resulting in the classification decision mentioned above, whereas the least relevant features are F2 and F5. As indicated by the prediction probabilities across the classes, the confidence in the labelling decision here is not perfect, which can be attributed to the influence of the negative features F1, F7, F4, and F8. On the other hand, the moderate positive influence of F3, F6, F9, F2, and F5 explains the algorithm's decision to label the case as C1 with such an average level of confidence.\",\n",
       " 'The most likely label for the provided data instance, according to the predictive algorithm used here, is C1. The confidence level associated with the above prediction decision is 64.62 percent, which means C2 has a 35.38 percent chance of being correct. The following input features can be prioritised in decreasing order according to their relative degrees of influence: F3, F7, F2, F6, F4, F1, F8, F9, and F5. As a result, the algorithm places little emphasis or attention on the values of F7 and F3 when classifying the given case whilst the values of F9, F8, and F5 are the most relevant. Regarding the direction of influence or impact of the input features, F5, F1, F8, and F2 are considered negative features because their contributions reduce the likelihood of C1 being the correct label. Positive features such as F4, F6, and F9, however, push the algorithm closer to assigning C1 to the situation in question.',\n",
       " 'C1 is the label predicted by the classifier for the case or example under consideration the confidence in the above prediction is about 96.35%. It is important to take into consideration, however, that there is also a very small chance equal to 3.65% that the correct label could be C2. The ranking of the features according to their respective contributions to the decision above is as follows: The top features with significant influences are F9, F19, and F1. The remaining features with moderate contributions are: F8, F15, F13, F18, F4, F6, F12, F20, F7, F10, F5, F2, and F16. Finally, the values of F14, F17, F11, and F3 are shown to have a very low impact on the prediction of C1 for the case under consideration. The assessment below only considers the features shown to have the most relevant impact in terms of the direction of the prediction here. Among the most contributing features, only F8 and F15 have a negative influence, while the remaining ones, F9, F19, F1, and F13, are shown to have positive contributions to the prediction for the case. Looking at the cumulative influences of each set of positive and negative features, it is not strange that the label assigned is C1 with a confidence level of 96.35%.',\n",
       " \"The model predicted the C1 class with very high confidence of 93.27%, hence we can conclude that there is only a 6.73% chance that the true label is C2. Two features have a very strong positive influence on the prediction of the C1 class and they are F7 and F3. The following features have a medium impact and are listed in decreasing order of influence: F5 and F12 have a negative influence, while F11 and F1 have a positive influence on the prediction of C1. F1, F8, and F14 have a positive influence on the prediction of the C1 class, while F4, F9, F13, and F2 influence the prediction negatively. Those with the least contribution regarding the model's decision for this case are shown to be  F10, F16, F14, and F6. Among these least contributing features F10 and F6 are shown to have negative contributions whereas F14 and F16 contribute positively in favour of the assigned label.\",\n",
       " 'This case or instance is labelled as C1 with a very high confidence level, however, the classifier estimates that C2 could be the correct label with a prediction likelihood of about 5.75%. The values F9, F3, and F1 played a major role in the aforementioned labelling choice and because F4 and F8 have minimal attributions, they are the lowest rated features. F3 and F9 have values, which increases the probability that C1 is the correct label. Other variables that drive the clasifier towards assigning the predicted class are F2 and F6. Here, F3, F6, F9, and F2, are referred to as positive input variables since their contributions are towards the generated C1 label. In contrast, the remaining six variables are shown to have a negative influence on the classifier, indicating that the correct label could be C2 instead of the C1 selected by the classifier and the strongest negative variables are F1, F7, and F10.',\n",
       " 'The prediction likelihood of class C1 is 73.85%, making it the most probable label for the given case. When making the above prediction, the input features are shown to have some degree of influence on the decision made by the classifier. While features such as F16, F25, and F19 have very low contributions to the classification, the features F22 and F26 are shown to be the main contributors to the decision. Finally, the features with moderate contributions are 21, F11, 42, F18, F12, and F10. As indicated by the prediction likelihoods across the classes, the classifier is shown to have a little doubt in the correctness or validity of C1, and the main features resulting in this little uncertainty are the negative features F22, F13, F10, F12, F5, F6, and F24. However, the values of F26, F11, F14, F18, F4, and F15 suggest that C1 is very likely the true label.',\n",
       " \"According to the model, the probability of C2 is 12.35% and that of C1 is 87.65% meaning C1 is the most probable label for the given case. The variables with the majority influence on the abovementioned decision are F25, F3, F12, F8, F20, and F21 whereas variables F16, F23, F4, F9, F6, and F26 are shown to have little to no influence on the model's decision with respect to the given case. The contributions and influence of variables such as F19, F1, F24, and F17 can be described as moderate.  Among the variables controlling the prediction decision here, F25, F20, F1, F17, F15, F7, F22, and F14 are the negative variables decreasing the model's response to the output of the label C1. Conversely, the highly influential variables F3, F12, F8, and F21 are the main drivers that contribute positively, increasing the probability of C1 being the correct class label. Overall, given that the variable with the highest influence on the model is F25, a negative variable, it is not unexpected that there is a little doubt in the classification decision here, as shown by the prediction probabilities across the classes.\",\n",
       " 'With a moderately high level of confidence, C1 is assigned to the given case by the classifier and this is due to the fact that the other classes, C3 and C2, have likelihoods of 3.0% and 14.0%, respectively. Across the input features, only F11, F3, F9, and F12 are shown to contribute negatively, shifting the classification away from C1 and towards C3 and C2. On the contrary, the features such as F5, F6, F4, and F2 are among the positive set of features that drive the verdict in support of assigning C1 to the given case. From the attributions of the different features, F5 is the most relevant contributor to the classification made here, while F8, F10, and F7 are ranked as the least influential features and considering the direction of influence of each input feature, it is understandable why the classifier is certain about the decision made.',\n",
       " \"Considering the values of the input variables, the classification model is very confident that the most probable label is not C2 but C1. The top input variables receiving much consideration from the model to arrive at the classification verdict are F38, F28, F31, F15, and F22. Among these most influential variables, F38 and F28 are regarded as negatives since their contributions serve to swing the classification decision in the opposite direction. On the contrary, F31, F15, and F22 have a positive influence, increasing the model's response to favour labelling the given case as C1. Other positive variables include F17, F14, and F13, whereas the other negative ones include F26, F9, and F6. Input variables such as F7, F29, F21, and F36 are shown to have zero attributions, that is, their values are not paid enough attention to influence the model's decision with respect to the given case.\",\n",
       " \"The odds are in favour of label C1 given that the probability of it being the correct label for the case under consideration is 81.32%. However, the likelihood of label C2 is 18.68%. The classification decision above is mainly due to the values of F8, F12, F9, and F5. The feature with the least attribution to the model's output label here is F6. The features F8, F9, and F12 have very strong positive contributions to the prediction, increasing the odds of the label C1. Other features with positive attribution in support of C1 are F7, F4, and F11. Unlike the features stated above, the remaining features, F5, F10, F2, F1, F3, and F6, have values that shift the final prediction verdict in the direction of C2 and account for its 18.68% likelihood.\",\n",
       " 'The model predicted C2 with a high probability equal to 88.70%, whereas C1 has only a 11.30% likelihood of being the true label. Considering the predicted likelihood of C1, there is only little confidence in its correctness as the true label for the case here. The value of F4 has a large negative influence on the C2 classification decision, while F8 is the top positive feature. F1, F5, F3, and F2 all have positive impacts on the C2 prediction, with F1 and F8 having the highest influence, F3 and F2 having low influence, and F5 being somewhere in the middle. Broadly speaking, the negative influences of F4, F6, and F7 only succeed in driving the decision slightly away from C2 towards the other label as shown by the predicted probabilities across the classes.',\n",
       " \"The algorithm's predicted output label for the given case is C2 with a very strong confidence level equal to 100.0%; hence C1 can't be the true label. Among the features, the most relevant ones are F3, F6, and F8 with very significant impact, pushing the prediction decision away from C1 towards C2. The next set of attributes, F9, F2, and F5, offer a moderate shift towards C2 coupled with marginal positive contribution from F4 and F1. From the above statements, all the features are shown to support the label assignment decision in the case under consideration. Consequently, it is no wonder that the algorithm has 100.0% confidence in the output decision or verdict above.\",\n",
       " \"In this case, the prediction algorithm is not 100.0% certain that the correct label for the given case is C2, since there is a 43.49% chance that the right label could be C1 instead. The algorithm's decision to label the case as C2 mainly stems from the influence of features such as F1, F12, F15, F3, and F14. On the other hand, little consideration is paid to the values of the least ranked features, F9, F6, and F7. Within the top-ranked features, F14 and F3 have a negative impact, increasing the prediction probability of label C1. Further decreasing the likelihood of the C2 class are the negative features are F2, F11, F16, and F7. However, all the remaining features strongly or moderately push for the classification output to be C2 and the notable positive features are F1, F12, and F15. Considering all the features' attributions, the uncertainty or doubt in the classification could be attributed to the algorithm's paying too much attention to the values of the negative features.\",\n",
       " \"Mainly based on the information on the case given, the classifier's output decision is as follows: C1 is the most probable label, followed by C3 and C2, with C2 being the least. To be specific, the prediction probabilities across the classes are as follows: C2 has 4.34%, C3 has 21.64%, and C1 has 74.0% chance of being the true label. The moderately high classification confidence is largely due to the impact of F3, F1, and F9. However, the values of F4 and F2 received very little consideration when the classifier was picking the most probable label for the given case. With respect to the direction of influence of the features, F3, F1, F10, F6, and F2 have varying degrees of positive contributions, driving the classifier to label the case as C1. On the contrary, F9, F12, F8, and F11 are among the negative features, shifting the classification decision in a direction away from C1.\",\n",
       " \"The model predicted C1 for the case under consideration which a predicted likelihood of 67.95% whereas, that of C2 is 32.05%. The top influencing features ordered from highest to lowest, are F9, F8, F6 and F1, and among them only F6 is shown to have positive attribution in support of the model's decision. F5, F10, and F3 have a smaller positive influence on the prediction, while F4 has an even smaller negative impact. F7 is the least relevant feature, and hence its negative attribution has very little influence on the model for this case.\",\n",
       " \"The case under consideration is labelled as C1 by the model employed for this classification problem. However, according to the model, there is a 45.34% chance that C2 could be the label, presenting some level of uncertainty in the classification verdict made here. F7, F15, F8, F16, F2, and F1 are the top features identified as very important to the model's decision, whereas those with negligible contributions include F33, F10, F25, F13, and F4. Across the input features, those with a negative influence that motivates the classification output to be C2 are mainly F15, F5, F16, and F21. The other negative features contributing to the predicted likelihood of C2 are F9, F17, F26, and F20. Contradicting all the negative features and motivating the prediction output to be C1 are the positive features F7, F8, F1, F2, F28, F19, and F23. In conclusion, it is very surprising to see that the confidence level of the C1 prediction is only 54.66% given the very strong positive contribution of F7, but one can say that the negative features successfully cast doubt on the decision with regards to the case under consideration.\",\n",
       " \"With a certainty level of 82.07 percent, the label choice for the given case is C2 and in a nutshell, the likelihood of C1 having the correct label is only 17.93%. The contributions of features like F30, F33, F38, and F36 are largely responsible for the classification above. The following three, with modest impact, are F23, F28, and F5. However, while choosing the proper label for a given case, the classifier does not consider all of the features. F39, F35, F20, and F14 are notable but insignificant features. F30, F33, F38, and F36 are the top features, with considerable positive contributions supporting the assignment of label C2. F23, F28, F10, and F40 are the top negative features that cause the classification to swing in a different direction. To bring things into perspective, due to the fact that the bulk of important attributes have positive attributions, it's not surprising that the classifier is confident that C2 rather than C1 is the correct label.\",\n",
       " 'C2 was predicted with a high degree of certainty by the model since the likelihood of the alternative class is only 11.30%. The value of F8 has a significant negative impact on the classification choice, whereas F6 has a moderately positive contribution. F4, F3,  F7, and F1 all have a favourable or positive impact on the C2 prediction, with F1 having the most positive impact, F3 and F4 having the least, and F7 being in between. F2 and F5 have a minor negative influence on the class assignment here, which together with F8 contributing to the decrease in the liklihood of C2.',\n",
       " \"With a higher level of certainty, the algorithm labels the given data or case as C2 because the predicted probability of class C2 is 99.93% while that of class C1 is only 0.07%. C1 is therefore less likely than C2 and the classification assertion or decision here is chiefly attributed to the impact of input features such as F16, F6, F39, F35, and F32. Among these relevant features, only F32 has a negative contribution, mildly dragging the verdict in favour of C1, whereas conversely, F16, F6, F39, and F35 have strong positive contributions in support of assigning C2 to the given data. Other features with moderate influence on the algorithm's verdict here include F3, F33, F10, F21, F22, F19, F24, and F4. However, some of the input features  are shown to have negligible contribution to the abovementioned classification output and in fact, these include F1, F27, F30, and F9. In summary, the most vital features with respect to this classification instance are F16, F39, and F6 with positive contributions strongly increasing the algorithm's response towards label C2 hence the 99.93% predicted probability.\",\n",
       " \"There is uncertainty about the correct label for the given example since both labels, C2 and C1 are shown to have a 50.0% chance of being correct. The prediction decision above is mainly attributed to the influence of the input features F1, F8, and F6, while F4, F2, and F5 are deemed less important to the decision above. Looking at the direction of influence of each input feature, only F8, F8, F4, and F5 are shown to have a positive contribution, increasing the model's response towards assigning C2. All the remaining six features have a negative contribution towards the decision here, supporting the assignment of the other class.\",\n",
       " 'The classification model assigned the label C2 to the given example and given that the confidence level is 100.0%, we can be certain that the chances of C1 being the true label are negligible. The most relevant features controlling the prediction decision above are F4, F5, and F3. F9, F6, and F1 are among the least relevant features. Most of the properties have values that sway the decision towards the other C1 class. The only features that increase the odds that C2 is the correct label are F4, F10, and F6. It is strange that the model has 100.0% confidence in its prediction for the selected sample, given that only a small number of the input features contribute positively to reaching the C2 estimate.',\n",
       " \"The model prediction for the test case is C1 and the confidence level of this is almost 100%. From examining the contributions of variables or attributes, the values of F6 and F13 push the prediction verdict in favor of the other label. On the contrary, F10, F14, F4, and F8 have values with a positive influence that biases the classification decision towards label C1. While attributes F5 and F12 contradict the prediction made, F15 and F2 have values that support the model's prediction for the given case.\",\n",
       " 'The label predicted by the classifier is C1 at a 71.80% confidence level. On the other hand, there is a 28.20% chance that C2 could be the label. The prediction can be mainly attributed to contributions from F38, F59, F47, and F27. Considerable positive contributions to the prediction here are from F38, F27, F52, and F59 since their values support the prediction of C1. Shifting the prediction towards C2 are the negative features F47, F16, F75, F13, and F32. There were some features with minuscule influence on prediction decision made for the case under consideration; these include F17, F54, and F6. In simple terms, the classifer deems the values of these features less important when assigning the label here.',\n",
       " \"The model, making a classification decision based on the input variables, predicts the class C2 label for this case with a predicted likelihood equal to 54.21%. It also shows a 45.79% probability that C1 is the correct label. The classification decision made above is primarily influenced by the variables F2, F4, F1, F8, and F10. The three most influential variables, F2, F4, and F8, have a negative impact since their values are shifting the labelling decision in the direction of C1 instead of C2. Positive variables are F10, F1, F11, F7, and F6, supporting the model's class assignment decision for this situation and one can conclude that it is the influence of the positives that motivates the decision towards C2.\",\n",
       " 'Based on the values of the input features, the classifier believes that the most probable label for the given data is C2, due to the fact that there is only a 19.30% chance that it could be C1 instead. The most influential features resulting in the decision or judgement above are F9, F24, F14, F6, F3, F30, and F22, though features such as F12, F31, F19, and F5 are indicated to have negligible contributions to the classification. Actually, the high certainty of the chosen label can be attributed to the very strong positive influence of F9 and the moderate positive influence of F24, F14, F11, and F6. Conversely, the negative features F30, F22, F3, and F18 reduce the likelihood of C2 since their values support labelling the case as C1.',\n",
       " \"According to the classification algorithm with a very high confidence level, the correct label for the given data instance is C2.  This prediction decision is heavily influenced by features such as F1, F6, F2, F10, F13, and F9. Among these top features, the only features with a negative contribution towards the assigned label are F9 and F13. With respect to the given instance, their negative contributions decrease the algorithm's response in favour of the least probable class.  F5, F3, F11, F12, and F8 positively support the assignment of label C2. Conversely, F7 and F3 have a similar direction of influence as F13 and F9.\",\n",
       " 'For the given data instance, the most probable class according to the classifier is C2 since the probability of C1 being the correct label is only about 10.0%.  The most influential features resulting in the prediction decision above are F8, F1, and F7 which are shown to negatively contribute to the decision above since they strongly push the classifier towards assigning a different label.  F5, F9, and F4 are shown to be the only features to positively contribute to the classification made here. Aside from the positive features, all the others negatively reduce the odds of the given data instance having C2 as its label.',\n",
       " 'Judging based on the information about the given case, the model outputs C1 with a prediction probability of 74.72%, however, it is vital to keep in mind that there is also a 25.28% probability that C2 could be the true label. The attribution analysis shows that all the input variables have varying degrees of influence on the model as it arrives at the abovementioned decision and the influence of the features can be ranked from the most relevant to the least relevant as follows: F5, F1, F3, F7, F6, F4, and F2. Across the input features, only F1 and F7 have negative attributions, reducing the likelihood of the predicted label which explain the 25.28% predicted likelihood of the C2 label. Therefore, F5, F3, F6, F4, and F2 are the positive input features pushing the decision higher towards C1 and away from C2.',\n",
       " \"The data is labelled C2 by the model as it has a somewhat greater prediction chance than C1. F6, F5, F10, F13, and F28 are the input variables that have the most impact on the above classification choice, whereas F14, F19, F30, F9, and F12 have the least influence. F6, F5, F28, and F13 are basically supporting the choice of the label in this scenario while on the contrary, F10, F18, and F1 are the primary negative factors. It's not unexpected that the model isn't 100 percent sure of the assigned label considering the degree of influence as well as the direction of influence of the variables.\",\n",
       " 'Although the case under consideration has variables with a significant negative impact, it also has many measurable variables that are positive, so there is a good chance that C1 is correct since it has a 91.95% certainty. F15, F2, and F6 are the most important input variables, thanks to which the model successfully assigns the selected label, C1. F22 and F19 have almost identical positive impacts, while F4 has negative effects, shifting the output decision in favour of a different label. However, the cjoint positive contributions of F22, F15, F6, and F19 was higher than that of F2, F9, F18, and F4, increasing the likelihood of the C1 class. Unfortunately, the values of the variables F17, F26, F5, and  F1 are likely ignored since their attributions are much closer to zero.',\n",
       " 'The classification output observations that follow are based on the information supplied about this specific case. The class label in this case is forecasted to be C3 out of the four possible labels, with a probability of around 83.08 percent. With a probability of 16.87 percent, C2 is the next most likely label. The third possible label, C1, has a 0.05 percent chance of being correct. The algorithm, on the other hand, confirms that C4 is unlikely to be the correct label. According to the attribution analysis, F4, F3, F1, F5, F6, F2 is the ranking of the input features based on how powerful their effect on the algorithm is. Furthermore, among the input variables, F4 and F5 exhibit negative attributions, causing the decision to be shifted away from label C3. Finally, F3, F1, F6, and F2 are the positive variables that sway the judgement in favour of C3.',\n",
       " 'The model predicted class C2 with a very high confidence level of 93.27% and looking at the predicted probabilities across the label, there is only a 6.73% chance that C1 is the true label. There are two features that have a very strong positive effect on the prediction of class C2 and these are F12 and F4. The following features have moderate impact and are listed in descending order of impact: F16 and F9 have a negative impact, while F3 and F8 have a positive impact on the prediction of C2. In addition, both F5 and F10 had a negative effect on the model, further decreasing the odds of C2 being the true label for the given case. Finally, in terms of model decisions for this case, the features with the least contributions are F14, F1, F11, and F6.',\n",
       " 'Between the three possible classes, there is a 100% certainty that the correct label for this case is C3. The features with a very high impact on the prediction made here are F7, F6, and F2, which are also shown to have a very strong positive contribution to the C3 prediction. Other features that shift the prediction in favour of C3 are F10, F11, F3, F12, and F8. On the other hand, F5, F4, and F9 negatively swing the model towards predicting a different label. Compared to F7, F6, and F2, all the negative features have a low to moderate influence on the prediction made here. Finally, F1 has the lowest positive contribution that also further increases the likelihood of the output label, C3.',\n",
       " 'For the given case or instance, the model assigns the label C2, with the prediction confidence equal to 56.56%. The variables F4, F1, F5, and F6 all contribute a lot to the classification decision above. While F4 and F5 are impacting positively, F1 and F6 are decreasing the likelihood of the assigned label. For the remaining features, both F3 and F7 shift the classification towards C2, whereas F2 has a marginal influence on the model, shifting the final verdict away in favour of the alternative label.',\n",
       " 'The prediction made for this case by the model is that C1 is most likely the true label, with a confidence level of 72.03% higher than the 27.97% of the C2 label. According to the input features attribution analysis conducted, the features with the most influence on the decision are F1, F8, F15, and F10, all of which increase the probability that C1 is indeed the true label. The top negatively contributing features, increasing the probability that perhaps the true label could be C2, on the other hand, are F11, F17, F5, and F4. Conversely, F9, F6, F2, and F14 also have positive contributions, further pushing the decision towards labelling the case as C1. Overall, the fairly high confidence in the classification decision here can be attributed to the fact that positive features have a much higher influence on the decision than their negative counterparts.',\n",
       " 'Tasked with labelling cases, the classification model labels the case under consideration as C1 since the probability of C2 is only 20.22%. The predicted probability of the less probable class, C2, reflects the fact that the model is a bit doubtful about the output label. Responsible for this doubt are the negative features F10, F14, F9, and F7 since they support labelling the given case as C2 over C1. On the contrary, F5, F3, F12, F8, F6, F4, F13, and F1 are among the positively contributing features, responsible for the moderately high confidence in the classification output decision.',\n",
       " \"The classification algorithm determines that neither C3 nor C4 nor C2 is a suitable label for the present context. C1 is quite guaranteed to be the correct label. The aforementioned conclusion has a higher degree of confidence due to the positive contributions of F18, F20, and F11. Aside from the above mentioned positive variables, F14, F15, F8, and F10 are also positive. However, their influences are moderate compared to F18, F20, and F11 . The remaining positive variables, F6, F7, F9, and F3, are among the algorithm's least influential input variables. Other attributes, such as F13, F16, F19, and F1, merely serve to reduce the likelihood of C1 being the proper label in the current context. Given the algorithm's high confidence in this classification, one may conclude that the negative variables had minimal impact on the algorithm's label selection here.\",\n",
       " 'The model gave the output label as C1 with a very high probability of 99.69%, leaving only 0.31% chance that C2 could be the right one. According to the contributions or attributions analysis done to understand the properties of various traits, F10 is by far the most influential trait. F8 had a positive impact on model predictions, as did F3. This is in contrast to F6 and F4, which have a negative impact on the model, pushing the classification verdict towards C2. Several input features are shown to have a limited impact on the output label produced by the model and they are: F11, F2, F5, F7, and F1. Overall, only the features F9, F6, F4, F11, F2, and F7 showed negative attributions, reducing the likelihood of the C1 label being assigned by the model but their joint impact was not enough to predispose the model toward a different classification decision.',\n",
       " 'Between the two classes, the given case is assigned the label C2 given that it has the highest predicted probability of about 93.0% since the probability of having C1 as the label is only 7.0%. Analysing the prediction made for the case under consideration, F6, F9, F4, and F10 are the features mainly pushing the prediction higher away from C2, while F5, F8, F3, and F1 improve the odds of the prediction being equal to C2. All things considered, the most relevant feature is F5, by contrast F2 and F11 are the ranked as the least relevant for the label assignment above.',\n",
       " \"The classifier says that C1 has a 67.54 percent chance of being the correct label for the given example or case; consequently the label C2 has a 33.46 percent chance of being the chosen class. The variables F2, F11, F9, and F1 have the most impact on the prediction judgement here. On the other hand, F8, F3, and F5 are seen as less relevant variables when determining the proper class. The variables F1, F10, F3, and F5 lower the probability of the assigned label C1 since they are negative variables favouring the C2 prediction decision. However, the other features' collective or joint attribution is strong enough to favour C1. In summary, F11, F2, and F9 are the most positive variables.\",\n",
       " 'The classification algorithm labels the presented data as C2 with the degree of confidence equal to 81.43 percent, although there is an 18.57 percent possibility that C1 is the correct label. The positive effects and contributions of input variables F8, F12, and F9 are mostly used to assign C2 to a specific scenario. Furthermore, the bulk of the remaining input variables contribute positively, making label C2 even more predictable. The only variables with negative contributions are F11, F3, F10, and F1, which move the choice to C1 rather than C2. Comparing the negative attributions to the positive attributions illustrates why the algorithm is certain that C2 is the correct label here.',\n",
       " 'The probability that the label is C1 is 51.62% and the probability that C2 is the correct label is 48.38%. For this case or example, the uncertainty of the model is mainly due to the direction of influence of the variables F1, F7, and F9. Reducing the chance that C1 is the correct label are variables F1, F9, F3, and F6. While F1, F9, and F3 have a strong negative impact, F6 has the least negative contribution. Per the attribution analysis, increasing the prediction probability of C1 are the variables F7, F10, and F2 which are supported by F5, F8, and F4 all with moderate positive influences on the classification decision made by the model.',\n",
       " \"The classifier assigns the label C2 since the probability associated with C2 is greater than that of C1. For the case under consideration, F1, F7, F11, and F9 are the sets of features significantly influencing the decision made by the classifier. However, features such as F5, F6, and F8 have limited to no impact on the classifier's output decision. F7, F1, and F9 are the features that are positively shifting the verdict toward predicting C2, not C1. In contrast, F11, F10, F6, and F8 have negative attributions, implying that they decrease the likelihood of C2 in favour of C1.\",\n",
       " \"The probability that C2 is the label for the given case is zero and judging by the predicted probability associated with the remaining classes, the classifier is fairly certain that the correct label is C3 given its likelihood of 75.0%. The features are ranked in order of their respective impacts, from most important to least relevant: F10, F11, F9, F7, F12, F3, F6, F4, F8, F2, F1, and F5. Examining the contributions of the input features revealed that the ratio of negative features is smaller than the number of positive features. The negative features, F6, F3, F7, F9, and F4, decrease the classifier's response towards the generated class but the F10 value has the strongest positive contribution, increasing the response of the classifier to support the C3 assignment. Lastly, the least ranked features, F8, F2, F1, and F5, have a weak positive effect on the above prediction outcome, further increasing the odds in favour of label C3.\",\n",
       " \"The most likely label for the given example based on the values of the variables is C1, according to the prediction probability of each class label. It can be concluded that the classifier is quite certain that C1 is the correct label because the probability of C2 is small. According to the attributions of the input variables, the most relevant features with a strong impact on the classifier's decision here are F9, F4, and F5, while on the contrary, the least relevant variables are  F3 and F2. F9, F8, and F5 are positive variables that boost the classifier's response in favour of C1. The primary negative variables are F4 and F6, however they have a little impact on the above classification when compared to F9. Because the majority of the influential features have a positive impact, the confidence level of the classifier used to make the classification decision is high.\",\n",
       " 'The given case is labelled as C2 since it has a prediction probability of 98.33% which implies that C1 is the least probable label. The higher confidence in the assigned label is mainly due to the contributions of input features F12, F15, and F1. In contrast, F8, F10, and F7 are the least ranked features. Based on feature attribution analysis, the top features F12, F15, and F1 have a strong positive influence, increasing the response of the classifier to assigning the label C2. Furthermore, pushing the decision further towards C2 are the other positive features such as F11, F4, F13, and F5. Supporting the prediction of the least probable class are the features F2, F6, F3, F14, and F7. When you compare the joint influence of the negative feature to that of the positive feature, it is evident why the classifier is very certain that C2 is the most probable label.',\n",
       " \"The correct label, according to the classifier,  is neither C3 nor C2, but C1, with a prediction likelihood of about 75.0%. By analysing the attributions of the input features, they can be ranked according to the level of impact, from the most important feature to the least relevant, as follows: F11, F1, F5, F2, F8, F12, F7, F10, F6, F9, F3, and F4. Among the twelve features considered by the classifier for the prediction verdict, seven have a positive influence on the classifier. F5, F2, F7, F12, and F10 are the five negative features that swing the assessment decision towards other classes. The value of F11 has a strong positive contribution to increasing classifier's response, favouring the assigning of C1. The last four features, F6, F9, F3, and F4, have a weak positive effect on the classifier's prediction for this case.\",\n",
       " \"It is important to note that the classifier's labelling decision is based solely on the information supplied. The classification verdict is as follows: C2 is the most probable label with respect to the case under consideration, since the prediction likelihood of the other label, C1, is only 12.50%. The most important variables contributing to the abovementioned classification are F13, F2, and F3, whereas remaining variables such as F11, F8, F9, F4, and F10 have a modest effect on the classifier's labelling decision for the given case. All the top features positively support the selection of C2 as the correct label and the negative variables increasing the chances of C1 are F5, F15, and F16. Given that these are the variables reducing the classifier's response towards generating label C2, it is not surprising that the classifier is very confident that C2 is likely the true label. In addition, the joint negative attribution of F5, F15, and F16 is very small when compared with the positive attributions of F13, F3, F11, and F2.\",\n",
       " 'According to the classification algorithm or model, C1 is the most likely class, with a very high confidence level, and C2 has a very low likelihood of being the right label. All of the inputs are proven to contribute to the categorization described above and the following is a ordering of the features from least essential to most significant based on their degree of influence: F2, F3, F4, F8, F1, F6, F7, and F5. It is clear from the attributions of the input attributes that the algorithm is quite certain that C2 is not the proper label for the given case since each attribute contributes positively, resulting in a significant push towards C1.',\n",
       " 'The model indicates that C1 and C4 have zero prediction probabilities, while that of C2 is 3.85%, meaning the most probable label for the given case is C3  and the confidence level is approximately equal to 96.15% certainty. The major features driving the above classification are F19, F8, and F17, while the least relevant features are F10, F9, F7, F5, and F3. The intermediate features have varying degrees of influence, from moderate to low, and these include F15, F11, and F20. Among the top influential features, only F15 has a negative contribution, driving the prediction slightly towards one of the other possible classes. Furthermore, the top two positive features, F8 and F19, have a stronger influence than all the negative features combined. It is, therefore, not surprising that the model is confident about the classification verdict here.',\n",
       " 'The predicted output label from the model is C2 with almost 100% certainty, indicating it is very certain it is correct and this is mainly because the likelihoods across the other labels C3, C4, and C1 are 0.47%, 0.05%, and 0.04%, respectively. Among the top features F1, F16, and F18, the features F18 and F16 positively influence the classification decision above in the direction of C2, whereas F1 influences in the opposite direction in favour of an alternative label. With a similar direction of influence as F1, the features F5, F3, F20, and F2 negatively impact the prediction of C2, whereas F8 positively impacts it. Features F7, F3, F15, and F12 also have a smaller influence on the prediction output for the given case and finally, the features F11, F14, and F9, have very little contributions to the classification made by the model for the case under consideration.',\n",
       " \"The prediction probabilities associated with the classes C1 and C2 are 99.56% and 0.44%, respectively. Therefore, we can conclude that the most probable label for the given data is C1. The classification model's decision here is largely based on the impacts of the F16, F6, and F10, whereas the F15, F11, and F17 have very little to say about the decision here. In terms of the direction of influence of the features, F16, F9, F5, F13, and F4 are the top positive features contributing to the prediction outcome of C1. Conversely, the marginal doubt in the classification decision (represented by the probability of C2) is largely due to the negative contributions of F6, F1, F3, and F7. To sum up, the very high certainty in the classification output decision could be explained by considering the fact that the joint influence of the negative features is smaller than that of the positive features.\",\n",
       " \"The model identifies the case as C1 since, the true label has just 33.63 percent chance of being C2 when the prediction probability is calculated. The in-depth analysis found that the bulk of the attributes had negative impacts, driving the prediction away from C1 and toward C2. F4, F15, F3, F12, and F17 are among the features that contribute negatively. Furthermore, these features' values are ranked higher than any of the positive features, which are F14, F2, F8, and F13. Finally, it can be concluded that the values of F1, F7, and F6 are less important in predicting the outcome of the case under review, hence they are ranked the least.\",\n",
       " \"0.0% is the predicted probability that C2 is the true label for the test example under consideration according to the classifier.  Judging based on the predicted probabilities associated with the other remaining labels, the classifier is 75.0% confident that C3 is the correct label. From the analysis, the features ranked according to the degree of impact from the most significant feature to the least relevant ones: F6, F8, F1, F5, F4, F2, F7, F11, F12, F3, F10, and F9. Examining the contributions or attributions of the features further  revealed that the ratio of positive features to negative features is seven to five.  The negative features swinging the prediction decision towards the other classes are F1, F5, F7, F2, and F11 since their contribution decrease  the probability that C3 is the true label for the given case. The value of F6 has the strongest positive contribution increasing the classifier's response in support of assigning C3 but the last four features, F12, F3, F10, and F9, have a weak positive influence on the labelling decision or conclusion with respect to the given case.\",\n",
       " 'The classifier is very certain that C1 is not the true label since the predicted probability of C2 is given as 100.0%.  Analysing the attributions of the features indicates that the most relevant features are F12, F4, F13, and F5 while F2, F8, and F9 are the least relevant features. The values of F3, F1, F10, F11, F7, and F6 have a moderate influence on the classification decision made here. Considering that the classifier is 100.0% certain that C2 is the true label, we can conclude that the collective negative attribution of F13, F1, and F6 is clearly outweighed by the positive attributions of features such as F12, F4, F3, and F5.',\n",
       " \"Based on the information provided to the classifier, the true label for the given case is likely C1, with a confidence level of 76.26%. Each input variable has a different degree of influence on the classifier's final labelling decision with respect to the case under consideration. Whilst F3, F11, and F12 have lower contributions to the classifier's decision, F13, F1, and F9 are identified as the major contributors resulting in the assignment and classification probabilities across the two classes. There is a 23.74% chance that perhaps C2 is the true label and the features responsible for this are the negative features, F9, F2, F5, F15, F10, F3, and F11. Driving the classifier's decision in favour of C1 are the positive features such as F13, F1, F14, F16, F4, F6, and F8.\",\n",
       " 'The classification model employed made its label selection decision based on the information provided about the case under consideration. With a moderately low degree of confidence, it classifies the case under consideration as C1. Specifically, per the model, the probability of labelling the case as C2 is equal to 48.66%, hence not as likely as C1. The decision made here can be attributed to the influence of features such as F16, F8, F2, F12, and F13. However, F1, F6, F4, F15, and F3 are the least relevant features with respect to the classification made.  The confidence level of the model is marginally above average and this can be attributed to the negative contributions of F10, F16, F9, F7, F1, F15, and F3. The negative features shift the prediction decision in the direction of C2, however, the positive contributions of other features such as F8, F2, F12, and F13 improve the odds of the C1 label.',\n",
       " 'The prediction results are as follows:  the probability that C1 is the correct label is 97.12%, the probability that C2 is the correct label is 2.55%, and the probability that C3 is the correct label is 0.33%. Judging based on the prediction probabilities across the classes, C1 is the most probable label. The very high confidence in the assigned label can be attributed to the very strong positive influence and contributions of the variables F1, F6, F10, F4, and F8. The other positive variables are F2, F9, and F7. The positive variables increase the probability that C1 is the correct label for the given case. Decreasing the probability of C1 are the negative variables F5, F11, F3, and F12. Considering that the combined effect of the negative factors is quite minimal in comparison to the top positive variables, it is not surprising that the model is very sure that neither C2 nor C1 is the best label for the given case.',\n",
       " \"C1 is the model's predicted output for this given case, with an accuracy of 87.13% meaning the likelihood of C2 is only 12.87%. F13, F4, F2, F12, and F14 have the most effect on the output prediction choice in this case, whereas on the other hand, F10, F5, F16, and F7 are not that important to the decision made here. F13, F14, and F12 are the top negative features when you consider direction of their respective impacts, decreasing the model's reaction to labelling the given scenario as C1 and also F3, F1, F5, F16, and F7 are the other features that contribute negatively. In a nutshell, F4, F2, F8, F9, and F6 are primarily positive improving the odds of C1  with respect to this classification conclusion.\",\n",
       " 'The classification model or algorithm classifies the provided data or case as C1 with a predicted likelihood of 94.16%, meaning that the chance of C2 being the true label is only 5.84%. The most relevant features driving the classification above are F7, F9, F8, F6, and F10, however, arranging the input features in-order of their contributions revealed that the least influential features are F2, F1, F11, and F5 since their values receive little consideration or emphasis from the algorithm. In relation to the directions of influence of input features, only F6 and F11 are shown to have negative contributions, which tends to drive the labelling judgement towards C2 instead of C1. Considering that the combined effect of all the negative features is lower than that of the positive features such as F7, F9, F8, F10, F3, and F4, it is valid to say that C1 is the most probable label.',\n",
       " 'The final classification made was C2, but with a likelihood of only 55.19%, the model is uncertain about this prediction. By far, feature F12 had the most impact and following F12 are F5, F15, and F6 have been identified as having the comparable influence on classification. The combination of F12, F5, F15, F6, and F1 features has shifted the classification decision from C2 to C1. While F13, F16, and F10 are all features with a moderate impact on the classification, F13  is the only one of that set that has had a positive impact on the C2 classification and the remaining positives are F2, F18, and F11. Lastly, the features F19, F14, F8, F7, and F9 had very marginal negative contributions to the classification verdict.',\n",
       " \"Based on the prediction probabilities, C1 is the most likely label for the given case considering the values of the input variables and because the likelihood of C2 is very marginal, so the classifier is very confident that C1 is the right label. An analysis of the contributions of the variables has shown that F9 is the most relevant, with the strongest influence on the classifier's decision, however, to arrive at the classification above, the classifier probably ignores the values of the least ranked variables, F7 and F6. The level of confidence of the classifier with respect to the above classification decision is higher, primarily because most of the influential variables have a positive impact. F9, F3, and F4 are the top positive variables that increase the likelihood of C1. Having a different direction of influence, F8, F6, F7, and F5 are the negative factors, but compared to F9, their impact on the prediction decision above is low.\",\n",
       " \"Based on the information available about the case under consideration, the classification model is very uncertain about the appropriate labels for the case. According to the model, there is an almost equal distribution in terms of the probability that any one of C1 and C2 is an appropriate label. This indicates that any of the possible labels could be the true one, but for simiplicity, the model selects the class as C1. The above judgement is mainly due to the influence of the following factors or variables: F6, F9, F12, and F3 while the least relevant variables are F10, F2, and F5. Positive variables like F3, F12, F19, and F1 increase the model's response in favour of the assigned label. Nevertheless, negative variables such as F6, F13, F8, and F9 reduce the possibility that C1 is an appropriate label because their values support the selection of C2. Uncertainty about the classification here can be due to the fact that the most important negative properties, F6 and F9, have very high impacts, which moves the model's judgement away from C1 towards C2.\",\n",
       " 'There is about an 81.01% chance that C2 is the probable label, hence the predicted probability for the C1 class is only 18.99%.  The algorithm or classifier arrived at the prediction verdict above mainly based on the influence of features such as F3, F4, F7, and F1. For the algorithm, the least relevant feature is F6, which is shown to have a very small contribution in relation to the label choice here.  When the directions of influence of the input features were investigated, it was discovered that F3, F2, F7, and F1 have positive attributions, pushing the algorithm higher towards the C2 label. Negative features such as F4, F5, and F8 assist in dragging or pushing the classification decision lower towards C2, where it was originally classified and this is mainly because their contributions to the prediction favour choosing or labelling the case as C1.',\n",
       " \"According to the classification algorithm, there is 77.69% chance that the given case is part of the C1 population. The features with the largest impact driving the algorithm to arrive at the above decision are F8, F1, and F15 which are followed in the decreasing order of influence by F14, F4, F13, F7, F3, F6, F11, F2, F5, F12, F9, and F10. Inspecting the direction of influence of the input features showed that, F8, F3, F6, F10, and F1 have negative influence on the prediction, shifting the algorithm's verdict towards the C2 class and can be blamed for the doubt in the classification decision. However,  strongly pushing the classification higher towards the C1 label are the positive features such as F15, F14, F4, F13, F7, and F11.\",\n",
       " \"The likelihood of the true label for the given test case being equal to the model's output prediction, C2, is 85.71% and since it's not 100%, there is a small chance of about 14.29% that the model could be wrong. Among the features employed for this classification, F4, F8, F10, F11, F12, and F9 are the top features influencing the model's prediction decision. The features with the strongest positive influence are F4 and F8 and in fact, these are shown to be the two main driving forces controlling the model's decision regarding the given case. Besides, some otf the other positive  features include F11, F12, F2, F13, and F9. However, the atrribution of F10, F3, F14, F6, and F7 indicates the true label could perhaps be C1. While the different input features have some sort of contribution to the prediction made for this test case, the features F5, F16, and F15 have the least impact on the final decision here.\",\n",
       " \"To begin with, the classification choice is entirely dependent on the information or data provided to the prediction model. According to the model, C2 has a 61.61 percent probability of being the true label, whereas C1 has a 38.39 percent chance of being the true label. Because the estimated probability of C2 is greater than that of C1, it is reasonable to assume that C2 is the most probable true label. The key variable responsible for this classification is F23, with a very significant positive effect on the model's conclusion, pushing it higher towards C2. F22, F7, F30, F14, F13, F20, F8, and F6 are the next set of relevant variables. F22, F30, F14, F20, F18, F12, and F8 have negative contributions that are responsible for the decrease in the chance that C2 is the actual label since they prefer to assign the C1 label instead. This means that the contributions of F7, F13, F10, F29, and F6, together with F23, can explain why the model is rather confident that C2 is the correct label.\",\n",
       " \"With a prediction likelihood of 62.34%, the model trained to generate predictions based on input variables identifies the presented example as C1. The model's label assignment choice for the given case is heavily impacted by the values of input variables such as F3, F1, and F9. The least important variables, on the other hand, are F8, F2, and F4. Furthermore, the impact of F5, F6, and F7 is regarded as moderate. F9 and F5 are the variables identified to have negative contributions to the classification when you take into consideration their respective direction of impact. All of the remaining variables have a positive influence, contributing to the classification of the presented case as C1. As a result, it is unexpected that the model's confidence is just 62.34% which suggest that the negative attributes may have a larger say in the appropriate label for the case under review.\",\n",
       " \"For the case under consideration, the model outputs C2 with high confidence level since the associated predicted class label is 89.73% whilst that of C1 is just 10.27%. Just few features out of the entire input features are shown to have control over the prediction made here. The prediction verdict C2 is mainly based on the variables F16, F43, F35, and F14. Other variables with moderate attributions include F17, F44, F3, F21, F5, and F40. Each variable mentioned above is shown to have different direction of contribution or impact for instance while F16, F14, F5, and F21 positively support the model's output decision, F43, F35, F17, F44, F40, and F3 contributed to decreasing the likelihood or odds of C2 being the true label for the given test instance. The variables shown to have no influence or contribution on the classification decision above are mainly F15, F39, F4, and F18.\",\n",
       " \"The model trained to solve the classification task labels the given case as C2 with a moderately high degree of confidence level equal to 60.13%. However, it is important to note that the prediction likelihood of C1 is 39.87%. Investigation of the contributions of the features to the above label assignment indicates that the most relevant features considered by the model are F16, F2, F9, and F10. Increasing the prediction likelihood of label C2 are mainly the positive features F16, F9, and F10. These features are termed positive features since their direction of influence is in support of the assigned label C2. On the contrary, F2, F3, and F12 are the top negative features, accounting for the uncertainty in the final prediction verdict. In plain terms, these negative features support labelling the case as C1, contradicting the model's decision in this case.\",\n",
       " \"The following assertions are based on the information provided to the classification model. The classification model's confidence in this case's prediction output is approximately 69.40% and this suggest that the chance of label C1 is about 30.60%. The prediction attribution analysis shows that F7 and F8 are the most important features, whereas F4 and F1 are the least influential. F6, F2, and F3 are recognised as the only negative features considering the direction of effect of the features since their contributions reduce the prediction likelihood of the specified label, C2. F7, F8, F5, F4, and F1, on the other hand, have a positive impact on the model in favour of labelling the provided situation as C2 rather than C1.\",\n",
       " 'Because the chance that the label is the alternative class C2 is only 1.94 percent, the model anticipates that C1 will be the correct label in this situation. Specifically, it can be concluded that the model has a high level of confidence in the label C1. The feature attribution analysis conducted suggests that the two most relevant features considered when choosing the C1 are F29 and F1. F8, F27, F7, F13, and F22 were some of the other factors that positively helped with this prediction. F18, F36, F23, and F6, on the other hand, are the features with a negative influence on the above prediction judgement. In comparison to the F31, F22, F27, and F1, the foregoing features have little impact on the model and this might explain why the model is so certain that the correct label is C1. However, it is crucial to note that not all features are considered by the model during the label assignment with the irrelevant features such as F41, F32, F39, and F40 having extremely low attributions which happens to be almost zero.',\n",
       " 'The model predicted that the example should be classified as C1 with a 76.06% likelihood but the model also identified that there was a 23.94% chance that the right label could actually be C2. The positive influence of features F4, F7, F9, and F1 on the model supports the class assignment of C1. Both F3 and F8 are features with a small positive impact on the classification decision for the given case. F11 and F5, in contrast, has a small negative impact on the output verdict that drives the decision away in favour of the other label. The features F12 and F6 have only a very small impact on the final classification decision. Finally, F10 is shown to have zero impact on the model in this case, hence it is not relevant to the prediction of class C1.',\n",
       " \"The classification model labels the given case as C1 at a very high confidence level since the probability that C2 is the correct label according to the model is only 3.50%. The assignment decision above is mainly based on the values of the features F3, F5, F7, and F1. On the other hand, the values of F6 and F2 are shown to have a very weak influence on the model's decision. The analysis revealed that only four of the input features support the decision by the model, while the remaining ones contradict the assigned label. The four positive features are F7, F1, F9, and F8.\",\n",
       " \"The model generated the label, C1, with a very high likelihood of 99.69%, hence the probability that C2 is the right label is only 0.31%. Based on the analysis performed to understand the attributions of the different features, F11 was by far the most impactful positive feature whereas, the most negative feature is identified as F6. F8 also had a positive influence on the model's prediction, as did F1, F10, and F2. This is in contrast to F5 and F7, which had a negative influence on the prediction. Many of the features under consideration had only smaller impact on the outcome of the model and these are F4, F9, F2, F3, and F10. Considering the attributions of the input features, only F6, F5, F7, F4, F9, and F3 are shown to have negative attributions, decreasing the likelihood of the predicted label, however, the collective influence of the negative features is not enough to swing the model towards a different label.\",\n",
       " \"The item is labelled as C2 with a high degree of confidence since the predicted probability associated with the other class is 0.0%. Looking at the contributions of the features, only F1 and F2, are shown to drive the model towards predicting C1. However, these features are ranked as the least relevant, implying that their values have a very low impact on the model's decision. All the positive features, F3, F6, F5, F4, and F7, are ranked higher than the negative ones, with higher impacts on the model, significantly supporting the assigned label which could explain the high confidence level.\",\n",
       " 'With the prediction probability distribution across the labels, C1 and C2, respectively, equal to 0.30% and 99.70%, the model labels this instance as C2. The most important features are F3, F4, and F7. The variables, F8, F5, F1, and F2, have values, increasing the chances of C1 being the label for this case. Increasing the odds of C2 being the correct label are the values of the remaining variables. The strong positive variables are F3, F4, and F7 coupled with the moderate positive influence of F6 and F9 pushes the prediction in favour of C2 hence the prediction confidence level achieved.',\n",
       " 'C3 is given as the predicted label with very high confidence, and according to the classification algorithm, there is no chance that either of the remaining three labels, C4, C3, and C1, is the right label for this case since the predicted probability of C2 is 100.0%. Based on the attribution analysis and investigations, the ranking of the input features from the most important to the least important is: F3, F6, F5, F4, F2, and F1. From the attribution analysis, F3 is the only one that positively contribute and support the above classification decision, while the remaining features such as F6, F5, F2, and F4 have negative contributions, shifting the decision in a different direction. In conclusion, looking at the predicted confidence level, one can say that the very strong attribution or influence of F3 is enough to dwarf the contributions of the features F6, F5, F4, F2, and F1.',\n",
       " 'In the present case, there is only a 12.50% chance that C1 is the correct label, which means there is an 87.50% chance that C2 is the true label. Therefore, the most probable class assigned by the model is C2. The above decision is mainly based on the influence of the following variables: F5, F4, and F7. Of these main variables, only F4 had a very strong positive impact on the model, increasing the prediction probability of the assigned label. The most important variables that lower the likelihood of C2 being the correct label are F7 and F5. The remaining two variables moving the decision away from C2 are F8 and F2. F3 and F6 are the least important variables, with a marginal impact on the model and this positive impact on the model is moderately low.',\n",
       " 'There is an evenly split chance that the prediction could be either of the two labels, C1 and C2. Based on the predicted probabilities, we can conclude that the model is uncertain about which label is the correct one. The abovementioned prediction decision is chiefly attributed to the influence of the following features: F9, F3, and F1, however, the least important or ranked ones are F5 and F2. The attributes F6, F4, F7, and F8 are shown to have moderate contributions.',\n",
       " 'The label assigned to the given sample is C1 at a confidence level of 56.81%. This means that there is a 43.19% chance that the sample could be C2, representing an uncertain classification decision. The values of F9, F2, F3, F4, and F8 are the major contributing factors resulting in the classification decision here. On the other hand, the least important features are F6, F7, and F5, with a low level of influence. Considering the direction of influence of the features (that is, either supporting or contradicting the prediction above), only F3, F4, and F8 are shown to have positive attributions, increasing the likelihood of the assigned label. This implies that the values of the remaining features F1, F9, F2, F7, F6, and F5 have negative attributions, shifting the verdict in the opposite direction in favour of C2. In simple terms, the correct label should be C2 according to the negative features enumerated above.',\n",
       " 'The classification algorithm classifies the given case as C2, since there is only an 18.57% chance that C1 is the correct label. The effects and contributions of positive input variables F9, F10, and F8 are the major drivers for the above classification. Besides, most of the remaining predictors such as F11, F4, F2, F14, and F5, are positive variables, decreasing the likelihood of the C1 label and making the label C2 more likely. The only variables with negative contributions are F3, F5, F13, and F6, which motivate generating the label C1 instead of C2. In summary, comparing negative attribution to positive attribution explains why the algorithm can determine that C2 is the right label for the given case.',\n",
       " 'The odds are in favour of C2 being the correct label for the given case. This is because the probability of the other label, C1, is only 1.03%. Ranking the features in order of relevance to the classification decision above, F1, F2, F4, F8, F5, F3, F7, and F6. Among the set of features used for this prediction, F2, F5, and F3 are the only ones shown to decrease the likelihood of the C2 decision. The positive features increasing the chances of C2 being the correct label are F1, F4, F8, F7, and F6. The joint attribution of the positive features is stronger than that of the negative ones, which explains the confidence level associated with class C2.',\n",
       " 'According to the prediction algorithm or model, there is almost 100% confidence that C2 is the label for the case under consideration. This is because the probability of C1 being the correct label is only 0.70%. The classification decision above is mainly based on the values of the following features: F12, F3, and F9 since their respective attributions are higher than any of the remaining features. F3 has a negative contribution to the prediction made by the model for this case, while in contrast, F12 and F9 have positive contributions, that push the classification decision in favour of C2. Unlike all the features mentioned above, the values of F14, F6, F2, and F11 have a limited impact on the classification decision above.',\n",
       " \"The C2 has a predicted probability of just 3.10 percent, but the C1 has a predicted probability of 96.90 percent, which implies that C1 is the most likely class chosen by the classifier for the supplied data. Not all of the input features are directly relevant to labelling the provided data and, per the attributions analysis, only F33, F5, F24, F10, F22, F21, F11, F23, F32, F29, F9, F17, F28, F20, F38, F19, F1, F4, F6, and F31 are the relevant features. However, F2, F8, and F35 are examples of irrelevant features since their contributions are mostly ignored by the classifier when classifying the given case. According to the attribution assessment, F33 and F5 have a very substantial combined positive influence, enhancing the classifier's response towards C1 rather than C2. In contrast, the top negative features are F24, F22, and F10, which weaken the classifier's response in favour of C2. When the attributions of F33, F21, and F5 are compared to the attributions of the negative features indicated above, it is not unexpected that the classifier is highly certain that C1 is the most likely label in this case.\",\n",
       " 'The classification algorithm labels this instance as C1, but its level of confidence is moderate considering the fact that there is about a 44.0% chance that C2 could be the appropriate label.  The features, F1, F4, F8, and F5, negatively influence the prediction verdict away from C1 and favour assigning C2 as the correct label. Contradicting the influence of the negative feature are features such as F2, F3, and F7, with positive contributions, improving the odds in favour of the probable label, C1.  To summarise, the top features with the most influence on the above label assignment are F2 and F1, but F5 and F9 are the least influential input features considered by the algorithm.',\n",
       " 'The case given is labelled as C2 with close to an 82.07% confidence level, implying that the likelihood of C1 being the correct label is only 17.93%. The classification above is mainly due to the contributions of different features such as F39, F13, F24, F38, F22, and F26. But, not all features are considered by the classifier to arrive at the decision made for the given case. These irrelevant features include F40, F33, F11, and F8. Among the influential features as shown, F39, F13, F24, F38, and F18 are the top positives that increase the probability of C2 being the true label. However, F22, F26, F36, F29, F4, F42, F27, and F20 are the top negative features, driving the prediction lower towards C2 in favour of C1. In closing, the most important features with regard to this classification output are F39 and F13, all with positive attributions, explaining the very high confidence level.',\n",
       " \"The output labelling decision is C1 with almost 100% certainty, which indicates that there is practically no chance that C2 is the right label choice for the case under consideration.  F61, F15, F1, F86, and F23 are the features with the highest joint positive impact, influencing the model's decision to output C1 and the feature F89 also has a high impact, but unlike F61, F15, F1, F86, and F23, F89 attempts to shift the decision away from C1 in the direction of C2. Also, F83 and F4 have a moderate impact on the decision towards C1, although this is still higher than features F65, F13, F90, and F77, which have a moderate impact, favouring the prediction of class C2. Besides, F4, F37, F2, F46, F6, and F28 all have a positive influence on the final classification verdict further increasing the odds in favour of the C1 label. It is worthy to note that for this classification decision, a large number of features are shown to be irrelevant hence received negligible consideration from the model, and these include F35, F3, F8, F85, and F51.\",\n",
       " 'The label assigned by the classifier in this instance is C2, which had a very high prediction likelihood of about 99.93%. According to this classifier, the probability of C1 being the correct class is only 0.07%. Analysis performed shows that the confidence level of the classifier here is due to mainly the values of the features F10, F11, F5, and F2. The least relevant features to this classification verdict are F12, F3, F6, and F13 since the magnitude of their respective attribution is smaller compared to the remaining features. Furthermore, only the features, F4, F8, and F3, have a negative influence, increasing the chances of predicting the alternative label C1. However, when compared to the joint influence of the positive features such as F10, F11, and F5, the influence of the negative features is smaller, hence explaining the high degree of confidence in the predicted C2 label.',\n",
       " \"The classifier is very certain that C2 is not the accurate label for the given data or example, but that C1 fits. F20, F5, F1, F14, F4, F6, and F13 are the input features that have the most influence on the choice or judgment. F16, F9, F30, F7, F22, F24, F11, F2, F27, and F28, on the other hand, are found to be irrelevant and have negligible inlfuence on the classifier. Amongst the top features, F20, F5, and F1 are the one shown to have negative contributions, greatly favouring C2, lowering C1's prediction probability. Despite the significant negative attributions of the top impactful attributes, the classifier is quite certain that C1 is the correct label, based on the prediction probabilities.\",\n",
       " \"The classifier's anticipated label for this case is C2 which is a decision that it is highly confident about since the predicted likelihood is 100.0%. The most important variables are F10, F6, F9, and F5, whose values lead to the aforesaid classification conclusion. Under this classification instance, examination of the attributions of the features showed that F7, F11, and F1 are the least essential features. Because majority of the case's attributes positively validate the assigned label, it's not unexpected that the classifier picked the C2. F10, F6, F5, F2, F12, and F13 are all positive variables, while F9, F4, and F8 are three contradicting variables that moderately drive the labelling judgment towards C1.\",\n",
       " 'The model selects C2 as the correct label with a probability of 57.58%, while the other class, C1, has a slightly lower probability of 42.42%. The most relevant attribute is F5, followed by F7, F1, F6, F8, F9, F3, F4 and finally F2, which is the least relevant. The features F8, F3, and F5 have a positive influence, increasing the probability of the classification output, while F1 has a negative attribution, swinging the model to assign C1 instead. F6, F4, F7, and F9 are some of the other negative attributes. Finally, F2 has a very small positive control over the prediction in this test case but it further increases the confidence in the label chosen for the given case.',\n",
       " 'The prediction verdict here is that the most probable class label is C1. Actually, the classification algorithm indicates that there is no possibility that the correct label is C2.   Majorly contributing to the above classification are F3, F7, F6, and F11, all with positive influence. It is therefore not surprising that the algorithm is confident that C1 is the right label. The other positive features considered to arrive at the decision here are F8, F9, F10, F4, and F1. According to the attribution analysis, only F2, F5, and F13 have negative contributions, which tend to attempt to swing the final verdict in favour of C2. To sum up, the joint negative influence is not enough to outweigh the positive features, hence the C1 is assigned for the given case.',\n",
       " \"Due to the prediction probability distribution across the class labels, the labels assigned to this example is C1 with a high degree of confidence, close to 100 percent. The most significant features driving the classification above, according to the attributions of the input features, are F14, F4, F6, and F9. F10 and F2, on the other hand, are the least essential features to this prediction here. In addition, just four of the input features have a negative impact, skewing the classifier's judgement in favour of the C2 label. F10, F6, F13,  and F2 are the opposing features. The contribution of the negative features, with the exception of F6, is quite modest when compared to the top positive features such as F4, F9, and F11.\",\n",
       " 'The classification algorithm predicts that the data sample given should be classified as C1 with a probability of 76.06%, but it also finds that there is a 23.94% probability that the correct label will be C2. The positive influence of the F6, F11, F8, and F9 features on the algorithm supports the C1 class tasks. F10 and F3 are features with little positive influence on the classification decision for a particular case. F5 and F12, in contrast, has a small negative impact on the output decision that result in the reduction in the likelihood of C1 hence can be said to favour labelling the case as C2. F1 and F4 had only a minor positive impact on the final labelling decision and finally F7 was shown to have zero effect on the algorithm in this case.',\n",
       " \"According to the model, there is a higher chance that the case's label is C1. This prediction decision is based primarily on the attribution of the following features: F3, F9, F4, and F5. Aside from F5, all the other features listed above have a strong positive influence, increasing the probability of the predicted class C1. Similar to F5, the values of features F10, F6, and F8 suggest the other label, C2, could be the correct label. However, unlike F3, F9, and F4, each of the negative features has a moderate contribution to the final decision. The remaining features F1, F2, and F11 are shown to have marginal contributions to the model's decision for this case, and F7 was ranked as the least important feature. In summary, with strong positive attributions from F3, F9, F4, and F1, the model is very certain about the classification verdict, with a certainty of 100.0%.\",\n",
       " \"For the case under consideration, the model's output labelling decision is as follows: there is no possibility that C1 is the label for the given case, C2 is the most likely class label, with a confidence level close of 100.0%. The values of the input features, F27, F74, F84, F79, F72, F45, and F42, are the main driving forces resulting in the above classification. The features with moderate influence on the decision here are F40, F52, F11, F20, F38, F56, F6, F25, F55, F47, F46, F75, and F1. Apart from all the abovementioned input features, all the remaining ones, such as F23, F34, F85, and F57, are shown to be irrelevant to the decision made here. Also per the attribution analysis, not all the influential features support labelling the given case as C2, and these are referred to as negative features since they reduce the probability that C2 is the right label here  and these are F42, F52, F46, F75, and F1. The notable positive features increasing the probability that C2 is the right label are F27, F74, F84, and F79.\",\n",
       " \"Based on the influence of features such as F10, F3, F9, and F1, the classifier is pretty confident that the correct label for the given data is C1, whilst, there is a 10.0% probability that the proper label could be C2.  The majority of the features have positive contributions, while only F1, F11, and F8 are the negative features, decreasing the classifier's response towards choosing C1. The notal positive features that increase the classifier's response higher towards label C1 instead of C2 include F10, F3, F4, F2, F7, and F9. Taking into consideration the attributions of the input features, we can attribute the classifier's confidence associated with this prediction to the fact that the negative features only have a moderate impact on the classifier's decision for the given data.\",\n",
       " \"The model's prediction for this test case is C2 with an almost 100% confidence level which implies that the likelihood of it being a different class label is closer to 0%. Among the top influential feature-set, F10 has a value shifting the label choice in favour of C1, while the others, F11, F5, and F18, all have a positive impact supporting the decision made by the model to assign the label C2. Other features with positive support or impact on the prediction made include F12, F14, F1, and F2. However, F7, F13, F3, and F15 are the other negatives shifting the prediction decision in the direction of the alternative class label. TO sum up, the positive features clearly outweigh the negative features interms of their contributions, hence the confidence level in the classification output.\",\n",
       " 'The classification verdict for the selected case is C1, and the model is very certain about that considering the prediction probabilities across the possible classes. The top variables influencing this decision are F24, F7, F6, F38, and F11. Other variables that are regarded as somewhat important are F19, F26, F14, F18, F37, F4, F28, F25, F3, F22, F21, F30, F8, F15, and F34. Among the top variables, F24 and F7 decrease the prediction response; therefore, they are pushing the verdict toward C2. Similar to these features, F19, F26, and F37 negatively support assigning C1 to the case. Positively supporting the predicted label are the features F6, F38, F11, and F14. Unlike all the features mentioned above, the values of the remaining features such as F23, F17, F32, and F13, are unessential when determining the correct label for this case.',\n",
       " \"Classifying the given case based on the values of its features, C1 is the best label for the given case since its prediction probability is 99.45%, while C2's is just 0.55 percent. The most relevant factors for the classification or prediction declaration above are F2, F8, and F4, whereas the least influential factors are F3, F1, F10, and F7. The other factors' influence can be described as modest and after further inspecting the direction of effect of the factors, F2, F8, F5, F10, and F7 all contribute positively to giving the label C1. These are the favourable factors that raise the likelihood of C1 being the correct designation, however, F4, F9, and F6 are mostly responsible for minimising the chances of C1 and promoting C2.\",\n",
       " 'Per the predicted likelihoods across the classes, the model predicts label C2 in this case with a high confidence level. Features F4, F5, F1, and F7 are all driving the model towards the C2 classification, with feature F4 being the strongest driver and F7 being the weak driver among the above mentioned set of features. Features F8 and F6 have moderate negative impact on the C2 classification, while feature F2 has a strong positive impact. Finally, feature F3 has a very weak negative impact on the C2 classification decision driving the model towards assigning C1 to the case here.',\n",
       " \"The classifier is very uncertain about the correct class for this example and this is because both classes are shown to be equally likely. The above prediction conclusion is mainly based on the influence of the top input features F1, F7, and F4, while F2, F6, and F5 have less influence on the classifier when classifying the given case. When the direction of influence or contribution of each input feature is examined, only F7, F7, F2, and F5 are revealed to have a positive contribution, improving the classifier's affinity to produce the label C1. The remaining features, F4, F1, F8, F9, F3, and F6 have a negative influence and contribution to the final decision.\",\n",
       " \"C1 is the label assigned to this data instance based on the fact that C2 is shown to be very unlikely, with a prediction probability of only 0.68%. The variables most relevant to increasing the probability of the prediction here are F2, F14, F20, and F22. Other positive features that increase the chances of predicting C1 are F6, F3, and F19, however, unlike F20, F14, F2, and F22, these have only moderate contributions to the model's classification decision for this instance. In contrast, F15 is the only top-ranked feature that led the model to classify towards C2, while other negative features with a moderately low contribution included F11, F1, F7, and F12. The least relevant features are F21, F8, F18, and F9, with a very low influence on the C1 prediction, however, unlike these features, F16 and F13 are shown to have no impact, since their attributions are very close to zero, when determining the correct label for the case under consideration. Finally, F13 and F16, according to the attribution analysis have no impact on the classification decision here.\",\n",
       " \"C1 is the class assigned to this case or instance. However, according to the classifier, there is a 5.75% chance that the other label, C2, is the correct one. The labelling decision above is mainly due to the values F2, F6, and F3. F8 and F5 are the least ranked features since they have marginal attributions. F6, F1, F9, and F2 have values, increasing the odds of C1 being the correct label and these four features are commonly known as positive variables given that they support the classifier's output decision for the given case. The remaining variables had negative attributions, driving the classification decision towards label C2 and the most negative variables are F3, F7, and F10.\",\n",
       " 'The model predicts the class label of this test case or instance as C2 and it is quite confident in the above prediction decision considering the predicted confidence level. The above prediction decision was made primarily based on the values of the following features: F6, F8, F3, and F18. The top features, F6 and F8, positively contribute to the final prediction of C2. Besides, F18 also has a positive impact, pushing the model to output C2. However, the value of F3 supports the prediction of the alternative label, C1. However, compared to F6 and F8, the influence of F3 is very small. The features with moderate influence or impact on the prediction made for this test case are F14, F16, and F1. While F14 moderately supports the C2 prediction, F16 and F1 have values, pushing the model toward predicting C1.',\n",
       " 'The case is labelled as C2 by the classification model, and according to the model, there is little to no chance that the correct label could be C1. Per the feature attribution inspection, F8 and F5 are the least influential features. The classification decision to label this case as C2 is mainly due to the positive contributions of F3, F7, and F4. However, the strong negative influence of F6 indicates that the true label could be C1, but since the likelihood of C1 is 0.0%, we can say that the positive features successfully drive the decision in favour of the C2 label. F2, F1, and F5 are the other negative features that unsuccessfully attempt to shift the decision in favour of C1. From the attribution analysis and the predicted likelihoods across the classes, we can conclude that the model is certain that C1 is not the true label.',\n",
       " 'The decision of the classification model on the true label with respect to the given case is based on the information provided to it. From the prediction probabilities, C1 is selected by the model as the most likely label, with a very high confidence level equal to 97.49%. According to the attributions analysis, the very high confidence in the validity of C1 can be attributed to the very strong positive influence of F15, F18, and F13. The contributions of all the other features are moderate to low. The least relevant features are F1, F2, F10, and F5, whereas the moderate ones include F14, F6, F11, and F7. The very marginal uncertainty with respect to the classification decision here can be blamed on the moderate influence of negative features such as F14, F6, F11, F16, F4, and F7. Aside from F15, F18, and F13, some of the other positive features are F19, F12, and F2, with moderate to low contributions, pushing the decision further higher towards C1 away from C2. Finally, F5 has a negligible contribution to the decision above.',\n",
       " 'With a moderate likelihood of 50.0%, the label for this case is judged to be C3. The classifier, on the other hand, says that C1 and C2 are equally likely, with a predicted probability of 25.0 percent. The aforementioned decision is mostly dependent on the features of the given case and the values of F7, F5, and F11 are demonstrated to be the primary factors influencing the classification output decision. When compared to F7, F5, and F11, the other variables, such as F1, F12, and F6, have lower attributions. According to the attribution assessment, F7, F5, F11, F12, and F10 are the factors that positively contribute to the choice, implying that they are the ones that push the classification closer towards C3. F1, F6, F3, F4, and F9, on the other hand, are the top negative factors that sway the choice somewhat toward the other labels, C1 and C2. In fact, it is because of these negative variables that the classifier presents the probabilities across the C2 and C1.',\n",
       " 'With a prediction probability of around 82.06 percent, the algorithm predicts class C1. In the aforementioned prediction judgment, F8, F10, F16, and F3 are all important. The top positively contributing features supporting the C1 prediction are F8, F10, and F3, while F16 is pushing the final prediction away. F12 also has a positive impact on the categorization, but F9 has a negative impact and finally, F17, F5, F19, and F7 have very little influence on the algorithm among the features, when picking the most appropriate label in this case.',\n",
       " 'For this test case, the model predicts C2 with 99.93% certainty and what this means is that there is only 0.07% chance that C1 could be the right one. The features with the highest impact are F4, F5, F13, and F6, which are all shown to contribute positively to the prediction decision mentioned above. While F3 and F1 support the prediction, F10 is the feature with the strongest negative support for the prediction. Of the features with a small impact, namely F9, F8, F2, F12, F11, and F7, only F8 and F12 negatively support the prediction while the others positively support it.',\n",
       " \"This model trained on eleven attributes predicts class label C2 for this case with a confidence level equal to 54.21%. This suggests that the likelihood of C1 being the correct label is 45.79%. The classification decision above is mainly based on the influence of the features F10, F8, F1, and F4. The most relevant features are the negative features, F10, F8, and F1. These features are regarded as negative features given that their values are shifting the prediction decision in the direction of C1. The positive attributes are F4, F6, F3, F7, and F11, supporting the model's prediction for this case.\",\n",
       " 'With a moderate confidence level of 67.95%, the model predicts C2 for the case under consideration, but it is important to consider the fact that there is a 32.05% chance that C1 could be the correct label instead. The most influential variables resulting in the aforementioned classification decision are F9, F5, and F7. While F9 and F5 have negative contributions towards the C2 prediction; favouring the assignment of C1 instead, F7 is the top positive contributing feature. F8, F3, and F10 had a small positive effect on prediction, whereas F1 had a smaller negative effect. Finally, F2 is the least relevant variable, and therefore, its negative attribution has no significant influence on the model with respect to the given case.',\n",
       " 'As per the classification algorithm, the most appropriate label for the given case is C2 because its prediction likelihood is 99.45%, whereas that of C1 is only 0.55%.  For the classification or prediction assertion above, the most important variables are F11, F3, and F4, while the least influential variables are F8, F2, F7, and F9. Regarding the direction of influence of the variables, the ones with positive contributions to assigning label C2 are F11, F3, F1, F7, and F9 which in fact increase the odds of C2 being the correct label. Finally, decreasing the odds of C2 and supporting C1 are mainly the values of the variables F4, F5, and F6.',\n",
       " 'C2 is the label predicted by the classification model employed and looking at the prediction probabilities, it valid to concluded that the model is very certain about the selected label. The features considered most relevant by the model for the above decision are F1, F3, F11, and F4, while those with the least consideration are F10, F9, and F12. On the basis of the analysis, majority of the input features positively affirm the prediction for this case; therefore, it is not surprising that the model chose the C2 label and the positive features include F1, F11, F4, F8, F6, F2, and F5. The three negative features that moderately bias the labelling decision towards C1 are F13, F3, and F7.',\n",
       " \"The confidence level for the prediction made for the given case is 71.57%. F7 has a significant impact on the outcome in the negative. The values F10, F2, F8, F4, F1, F9, and F3 all have a positive impact on the results, but they are still less than the effects of F7. The analysis shows that F7 has the highest impact on the model's prediction decision here,   it has an overwhelmingly negative effect. F2, F8, F4, and F1 have a positive effect on the model's prediction. Because of the strength of the F7 feature, all other features have little effect on the outcome. In addition, the uncertainty in the prediction could be attributed to the pull of F7, which drives the model to predict an alternative label.\",\n",
       " \"53.78% and 46.22%, respectively, are the chance or likelihood of any of the classes C2, and C1 being the appropriate label for the case given here. As a result, it's safe to say that C2 is the most likely label for this situation and F1 is identified as the most influential feature whereas F2, F4, and F5 have very low contributions to the decision made by the classification algorithm with respect to the given case. In addition, F8, F3, F10, F9, F6, and F7 have moderate contributions higher than F2, F4, and F5 but lower than F1. Despite the strong positive influence of F1 and F3 supporting the assignment of C2, the negative influence of F8, F10, F9, F7, and F5 shift the classification judgment fairly towards the C1 label which explains the 46.22% likelihood.\",\n",
       " \"The output decision for the provided data is C1, with a very high confidence level, based on the output prediction probabilities across the two classes since C2 has a probability of around 0.00%. F9, F4, and F10 are the most influential factors in the above-mentioned label assignment, however F7 and F5 are the least influential. The unusually high degree of confidence associated with the classification choice in this case might be attributable to the fact that the bulk of the input variables exhibit attributions that improve the model's responsiveness towards label C1. F3, F6, and F7 have only the negative contributions, attempting to persuade the model to classify this case as C2. To cut a long story short, the joint contribution of the negative variables is quite low in comparison to that of the positive variables, resulting in the model's certainty in the decision above.\",\n",
       " \"When given the task of labelling the given case one of the possible labels, C2 and C1, the model assigns C2 as the most likely correct label, with a confidence level of roughly 99.90%. This degree of confidence indicates that the likelihood of C1 being the right designation is merely 0.10%. According to the attribution analysis, each variable has a distinct degree of effect or contribution to the model's arriving at the above-mentioned classification. F3, F4, F1, and F7 are the features accounting for the model's extremely high confidence in the assigned label. In fact, the only input variables having a negative impact are also the least relevant ones, F2 and F5.\",\n",
       " 'According to the classification model employed here, the most probable label for the given case is C2 with a confidence level equal to 98.97%.  Per the attributions analysis, F4 and F8 are the most significant and influential features driving label selection. The least ranked features are F6 and F2, while F1, F7, F5, and F3 have moderate contributions. Negatively supporting the above classification output are F8, F5, and F3, pushing the model to assign the alternative label. However, given the fact that the prediction probability of C1 is only 1.03%, it can be concluded that the joint positive influence of F4, F1, F7, F6, and F2 strongly drives the model to label the case as C2 instead of C1.',\n",
       " \"The classifier is quite sure that the right label for the data given is C2 based on the influence of variables such as F7, F2, F5, and F11. There is a 10.0% chance that the correct label is C1 and per the attributions examination conducted, the bulk of the traits contribute positively, with only three contributing negatively. The negative variables are F11, F8, and F9, which reduce the classifier's preference for C2. F7, F2, and F5 are notable positive variables that boost the classifier's response to outputting C2 rather than C1. All in all, the classifier's confidence in this prediction may be attributed to the fact that the negative variables only have a minor influence on the prediction choice here.\",\n",
       " 'C2 was assigned to the given case by the classifier with a likelihood of 93.32%, leaving thhe likelihood of the C1 equal to only 6.68%. The most influential features were F3, F5, and F14. The remaining features with non-zero attributions are F20, F1, F12, F7, F33, F18, F10, F17, F27, F26, F19, F32, F11, F16, F31, and finally F21. F3 and F5 were highly influential in the positive direction, increasing the odds of the predicted label being correct, whereas F14 had a negative impact, driving the prediction in favour of a different label. Furthermore, F20 had a positive impact on the prediction, whereas F1 and F12 negatively influenced the prediction. Finally, the features that we can say have no impact at all on the prediction made here are as follows: F4, F22, F24, F9, and F15.',\n",
       " 'With moderately high confidence, the classifier indicates that the most probable label for the given data is C1 with only just a 21.80% chance that it could be C2. The main driving features for the above classification or prediction decision are F15 and F14. The remaining features such as F9, F2, F13, and F19 have moderate to low influence on the above decision. Inspecting the attributions of the the input features showed that the ones with negative impact or contribution are F9, F19, F5, F10, and F18. From the attributions, we can see that the remaining features have positive contributions or influence and as a matter of fact, the certainty of the classifier for this classification can be attributed mainly to the strong positive contributions of F15 and F14 coupled with the contributions of the other positive features such as F2, F13, F11, and F17.',\n",
       " 'Given the fact that the likelihood of C1 being the correct label for the case under consideration is only 36.34%, the model assigns the label C2. The prediction decision between the two classes is highly based on the values of the features F3, F6, F14, and F10, whereas  those with the least attributions or contributions regarding this label assignment are F5 and F8. Among the top influential features, F3 and F6 have very strong positive contributions, increasing the probability of the label C2, while the value of F14 value suggests the other label, C1, could be the true label. This pull or shift towards label C1 is further supported by the values of F11, F4, F1, F13, F5, and F16. Conversely, the remaining features, together with F3 and F6, positively encourage the prediction of C2.',\n",
       " 'C3, out of the three potential classes, is the the label assigned with a high probability of 50.0%. However, the classifier indicates that C2 and C1 are equally likely, with a predicted probability of 25.0%. The aforementioned judgement is mostly based on the variables of the given case. The variables F11, F7, and F1 are shown to be the main factors resulting in the classification output decision. The remaining variables, such as F9, F12, and F8, have lower attributions compared to F11, F7, and F1. The attribution analysis also indicated that F11, F7, F1, F12, and F10 are the variables that positively contribute to the decision, meaning they are the ones that shift the classification higher towards C3. On the contrary, F9, F8, F3, F6, and F5 are the top negative variables that steer the decision slightly towards the other labels, C2 and C1. In fact, it is because of these negative variables that the classifier indicates the probabilities across the C1 and C2.',\n",
       " \"The chances of selecting the correct label from one of the possible labels C1, C2, and C3 are 18.51%, 5.86%, and 75.63%, respectively. As a result, it can be deduced that the classifier's anticipated label in this situation is C3. The values of the input features were used as the basis to make the aforementioned prediction judgments. Some of these features have values that positively support the assigned label, while others have values that contradict the classifier's decision, driving it toward one of the other two labels. F9 is the most influential feature, following which are the variables F10, F1, F7, and F2, enumerated according to their respective relevance to the aforementioned label selection. F9, F7, and F2 are positive features that increase the classifier's response towards generating the C3 label, but F10 and F1 are negative features, lowering the odds of C3 being the correct label. F5, F6, F11, F12, and F3 are features that have a moderate influence on the classifier in this case, while F4 and F8  have only a marginal impact. F10, F1, F11, and F8 are the features that have values supporting the assignment of any of the other labels, while the rest favour the C3 prediction, therefore, the predicted probabilities across labels is unsurprising. Furthermore, the predicted likelihood of C3 is higher than all the other labels which is attributed to the fact that the positive features' combined impact is bigger than negative features' combined impact.\",\n",
       " 'In this case, the model expects C1 to be a label since the probability that the label is the alternative class C2 is only 1.94%. This means that the model has a lot of confidence in the selected label, C1. F34 and F29 are the two most important prediction variables positively controlling the assignment of C1 in this case. Other variables that contributed positively to this prediction included F18, F3, F14, F37, and F23. On the other hand, the values F4, F39, F27, and F21 constitute a feature set with a negative impact on the above prediction decision. However, the above features have little effect on the model compared to the F38, F23, F3, and F29, which may explain why the model is confident that the true label is probably C1. Finally, for the case under consideration, F7, F16, F41, F12, F42, and F10 are some of the features, with practically no effect on the prediction decisions of the model, hence they can be considered negligible to the classification here.',\n",
       " \"The following classification assertions are based on the information provided on the case under consideration. The most probable or likely label judged by the classifier is C1 since its prediction probability is 60.0% compared to the 40.0% of C2. The influence of the features on the classifier's decision here can be ranked in the order F7, F10, F2, F9, F4, F5, F6, F8, F3, F1, F11. In fact, with the exception of F11, all the features are shown to have attributions, resulting in the predicted probabilities across the labels. The F7, F10, F2, and F3 have negative contributions, leading to the classifier's confidence in the validity of the C1 label and this is because they are the features that support labelling the case as C2. However, the positive features F9, F4, F5, F6, F8, and F1 tip the scales higher in favour of C1. Since the most influential features F7, F10, and F2 have negative contributions, it is not surprising that the classifier has the probability of C2 equal to just about 40.0%.\",\n",
       " \"At a confidence level of 100.0%, the model labels this case as C1 and what this indicate is that there is no chance for C2 to be the correct label given the values of the input features. The above classification decision can be attributed to values for features such as F4, F3, F7, F21, F20, and F8. For this C1 prediction, the most important features are F4, F3, and F7. These are all positive features, meaning they strongly support the model's decision with respect to the case under consideration and a further push towards the assigned label is offered by the contributions of the other positive features such as F20, F8, F12, and F5. On the other hand, shifting the decision in the opposite direction are the negative features such as F21, F22, F19, F1, and F10. However, compared to F4, F3, and F7, the joint influence of the negative features mentioned above is weak. Finally, the values of the features F13 and F2, both with almost zero attributions, are not relevant when it comes to deciding the correct label for this case.\",\n",
       " 'For the case under consideration here, there is a 70.83% probability that the true label is C1 and what this means is that there is also a 29.71% chance that C2 could be the correct label. Among the features, the top two most impactful are F1 and F4. The next features, ranked in order of the magnitude of their respective attribution are F5, F7, F8, F9, F6, F3, and F2. Out of the nine features, only three of them have values pushing for the prediction of label C2 while the rest are referred to as positive features given that their values motivate the prediction of class C1. The three attributes with the negative impact, shifting the prediction decision away from C1, are F4, F5, and F7. The collective influence of positive features is higher than that of negative features F4, F5, and F7.',\n",
       " 'The algorithm labels the data given as C1 and the prediction probabilities across the possible labels C1 and C2, respectively, are 51.39% and 48.61%. Judging based on the prediction probabilities, the algorithm shows signs of uncertainty in the above decision. F7, F8, F6, and F1 are the primary contributors to the classification verdict here. The contributions of F1, F2, and F3 are moderate, while those of F9, F5, and F8 are lower compared to the other variables. Positively supporting the classification are F7, F6, F5, and F4, while all the remaining variables have a negative impact that decreases the probability of C1 being the correct label. F8, F1, and F2 are negative variables that can be blamed for the uncertainty in the classification decision being made here.',\n",
       " 'The label assignment decision is solely based on the values of the different input features passed to the classification algorithm since the values of these features are used as the basis to make the prediction judgments. The likelihood of any of the classes C2 and C1 being the correct label is 76.26% and 23.74%, respectively, therefore, it is valid to assert that the true label for this case is C2. From the attribution analysis, F14, F6, and F11 have the highest contribution to the decision, whilst F16 and F13 are the least relevant features. In between these two ends are the moderately influential features, such as F8, F10, F9, F15, and F1. Furthermore, the negative features F6, F9, F3, F7, F2, F4, and F16 can be blamed for the fact that the algorithm is not 100.0% certain about the labelling decision and this mainly because the negative features contribute towards choosing C1 instead of C2. Conversely, the positive features such as F14, F11, F8, F10, F15, F1, and F5 are the ones driving the decision higher towards C2.',\n",
       " 'Here the classifier labels the given case as C1 with a moderately high confidence level. Specifically, the prediction likelihood of class C2 is only 21.67%. The main drivers for the classification above are F26, F36, F18, and F34. Among these top features, F26 and F36 have the most significant influence on the classification outcome, and they happen to have positive contributions, increasing the likelihood of class C1. On the other hand, the F34, F18, and F14 have a moderate negative contribution, reducing the odds of a C1 prediction. F8, F19, F16, and F39 are other notable positive features, while F20, F11, F33, and F1 are notable negative features. However, the classifier did not take into account all of the input features when arriving at the above-mentioned classification verdict; the features including F41, F37, and F30 are deemed irrelevant. To summarise, considering the attributions of influential features such as F26, F36, and F34, it is evident why the classifier is quite certain that C1 is the most probable label for the given case.',\n",
       " 'The label assigned in this case by the classifier is C3, with a moderately high prediction confidence of 66.11%. Since the confidence level with respect to this C3 is not 100.0%, it is possible that one of the other labels is the true or correct label, and C2 is the next most likely label. The input variables F4, F5, F3, and F6 have a significant impact on the abovementioned prediction judgement. The value of features F4, F3, F9, and F11 contributes positively to the C3 label, instead of the other labels. F5, F6, F1, and F2 are the variables having a contradictory influence, shifting the final decision in the direction of the other labels. The remaining positive variables are F8, F12, F10, and F7. Of all the predictors, the ones that contributed the least to the prediction included F1, F10, F2, and F7. In summary, given the attributions of the predictors, it is clear why the classifier indicates that C3 is the correct class in this scenario.',\n",
       " 'The classification assertions arrived here are mainly based on the influence and contributions of the different input variables. The prediction probabilities across the four possible classes C3, C2, C4, and C1 are 0.05%, 0.04%, 0.47%, and 99.45%, respectively.  Therefore according to the classifier, the most likely class label for the case under investigation is C1 and it is quite sure that neither C4 nor C3 nor C2  is the true label here. The influence of F11 is shown to be the major contributing factor resulting in the prediction decision made by the classifier and the contributions of the remaining features such as F6, F15, F12, and F8 are moderately low compared to that of F11. The strong positive influence of F11 coupled with other positive features such as F12, F20, and F8 can explain the very high confidence level in the prediction decision. On the flip-side, the input features F6, F15, and F17 are considered negatives since their attributions marginally reduce the prediction probability of the C1 label.',\n",
       " 'The prediction likelihood of class C2 is 84.87%, making it the most probable label for the given case. When making the above prediction, the most relevant features considered are F2, F7, F1, and F3. Conversely, F10, F11, and F9 are the least influential features, with their values receiving little consideration from the model regarding this classification. Assessing the direction of influence or contribution of the features suggest that there is a split between the number of features with a negative influence and those with a positive influence. However, only two of the negative features, F7 and F6, have a somewhat high influence; the others , F5, F10, F11, and F8, have a lower negative influence. To put it concisely, the combined influence of the positive features, such as F2, F3, F4, F12, and F1, outweighs that of all the negative features combined, therefore, it is entirely plausible to see such confidence level of the model for the classification here.',\n",
       " 'With respect to the given case, the most probable label for the given case is C2, with a 99.81% chance of being the correct label, therefore the probability of C1 is only 0.19% for this case. Among the input variables, only four features are shown to have a negative influence on the classification decision above: F3, F14, F12, and F5 since their contributions to the decision only favour labelling the given case as C1 instead. On the flip side, pushing the classification strongly towards C2 are the features F8, F6, F7, and F1 explaining the very high confidence in the choice of label assigned here. ',\n",
       " 'For a particular case, the model predicted the class designation C1 with 75.50% confidence. Based on the attributions analysis, the feature that had the biggest impact on the final labelling decision were the F6 and F1, which happened to strongly support the assignment of label C1. Contributing differently to F6, the feature F3 is the top negative feature, reducing the odds that C1 is the correct label. F7, F4, F3, and F2 have similar influences on the model in terms of the magnitude of their contributions or attributions, however, the directions of their respective effects are different: the features F7 and F4 positively support the model, driving the prediction towards class C1, while F3 and F2 work against it. F9, F5, and F8 are features that have little effect on the model when assigning the label for the given case, and all of them negatively contributed to the C1 class selection. Among all the features with little contribution to the prediction verdict above, F8 is the least relevant.',\n",
       " 'The model reveals that C4 and C3 each has a zero prediction probability, while C1 has a 3.85%. This indicates that C2 is the most likely label for the present context with approximately 96.15% certainty. F7, F11, and F20 are the most important elements driving the above classification, whereas F5, F15, F14, F16, and F13 are the least important. The intermediate elements, which comprise F6, F8, and F12, have varied degrees of influence, ranging from moderate to low. F6 is the only with a negative contribution among the top influential features, F7, F11, F20, F6, and F8, skewing the forecast slightly towards a different possible label. Furthermore, the top two positive elements, F11 and F7, have a greater effect than the sum of all the negative ones.',\n",
       " \"The prediction is that class label C2 is very likely the correct label, given that the associated confidence level is 99.93%. The features F13, F6, and F4 appear to have very smaller or little impact on the prediction of C2 compared to F1, F5, F10, F12, and F9, according to the attribution analysis. F1 and F5 are the features with the highest impact on the model's output prediction verdict above and fortunately the values of these features positively support the C2 classification verdict. Other positive features increasing the odds in favour of C2 include F10, F12, F7, and F8. On the contrarily, the feature F9 negatively influences the model's prediction of C2, shifting the verdict in the opposite direction. It is important to note that, only the features F9, F2, and F4 have negative attributions, while all the remaining ones have positive attributions. The joint positive attribution outweighs the negative attributions from F9, F2, and F4.\",\n",
       " \"For the given case, the model predicted the class label C2 with a certainty of around 75.50%. By far, the feature with the most impact on the final classification was F4, which positively supports the decision. Feature F7 was the feature that contributed the most to pushing away the classification decision from C2, that is, they are decreasing the likelihood of C2 being the correct label. F1, F5, F7, and F8 all had a similar impact on the classification. However, the direction of influence is different, with features F1 and F5 pushing the model's decision to class C2 and features F7 and F8 doing the opposite. F9, F3, and F6 are the features that had closer to negligible impact on the final classification, all of which had a negative contribution towards class C2.\",\n",
       " 'Judging based on the values of the input features, a decision is made by the classifier to label the given data as C2 with a prediction confidence equal to 84.90%. The major influential features resulting in the classification here are F33, F3, F11, and F29. F33 and F3 are identified as the most negative features, with contributions that lead to a decrease in the classification confidence of label C2. F29 and F11, on the other hand, are the top positive features, leading the classifier to label the case as C2. Other notable negative features are F6, F12, and F38 while other notable positives are F35, F20, F2, and F43. Unlike all those mentioned above, F31, F41, F4, and F10 are among the many irrelevant features with negligible contributions to the classification decision here.',\n",
       " \"The prediction model predicts C2 for the case under consideration since the likelihood of C1 which is equal to 30.05%, is lower than that of C2 and this verdict came about mainly based on the values of the input features passed to the model. F10, F7, and F1 are identified as the most influential features with higher impact on the model's labelling decision here and among them F10 and F7 have negative contributions decreasing the model's response towards the assigned label. Furthermore, F1, F6, and F3 have a positive impact on the model and in effect pushes the decision higher towards C2, while F4, F9, and F2 have identical direction of impact as that of F7 and F10. Finally, F8 is the least relevant feature, therefore, its negative attribution has little effect on the model in this case and also the positive influence of F5 further supports the  assigned label.\",\n",
       " 'The model assigned the label C2 to the given instance since its associated likelihood is far higher than C1. The most relevant features controlling the prediction decision above are F1, F3, and F4. The less relevant ones include F5, F9, and F7. The majority of the features have values, swinging the verdict towards the other class, C1. The only features increasing the likelihood or probability of C2 being the correct label are F1, F8, and F9. Given that only few features positively contribute to arriving at the C2 prediction, it is very strange that the model has 100.0% confidence in its prediction for the selected instance.',\n",
       " 'There is an 80.0% chance that the true label for the given case is C1. Nine out of twenty features have a positive impact. Most features have a moderately low positive or negative impact, with the exception of F2, F7, and F16 and it appears as if F2 has an extremely negative impact, while F7 and F16 have the greater positive impacts. F5 has positive impacts, whereas the attributions of the features F1 and F9 are negatives. The least important features include F19, F8, F15, F4, F10, F3, and F20 with varying smaller effects.',\n",
       " \"There is disagreement about which label is acceptable for the case under consideration since the model is unsure which of the two labels is right. The confusion in the aforementioned classification may be attributable only to the effect of F18. F18 is by far the most influential variable, with a negative contribution that reduces the chance of label C2 being the correct label in the given case substantially; supporting the that case should be labelled as C1. Compared to the influence of F18, the remaining variables have a moderate to low effect on the classification decision made here for the case under consideration. F17, F28, and F24 are notable moderately key variables, with positive contributions boosting the likelihood of label C2. F7, F14, F9, F12, F30, F29, F4, F13, F11, and F8 are not among the features demonstrated to contribute to the classification above; since they have very insignificant impact on the model's conclusion here.\",\n",
       " 'For the case under consideration, the probability of C2 being the correct label is only 12.50%, implying that there is an 87.50% chance that C1 is the true label. The decision above was arrived at mainly based on the values of the following variables F3, F2, and F1. Among these top variables, only F3 has a very strong positive impact on the model, increasing the likelihood of C1 prediction. The most important variables decreasing the prediction are F2 and F1 and  the remaining two shifting the verdict away from C1 are F9 and F6. F5 and F4 are the lowest-ranked variables, less important to the prediction made here since they have a moderately low positive impact on the model.',\n",
       " \"With a high degree of confidence, close to 100 percent, the classifier's final label choice for the given case is C1 due to the predicted probability distribution between the class labels. Analysis of the attributions of the input features indicates that the most relevant features driving the classification above are F1, F9, F4, and F5, whereas F10 and F2 are shown to have little contribution to the decision.  Furthermore, only four of the features have a negative influence, swinging the classifier decision in this case towards the C2 label and  they are F9, F14, F10, and F2. However, except for F9, the contribution of the other negative features is very low when compared with the top positive features such as F4, F15, and F5.\",\n",
       " 'Based on the values of the six input features, the model assigned the label C2 to the given case with a higher degree of confidence and according to the model used here, there is a near-zero chance that the label could be C1. Influencing the prediction assessment above are the top four features, F3, F4, and F2, whereas, the least significant feature here is F6. Among the input features, only two, F4 and F6, contradict the label assignment decision above since their values are shifting the label decision in the C1 direction. However, the joint attribution of these features is outweighed by the remaining four features, F3, F2, F5, and F1. This could explain why the model is very certain about the C2 prediction made for the case under consideration.',\n",
       " \"The most likely label for the given data is C1 and this decision is as the result of the variables passed to the classifier. F7, F17, F19, and F14 are the primary contributors to the aforementioned prediction output. F6, F5, F26, F15, F20, and F8, on the other hand, make insignificant contributions to the classifier labelling the given example. F25 and F21, as well as F13, F18, have a moderate influence on the label selection. The classifier's confidence in the label decision above might be explained away by comparing the greater positive attributions of F21, F25, F7, and F14 to the negative attributions of F13, F10, F19, F16, F17, and F22.\",\n",
       " 'Because the prediction algorithm outputs reveal that the likelihood of C1 being the correct label is equal to 93.02%; hence, there is only a little possibility that the true label for the provided data instance is either of the other labels, C3, C2, and C4. The variables F6, F12, F11, and F2 are the most crucial ones driving the label assignment conclusion above, whereas F1, F7, and F3 are the least vital ones. Taking into account the direction of effect of each input feature, as demonstrated by the attribution analysis, it is possible to deduce that the positive features driving the prediction upward towards C1 are F6, F4, F11, F5, F2, F12, and F7. The negative contributions of F8, F10, F3, F1, and F9 are ascribed to the marginal uncertainty in the expected output decision. When the predicted probabilities across the classes are considered, it is possible to infer that the combined positive contribution outranks the negative contributions; therefore, the algorithm is certain that C1 is the real label.',\n",
       " 'Based mainly on the values of the input variables F2, F7, F44, and F8, the predictor classifies the case as C1 with a 90.15% labelling confidence level, indicating that there is only a 9.85% probability that the right label could be C2. Variables that contribute positively to the prediction verdict include F2, F33, F39, and F8. The values of these variables increase the odds of the model labelling the given case as C1. On the other hand, F44, F7, F11, and F36 are the variables influencing the prediction decision in favour of C2 instead of C1. Simply put, the values of these negative variables contradict the label assigned here and finally, the model places little emphasis on the values of features such as F17, F21, F41, and F18 when determining the correct label in this instance, as they have nearly zero influence.',\n",
       " 'The classification output decision is based solely on the information supplied to the model and it predicts class C4 with a higher confidence level, equal to 94.10%, indicating the model is very confident that the correct label for the given case is not either class C3 or class C1 or class C2. The classification output decision with regards to the given case boils down to the values of the features F5, F4, F6, and F3, which are shown to have the most significant influence on the model. Among these relevant features, only F5, F3, and F4 have a positive impact, increasing the response towards labelling the case as C4. Conversely, the remaining ones, F6 and F1, have negative attributions, decreasing the odds of the assigned label. Finally, feature F2 has little impact on this prediction among the features since its value received little consideration from the model.',\n",
       " \"The model predicts that this case is likely C2 with a confidence level equal to 66.80%, meaning there is a 33.20% chance that it could be C1 instead. According to the analysis for this case under consideration, the most relevant features considered by the model are F5, F2, F1, F8, and F7, however, the least relevant features are F9 and F4. The F1, F8, F7, and F6 can be regarded as positively supporting features given that they increase the model's response in favour of the prediction conclusion above. In contrast, the F5, F2, and F3 are the features supporting the prediction of the alternative or other class label C1. Even though only a small number of features support the prediction of C1, their collective or joint influence is enough to upset the joint influence of the other features, leading to the uncertainty of the C2 prediction.\",\n",
       " \"Based on the values of the input variables resulting in the predicted likelihoods across the classes, the classification algorithm is confident that the right label for the provided data is C1. According to the algorithm, there is no possibility that C2 is the correct label. However, the attributions of F13, F11, F3, and F9 indicate that the correct label might be C2 rather than C1. The top four variables are F12, F7, F6, and F8, all of which have a positive influence on the algorithm's prediction output, hence confirming the C1 classification. This conclusion is further supported by the contributions of F2, F10, F1, F5, and F4, which are also positive variables.\",\n",
       " 'With a higher degree of confidence, the classifier assigns the label C2 due to the fact that there is a close to zero chance that C1 is the label. The confidence level with respect to this classification output is largely due to the strong positive influence of F2. However, decreasing the probability that C2 is the true label are the negative features F5, F7, F4, F3, F6, and F9. Furthermore, F1 and F8 also increase the likelihood of C2 being the true label. In conclusion, the joint impact of the negative features is very weak compared to the positive features, hence the strong driving force of the classifier to assign the chosen label, C2.',\n",
       " \"According to the input variables, there is a 99.81% chance that C2 is the correct label for the given data instance, with a prediction probability of the alternative label, C1, equal to 0.19% which shows that there is little chance that C1 is the true label. F2, F9, and F6 are the top contributing features leading to the classification decision here. On the contrary, the F7, F12, and F1 are the least relevant features. The input features with moderate influence are F4, F10, F3, F5, F14, F8, and F13. Even though the different features have some level of influence on the classification, not all of them positively contribute. Actually, F3, F11, F13, and F12 have negative attributions, decreasing the classifier's response towards assigning C2; however, the joint influence of these features is outweighed by the positive attributions of F2, F9, F6, F4, and F10.\",\n",
       " 'With a confidence level equal to 81.43%, the classification algorithm labels the given data as C2, however, there is about an 18.57% chance that C1 could be the right label.  The assignment of C2 to the given case is mainly based on the positive influence and contribution of input features F7, F1, and F6. Furthermore, the majority of the remaining input features have positive contributions, further increasing the predictability of label C2. F5, F10, F4, and F12 are the features with negative contributions, shifting the decision towards C1 instead of C2. Summarizing, comparing the attributions of the negative features to even those of the top three positive features explains why the algorithm is certain that C2 is the right label here.',\n",
       " \"The classification model employed here is very certain that the correct label is C1, implying that there is a near-zero chance that C2 is the label. The top six variables with the most influence on the prediction are all shifting the prediction in favour of C1. This might explain why the model is very certain about the C1 label and these top positive attributes are F14, F19, F29, F6, F38, and F20. F12, F36, F16, F27, and F26 all have moderately low-negative contributions, weakly swinging the direction of the model's decision towards C2. Finally, the decision to label the case as C1 is marginally supported by F2 and F33, whereas F13, F5, and F41 suggest that C2 could be the true label. In conclusion, the very high confidence level with regard to this prediction can be explained away by considering the very strong positive influence of F14, F19, F6, and F29.\",\n",
       " 'For the given case, the model generates the label C2 instead of C1, since C2 has a higher prediction likelihood than C1. According to the attribution graph shown, F7, and F9 are the most influential variables, resulting in the classification verdict above. F1, F4, and F6, on the other hand, are the least important variables considered by the model. F8, F2, F5, and F3 are shown to have a moderate influence on the classification made here. To sum up, with F1, F4, and F6 being the only variables contributing negatively, it is foreseeable why the model is quite certain that C1 is not the correct label for the given case.',\n",
       " 'Per the classifier, the most probable class with a very high confidence level is C1 mainly because the probability that C2 is the correct label is zero. From the attributions analysis, all the inputs are shown to contribute to or influence the above classification. The ranking of the features from the least important to the most important based on their degree of influence is as follows: F3, F5, F8, F2, F6, F7, F1, F4. Simply looking at the attributions of the input features, it is obvious why the classifier is very confident that C2 is not the correct label for the given All the features have positive contributions, resulting in a strong push towards C1.',\n",
       " 'The model predicted the C3 class for the test case with a very high degree of confidence. F2 is the only feature contributing against the prediction of the C3 class, while F3 and F5 contributed positively towards the prediction of C3. In decreasing order, F1, F6 and F4 were the three features with the least positive impact on the prediction of C3. Overall, given that only F2 has negative influence on the decision, it is not surprising to see the associated confidence level of the assigned label.',\n",
       " 'Per the classification algorithm, the most probable class is C2 since the prediction probabilities indicate there is little to no chance that the correct label for the given data instance is any of the following classes: C1, C3, and C4. This labelling is primarily owing to the roles that the features F12, F13, and F3 performed. On the lower end of the spectrum are the input features F6, F17, F10, and F9, which are demonstrated to be less essential for this labelling assignment task. Finally, only F19 and F11 are features having a negative effect, reducing the likelihood of C2 being the accurate classification here.',\n",
       " \"The predicted label is C1 at a confidence level of 92.11%, insinuating that there is a 7.89% chance that the label could be C2. In this case, the feature with the most significant influence on the model's decision is F11, with a very strong positive contribution in support of the C1 prediction. The next set of features with moderately high impact is F4, F6, F10, F2, and F7. Among this set, only F7 and F4 have a negative influence in support of label C2. Finally, on the lower end, the values of F8, F5, and F12 are deemed less important by the model when labelling this case.\",\n",
       " \"Taking into account the values of the input features, the prediction model's output for the case under consideration is C2. Given that there is a 27.27% probability that it could be C1, this labelling decision is not 100.0% certain. For the case under consideration, the label assignment is mainly due to the values of F1, F8, F5, and F10. F10 is identified as the most important or relevant, while F9 is considered the least important, since its contribution to the model is only marginal. In terms of the influence direction of each feature, F10 and F8 have a very strong positive contribution, driving the prediction higher toward the C2 class followed by F1, F5, and F6 all with moderately positive influence, whereas F9 has a negligible positive impact on the model in this case. Finally, for this case, F4, F2, F7, and F3 all have a negative impact on the prediction verdict, however, their pull or influence is not enough to transfer predictions in the direction of another class label, C1. \",\n",
       " 'The prediction output decision by the model is that the likelihood of label C2 is 94.15% and that of class C1 is only around 5.85%, meaning the model is certain that C2 is likely the true label for the given case. First of all, the classification is performed with negligible contributions from the variables F23, F30, F15, F8, and F12 since their attributions are very close to zero. However, examination or inspection of the attributions of the different variables reveals that F14, F25, F7, F13, and F16 are the highly influential ones driving the predicted probabilities across the classes. In addition, the decision about the correct label for this case is moderately influenced by the values of F19, F3, F26, F24, F22, F1, and F2. In terms of the direction of influence or contributions of the variables, F14, F7, F13, F3, and F26 are the top positive variables, encouraging the predicted output to be equal to C2. Pushing the decision towards the C2 label and further away from C1 are the contriutions of the variables F22, F1, F18, and F10. Finally, the 5.85% likelihood of C1 can be attributed to the negative contributions of the top negative variables F25, F16, F10, F24, and F2.',\n",
       " 'The most probable label, according to the classifier for the given data, is C1, which happens to have a higher predicted probability than that of C2.  The major players in the above prediction output are F18, F10, F4, and F16. Conversely, F23, F12, F2, F15, F6, and F17 have negligible contributions when it comes to the classifier labelling the given case. Features such as F11, F14, F21, and F5 have a moderate influence on the decision.  Comparing the stronger positive attributions of F10, F18, F14, and F21 to the negative attributions of F4, F16, F11, F19, F3, and F20 could explain why the classifier is quite confident in the label choice above.',\n",
       " \"The most likely label is C4 since there is a 30.83% chance it could be C1, a 35.74% chance it could be C4, and a 33.42% chance it could be C3. Therefore, the correct label is not C2, which the model is very certain about. The above decision is primarily controlled by the values F1, F6, F4, and F5 which are shown to have positive influences that support the model's classification judgement here. In contrast, the remaining features F2 and F3 negatively support the classification decision, decreasing the chances of C4 being the correct label. In view of the fact that the probability distributions across the classes, we can conclude that the model is very uncertain about which label is appropriate for the given data instance and the features F2 and F3 should be blamed for this.\",\n",
       " \"Because the prediction probability of C2 is equal to 0.0%, the presented case is labelled as C1 with a very high level of confidence. For this classification scenario, the input features that have the greatest influence on the end outcome are F22, F20, F37, and F10. F16, F23, F24, F5, and F19 have a mild impact. However, because F4, F27, F1, and F18 have insignificant attribution values, they have little influence on the model's judgement. Among the top features, F22, F20, F37, and F10, only F22 and F37 exhibit negative attributions that favour the least likely class, C2, whereas F20 and F10 positively support the model's classification result for the provided data. Finally, only F3 and F14 positively contribute to the model's decision among the remaining significant features: F14, F28, F29, F3, F9, F31, and F12.\",\n",
       " 'Between the four possible classes, the label for this case is predicted as C4, with a 73.08% likelihood that this is correct. With a likelihood of about 26.92%, the next probable label is shown to be C1. The prediction assessment above is mainly based on the values of the features F19, F17, F4, F12, and F13. The strongest impact came from F19, followed by F4, F17, F13, and F12. The collective contributions of the positive features F19, F17, F1, and F14 far outweigh the contributions of the negative attributes F4, F13, F12, and F15. Of the twenty attributes, majority of them are shown to have values pushing the prediction towards one of the three other possible classes and as such, it is surprising to see that the model is not 100% confident in the C4 prediction. On the grounds that the likelihood of C4 being correct is 73.08%, we can conclude that the model is quite confident with its final decision for the case under consideration.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['narration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrs_w_names = [nums_to_names(narr, eval(c2s), eval(f2s))\n",
    "                        for narr, c2s, f2s\n",
    "                        in zip(dataset['test']['narration'],\n",
    "                            dataset['test']['class2name'],\n",
    "                            dataset['test']['ft_num2name'])]\n",
    "narrs_w_names.extend([nums_to_names(narr, eval(c2s), eval(f2s))\n",
    "                        for narr, c2s, f2s\n",
    "                        in zip(dataset['train']['narration'],\n",
    "                            dataset['train']['class2name'],\n",
    "                            dataset['train']['ft_num2name'])])\n",
    "narrs_w_names.extend([nums_to_names(narr, eval(c2s), eval(f2s))\n",
    "                        for narr, c2s, f2s\n",
    "                        in zip(dataset['validation']['narration'],\n",
    "                            dataset['validation']['class2name'],\n",
    "                            dataset['validation']['ft_num2name'])])\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(narrs_w_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "lens = [len(word_tokenize(narr)) for narr in narrs_w_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187.61194029850745"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.216585146179"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2466"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({ word for narr in narrs_w_names for word in word_tokenize(narr) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = set(dataset['train']['task_name'])\n",
    "tasks.update(dataset['test']['task_name'])\n",
    "tasks.update(dataset['validation']['task_name'])\n",
    "len(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.725\n",
      "3.3910728390879488\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter(dataset['train']['task_name'])\n",
    "cnt.update(dataset['test']['task_name'])\n",
    "cnt.update(dataset['validation']['task_name'])\n",
    "print(np.mean([t[1] for t in cnt.most_common()]))\n",
    "print(np.std([t[1] for t in cnt.most_common()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f53d471a03fb5b9741311ec5f82522ec5f217d64ed47634b801d3f5199a0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

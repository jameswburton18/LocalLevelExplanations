{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference Routine from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-04 10:35:52.375411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-04 10:35:52.977939: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-04 10:35:52.978003: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2023-01-04 10:35:52.978011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Global seed set to 453\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from src.datasetComposer import DatasetBuilder, composed_train_path, composed_test_path, compactComposer, test_path, train_path, test_path,setupTokenizer\n",
    "from src.inference_routine import InferenceGenerator\n",
    "from src.datasetHandlers import SmartCollator\n",
    "from src.model_utils import get_basic_model\n",
    "from src.trainerArgs import CustomTrainer, getTrainingArguments\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "\n",
    "iterative_gen = True\n",
    "composed_already = True\n",
    "\n",
    "# Define the parameters used to set up the models\n",
    "modeltype = 'iterative' if iterative_gen else 'normal'  # either baseline or 'earlyfusion'\n",
    "\n",
    "# either t5-small,t5-base, t5-large, facebook/bart-base, or facebook/bart-large\n",
    "modelbase = 't5-base' #'facebook/bart-base'\n",
    "\n",
    "# we will use the above variables to set up the folder to save our model\n",
    "pre_trained_model_name = modelbase.split(\n",
    "    '/')[1] if 'bart' in modelbase else modelbase\n",
    "\n",
    "# where the trained model will be saved\n",
    "output_path = 'TrainModels/' + modeltype + '/'+pre_trained_model_name#+'/'\n",
    "\n",
    "#tests = json.load(open(test_path,encoding='utf-8'))\n",
    "\n",
    "\n",
    "rand_seed = 453\n",
    "seed_everything(rand_seed)\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "arguments = train_arguments = {'output_dir': output_path,\n",
    "                               'warmup_ratio': 0.2,\n",
    "                               #'disable_tqdm':False,\n",
    "                               'per_device_train_batch_size': 8,\n",
    "                               'num_train_epochs': 4,\n",
    "                               'lr_scheduler_type': 'cosine',\n",
    "                               'learning_rate': 5e-5,\n",
    "                               'evaluation_strategy': 'steps',\n",
    "                               'logging_steps': 500,\n",
    "                               \n",
    "                               'seed': rand_seed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the modules from the inference routine\n",
    "from types import SimpleNamespace\n",
    "from src.inference_routine import NarratorUtils,ExplanationRecord,LocalLevelExplanationNarration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dont forget to call initialise_Model() before running any inference\n"
     ]
    }
   ],
   "source": [
    "\n",
    "narrator_utils = NarratorUtils(modelbase,output_path)\n",
    "\n",
    "# initialise the model\n",
    "classification_explanator = narrator_utils.initialise_Model()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import random\n",
    "# Example of input\n",
    "\n",
    "ml_task_name = 'Car Insurance Risk'\n",
    "\n",
    "prediction_probabilities = {'Low': 0.76, 'High': 0.24}\n",
    "# the features used to make the prediction\n",
    "feature_names = ['Height', 'Mar_status', 'cur_loc', 'nb_friends', 'last_trip']\n",
    "\n",
    "bcc = feature_names.copy()\n",
    "random.shuffle(bcc)\n",
    "\n",
    "# get the order and directions of influence from the explainable output from the XAI technique\n",
    "# the methods expects the keys ['explanation_order','positives','negatives','ignore']\n",
    "# 'positives' is the list of all the features with positive influence on the prediction decision and 'negatives' is the inverse.\n",
    "# 'ignore' is the list of features identified as having very limited contribution to the prediction decision\n",
    "\n",
    "attributions = {'explanation_order': ['Height', 'last_trip', 'cur_loc', 'nb_friends', 'Mar_status','Income'],\n",
    "                'positives': ['Height', 'last_trip', 'Mar_status'],\n",
    "                'negatives': ['cur_loc', 'nb_friends'],\n",
    "                'ignore': ['Income']\n",
    "                \n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to generate the texts via the iterative generation then we have to define the style\n",
    "# We want our output text to first table about the prediction output\n",
    "# step 1: talk about the feature order based on the attributions\n",
    "# step 2: talk about the features with positive contributions to the decision\n",
    "# step 3: ----- negative features\n",
    "# step 4: ------- features with limited influence\n",
    "# step 5: Make conclusion based on all the input information\n",
    "\n",
    "# this will instruct the narrator to follow our desired output style\n",
    "iterative_generation_steps = {'step 0': '',\n",
    "                              'step 1': attributions['explanation_order'],\n",
    "                              'step 2': attributions['positives'][:2],\n",
    "                              'step 3': attributions['negatives'][:1],\n",
    "                              'step 4': attributions['negatives'][1:] + attributions['positives'][2:],\n",
    "                              'step 5': attributions['ignore'],\n",
    "                              'step 6': '-'\n",
    "                              }\n",
    "\n",
    "\n",
    "full_text_generation_steps = {'step 0': '',\n",
    "                              'step 1': attributions['explanation_order'],\n",
    "                              }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ML model predicted the label : Low\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The model labels the given case as Low with a confidence level equal to 76.0%, while the other label,  High, has an uncertainty level of 24.10%. cur_loc, nb_friends, and Mar_status are shown to have a negative impact on the model's decision here. Height and last_trip have a positive impact on the model's decision, increasing the likelihood of the Low label.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_gen = True\n",
    "generation_instruction = iterative_generation_steps if iterative_gen else full_text_generation_steps\n",
    "# Process the explanation output and the text generation instruction\n",
    "exp_record = ExplanationRecord(ml_task_name,feature_names,prediction_probabilities, attributions,iterative_mode=True)\n",
    "processed = exp_record.setup_generation_steps(generation_instruction,)\n",
    "\n",
    "# the final bit \n",
    "iterativeGen =LocalLevelExplanationNarration(classification_explanator,narrator_utils,device,iterative_mode=True,)\n",
    "iterativeGen.generateTexts(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ML model predicted the label : Low\n",
      "All facts presented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"The model labels the given case as Low with a confidence level equal to 76.0%, while the other label,  High, has an uncertainty level of 24.10%. cur_loc, nb_friends, and Mar_status are shown to have a negative impact on the model's decision here.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterative_gen = False\n",
    "generation_instruction = iterative_generation_steps if iterative_gen else full_text_generation_steps\n",
    "# Process the explanation output and the text generation instruction\n",
    "exp_record = ExplanationRecord(ml_task_name,feature_names,prediction_probabilities, attributions,iterative_mode=True)\n",
    "processed = exp_record.setup_generation_steps(generation_instruction,)\n",
    "\n",
    "# the final bit \n",
    "iterativeGen =LocalLevelExplanationNarration(classification_explanator,narrator_utils,device,iterative_mode=True,)\n",
    "iterativeGen.generateTexts(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f53d471a03fb5b9741311ec5f82522ec5f217d64ed47634b801d3f5199a0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

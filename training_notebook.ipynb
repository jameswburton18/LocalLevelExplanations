{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/Local_level_model_explanations/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-12 15:38:54.341467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 15:38:54.930196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2022-12-12 15:38:54.930243: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/home/james/Downloads/TensorRT-8.5.1.7/lib\n",
      "2022-12-12 15:38:54.930248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from src.datasetComposer import DatasetBuilder, composed_train_path, composed_test_path, compactComposer, test_path, train_path, test_path\n",
    "from src.inference_routine import InferenceGenerator\n",
    "from src.datasetHandlers import SmartCollator\n",
    "from src.model_utils import get_basic_model\n",
    "from src.trainerArgs import CustomTrainer, getTrainingArguments\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterative_gen = True\n",
    "composed_already = False\n",
    "\n",
    "# Define the parameters used to set up the models\n",
    "modeltype = 'iterative' if iterative_gen else 'normal'  # either baseline or 'earlyfusion'\n",
    "\n",
    "# either t5-small,t5-base, t5-large, facebook/bart-base, or facebook/bart-large\n",
    "modelbase = 'facebook/bart-base'\n",
    "\n",
    "# we will use the above variables to set up the folder to save our model\n",
    "pre_trained_model_name = modelbase.split(\n",
    "    '/')[1] if 'bart' in modelbase else modelbase\n",
    "\n",
    "# where the trained model will be saved\n",
    "output_path = 'TrainModels/' + modeltype + '/'+pre_trained_model_name+'/'\n",
    "\n",
    "#tests = json.load(open(test_path,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "Only the data for the iterative generation have been processed already to reduce the loading and processing time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m iterative_gen:\n\u001b[0;32m----> 3\u001b[0m     experiments_dataset \u001b[39m=\u001b[39m DatasetBuilder(train_data_path\u001b[39m=\u001b[39;49mcomposed_train_path,\n\u001b[1;32m      4\u001b[0m                                      test_data_path\u001b[39m=\u001b[39;49mcomposed_test_path,\n\u001b[1;32m      5\u001b[0m                                      modelbase\u001b[39m=\u001b[39;49m modelbase,\n\u001b[1;32m      6\u001b[0m                                      iterative_mode\u001b[39m=\u001b[39;49m iterative_gen,\n\u001b[1;32m      7\u001b[0m                                      composed_already\u001b[39m=\u001b[39;49mcomposed_already)\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     experiments_dataset \u001b[39m=\u001b[39m DatasetBuilder(train_data_path\u001b[39m=\u001b[39mtrain_path,\n\u001b[1;32m     10\u001b[0m                                      test_data_path\u001b[39m=\u001b[39mtest_path,\n\u001b[1;32m     11\u001b[0m                                      modelbase\u001b[39m=\u001b[39m modelbase,\n\u001b[1;32m     12\u001b[0m                                      iterative_mode\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m                                      composed_already\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/CodingProjects/Local_level_model_explanations/src/datasetComposer.py:90\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, modelbase, train_data_path, test_data_path, preamble_choice, iterative_mode, composed_already, step_continue, include_full_set)\u001b[0m\n\u001b[1;32m     88\u001b[0m     composed_already \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m composed_already:\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_raw \u001b[39m=\u001b[39m composeDataset(json\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(train_data_path,encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)), iterative_mode\u001b[39m=\u001b[39miterative_mode,include_full_set\u001b[39m=\u001b[39minclude_full_set)\n\u001b[1;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_data_raw \u001b[39m=\u001b[39m composeDataset(json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(test_data_path, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)), iterative_mode\u001b[39m=\u001b[39miterative_mode,include_full_set\u001b[39m=\u001b[39minclude_full_set)\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(fp, \u001b[39m*\u001b[39m, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_float\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, parse_constant\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, object_pairs_hook\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m     \u001b[39m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m loads(fp\u001b[39m.\u001b[39;49mread(),\n\u001b[1;32m    294\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, object_hook\u001b[39m=\u001b[39mobject_hook,\n\u001b[1;32m    295\u001b[0m         parse_float\u001b[39m=\u001b[39mparse_float, parse_int\u001b[39m=\u001b[39mparse_int,\n\u001b[1;32m    296\u001b[0m         parse_constant\u001b[39m=\u001b[39mparse_constant, object_pairs_hook\u001b[39m=\u001b[39mobject_pairs_hook, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_decode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors, final)\n\u001b[1;32m    323\u001b[0m     \u001b[39m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "if iterative_gen:\n",
    "    experiments_dataset = DatasetBuilder(train_data_path=composed_train_path,\n",
    "                                     test_data_path=composed_test_path,\n",
    "                                     modelbase= modelbase,\n",
    "                                     iterative_mode= iterative_gen,\n",
    "                                     composed_already=composed_already)\n",
    "else:\n",
    "    experiments_dataset = DatasetBuilder(train_data_path=train_path,\n",
    "                                     test_data_path=test_path,\n",
    "                                     modelbase= modelbase,\n",
    "                                     iterative_mode= False,\n",
    "                                     composed_already=False)\n",
    "\n",
    "\n",
    "experiments_dataset.fit()\n",
    "\n",
    "print('Dataset built')\n",
    "print(f\"Training size: {len(experiments_dataset.train_dataset)}\")\n",
    "print(f\"Test size: {len(experiments_dataset.test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prediction: predictionlabel && predictionlabel: 51.62% && predictionrankB: 48.38% <|section-sep|> featAN featCP featDP featEN featFP featGP featHP featIP <|section-sep|> <mentions> featCP featDP featFP featGP featHP featIP <|section-sep|> featAN featEN <|section-sep|> </mentions> <|section-sep|> <explain> [N0S]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(experiments_dataset.tokenizer_.batch_decode(experiments_dataset.train_dataset[0].input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([37466, 26579,    35, 16782, 33480,  1437, 49145, 16782, 33480,    35,\n",
       "         4074,     4,  5379,   207,  1437, 49145, 16782, 40081,   387,    35,\n",
       "         2929,     4,  3170,   207,  1437, 50279, 11930,  1889, 11930,  7496,\n",
       "        11930,  5174, 11930,  2796, 11930,  9763, 11930, 12694, 11930,  7331,\n",
       "        11930,  3808,  1437, 50279,  1437, 50277, 11930,  7496, 11930,  5174,\n",
       "        11930,  9763, 11930, 12694, 11930,  7331, 11930,  3808,  1437, 50279,\n",
       "        11930,  1889, 11930,  2796,  1437, 50279,  1437, 50278,  1437, 50279,\n",
       "         1437, 50270,  1437, 50295])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_dataset.train_dataset[0].input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Below we create the narration model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n",
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "rand_seed = 453\n",
    "seed_everything(rand_seed)\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "arguments = train_arguments = {'output_dir': output_path,\n",
    "                               'warmup_ratio': 0.2,\n",
    "                               #'disable_tqdm':False,\n",
    "                               'per_device_train_batch_size': 8,\n",
    "                               'num_train_epochs': 4,\n",
    "                               'lr_scheduler_type': 'cosine',\n",
    "                               'learning_rate': 5e-5,\n",
    "                               'evaluation_strategy': 'steps',\n",
    "                               'logging_steps': 500,\n",
    "                               \n",
    "                               'seed': rand_seed}\n",
    "\n",
    "# Build actual trainingArgument object\n",
    "training_arguments = getTrainingArguments(train_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/essel/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/essel/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "getModel = get_basic_model(experiments_dataset)\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(model_init=getModel,\n",
    "                        data_collator=SmartCollator(\n",
    "                            pad_token_id=experiments_dataset.tokenizer_.pad_token_id),\n",
    "                        args=training_arguments,\n",
    "                        train_dataset=experiments_dataset.train_dataset,\n",
    "                        eval_dataset=experiments_dataset.test_dataset,\n",
    "                        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save the model with the lowest evaluation loss\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "# get the best checkpoint\n",
    "best_check_point = trainer.state.best_model_checkpoint\n",
    "\n",
    "\n",
    "params_dict = train_arguments\n",
    "\n",
    "params_dict['best_check_point'] = best_check_point\n",
    "params_dict['output_path'] = output_path\n",
    "json.dump(params_dict, open(f'{output_path}/parameters.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = trainer.model.generator.to(device)\n",
    "inference_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = json.load(open(test_path, encoding='utf-8'))\n",
    "# ,iterative_mode=False,force_section=False\n",
    "if iterative_gen:\n",
    "    test_examples = compactComposer(\n",
    "        tests, iterative_mode=iterative_gen, force_section=True)\n",
    "    test_examples2 = compactComposer(tests, iterative_mode=iterative_gen, force_section=False)\n",
    "else:\n",
    "    test_examples = compactComposer(tests, iterative_mode=iterative_gen, force_section=False)\n",
    "\n",
    "# Instantiate the inference routine\n",
    "iterativeGen = InferenceGenerator(inference_model,\n",
    "                                  experiments_dataset,\n",
    "                                  device,\n",
    "                                  max_iter=8,\n",
    "                                  sampling=False, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation Generation Iteratively\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "max_full_len = 300\n",
    "if iterative_gen:\n",
    "    print(\"Explanation Generation Iteratively\")\n",
    "    iterativeGen.sampling = False\n",
    "    #output_sentences = iterativeGen.MultipleIterativeGeneratorJoint( test_examples, parsed_args.seed,)\n",
    "    output_sentences = iterativeGen.MultipleIterativeGeneratorJoint(\n",
    "        test_examples[:1], rand_seed, length_penalty=1.6,max_length=269)\n",
    "    output_sentences2 = iterativeGen.MultipleIterativeGeneratorJoint(\n",
    "        test_examples2[:1], rand_seed, length_penalty=1.6,max_length=269)\n",
    "else:\n",
    "    print(\"Full Explanation Generation\")\n",
    "    output_sentences = iterativeGen.MultipleFullGeneratorJoint(\n",
    "        test_examples, rand_seed, max_length=max_full_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples[:1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The most probable label for the given case is C1 with a predicted probability of 69.02%, which implies that there is a 30.98% chance that  C2 could be the correct label. F3, F2, and F8 are the features with the most influence on the classifier's decision. F5, F7, and F10 are the features that increase the likelihood of C1 being the correct label. F3, F2, and F8 are the positive features that increase the likelihood of C1 being the correct label. F5, F7, and F10 are the positive features that increase the odds of C1 being the correct label. On the other hand, F9 and F4 are shown to have very little impact on the classifier's decision here.\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f53d471a03fb5b9741311ec5f82522ec5f217d64ed47634b801d3f5199a0064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
